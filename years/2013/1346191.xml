<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  Cloud computing for real-time processing of photon light source data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>01/01/2014</AwardEffectiveDate>
<AwardExpirationDate>06/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>150000.00</AwardTotalIntnAmount>
<AwardAmount>150000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Glenn H. Larsen</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>This SBIR Phase I project proposes to use commercial cloud computing resources for real-time processing of X-ray computed tomography data collected at a photon light source facility. The amount of data being collected at photon light sources is poised to increase dramatically due to improvements in detectors for more rapid data collection and advancements in the underlying accelerator technology for greater production of photons. As a result, storing and processing data sets on a scientist's personal workstation or small computing cluster will become impractical as data set sizes continue to grow much faster than Moore's law. The proposed solution is to develop a persistent, managed computing cluster in the cloud that can grow to meet the computational and storage needs of scientists without them having to deal with managing physical hardware. X-ray computed tomography is used as a test application where an automated workflow can be developed that will store and analyze the collected tomography data during the experiment with minimal user intervention.&lt;br/&gt;&lt;br/&gt;The broader/commercial impact of a persistent, managed computing resource in the cloud is providing scientists an affordable tool that can grow to meet their computational and storage needs without the hassle of managing physical hardware. Storing and analyzing data are routine tasks that are fundamental to scientific research. As technology improves data collection rates and data fidelity, scientists are dealing with larger and larger data sets. Some examples of this growth include genomics, where the cost to sequence a genome has dropped precipitously, astronomy, where powerful telescopes can quickly capture high resolution images, and high energy physics, where data sets need to be reduced in real-time so that their sizes are more manageable. This dramatic increase is contributing to the "Big Data" problem that scientists are facing as their traditional data processing tools are no longer sufficient to tackle these data sets. By taking advantage of the economies of scale achieved by cloud service providers, Billow can provide reliable storage and high availability computing at a lower cost. This approach allows scientists to focus on the science by giving them the necessary tools to analyze their data and develop new data analysis algorithms.</AbstractNarration>
<MinAmdLetterDate>12/19/2013</MinAmdLetterDate>
<MaxAmdLetterDate>12/19/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1346191</AwardID>
<Investigator>
<FirstName>Billy</FirstName>
<LastName>Poon</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Billy K Poon</PI_FULL_NAME>
<EmailAddress>billykpoon@gmail.com</EmailAddress>
<PI_PHON>5102898105</PI_PHON>
<NSF_ID>000646299</NSF_ID>
<StartDate>12/19/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Billow, Inc.</Name>
<CityName>Berkeley</CityName>
<ZipCode>947072201</ZipCode>
<PhoneNumber>5102898105</PhoneNumber>
<StreetAddress>1737 Solano Avenue #204</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>078822955</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BILLOW, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[Billow, Inc.]]></Name>
<CityName>Berkeley</CityName>
<StateCode>CA</StateCode>
<ZipCode>947072201</ZipCode>
<StreetAddress><![CDATA[1737 Solano Avenue #204]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramReference>
<Code>5371</Code>
<Text>SMALL BUSINESS PHASE I</Text>
</ProgramReference>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<ProgramReference>
<Code>8033</Code>
<Text>Hardware Software Integration</Text>
</ProgramReference>
<ProgramReference>
<Code>8039</Code>
<Text>Information, Communication &amp; Computing</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~150000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-967268b1-47db-733c-8074-e36a0bb4fa9b"> <span id="docs-internal-guid-967268b1-47de-4355-2823-d5588e498889"> </span></span></p> <p dir="ltr"><span><span> </span></span><span>The goal of this National Science Foundation (NSF) Small Business Innovation Research (SBIR) Phase I project awarded to Billow, Inc. (Award Number 1346191) was to establish the feasibility of using Amazon Web Services (AWS) to process data collected at photon light source facilities. X-ray computed tomography was selected as the test application because this experimental method generates on the order of 10 to 100 gigabytes (GB) per dataset, the reconstruction algorithm is mature and standardized, and the data processing workflow is easily parallelizable. We concluded that it is feasible if the application is optimized to use the different cloud computing resources effectively. By benchmarking the processing, storage, and networking capabilities of a compute-optimized configuration (c3.8xlarge) on AWS, we found that the performance is comparable to existing high performance computing (HPC) resources, but with a few caveats. Due to virtualization, there was a performance penalty in raw computing speed, but that was offset by the fast local storage offered by the solid state drives (SSD) configured in RAID0. And with the proper configuration of the virtualization settings, it was possible to achieve full use of the 10 Gbit/s network connection.</span></p> <p dir="ltr"><span><span> </span></span><span>From our benchmarking of Simple Storage Service (S3), we think that the real potential for cloud computing in scientific applications is from effectively using S3 to provide scalable bandwidth to deliver data to processing nodes. If the data are structured properly, by following several best practices for organizing the files, bandwidth should scale linearly with the number of virtual machines, or instances. This property should fit well with &ldquo;Big Data&rdquo; applications because if an experiment produces a lot of data, the processing of that data should be done in the most parallel way possible. We originally thought that Hadoop would be able to accomplish this and Amazon&rsquo;s Elastic MapReduce (EMR) service would provide a convenient interface for submitting and tracking Hadoop jobs, but our experience in Phase I shows that there were technical challenges that are not easily resolved. For Phase II, we envision a potential solution by having instances behave independently and processing chunks of data one at a time. The raw data would be downloaded from S3 and when the work is complete, the result would be uploaded back to S3. This would allow the instance to make full use of the SSDs as a temporary workspace and use S3 to share output.</span></p> <p dir="ltr"><span><span> </span></span><span>For X-ray tomography, the image correction step is embarrassingly parallel and each image can be processed independently of all the other images. The entire dataset would be divided into smaller files stored in S3 and the processing nodes would only need to download the part that the node will process. The corrected images would then be uploaded back into S3 where all the nodes would have access to them for constructing the sinograms. Again, this step can be parallelized by having each node start with a different section of the sinogram. The reconstruction and ring correction steps would follow the same pattern.</span></p> <p dir="ltr"><span><span> </span></span><span>Visualization is another area that can greatly benefit from cloud computing. If we can view and tweak the reconstruction without having to transfer it to a local workstation first, that would enable faster discovery since large datasets would not need to be constantly transferred between a computer for viewing and a cluster for heavy computing. With AWS and streaming visualization using the graphics processor ena...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[     The goal of this National Science Foundation (NSF) Small Business Innovation Research (SBIR) Phase I project awarded to Billow, Inc. (Award Number 1346191) was to establish the feasibility of using Amazon Web Services (AWS) to process data collected at photon light source facilities. X-ray computed tomography was selected as the test application because this experimental method generates on the order of 10 to 100 gigabytes (GB) per dataset, the reconstruction algorithm is mature and standardized, and the data processing workflow is easily parallelizable. We concluded that it is feasible if the application is optimized to use the different cloud computing resources effectively. By benchmarking the processing, storage, and networking capabilities of a compute-optimized configuration (c3.8xlarge) on AWS, we found that the performance is comparable to existing high performance computing (HPC) resources, but with a few caveats. Due to virtualization, there was a performance penalty in raw computing speed, but that was offset by the fast local storage offered by the solid state drives (SSD) configured in RAID0. And with the proper configuration of the virtualization settings, it was possible to achieve full use of the 10 Gbit/s network connection.  From our benchmarking of Simple Storage Service (S3), we think that the real potential for cloud computing in scientific applications is from effectively using S3 to provide scalable bandwidth to deliver data to processing nodes. If the data are structured properly, by following several best practices for organizing the files, bandwidth should scale linearly with the number of virtual machines, or instances. This property should fit well with "Big Data" applications because if an experiment produces a lot of data, the processing of that data should be done in the most parallel way possible. We originally thought that Hadoop would be able to accomplish this and AmazonÃ†s Elastic MapReduce (EMR) service would provide a convenient interface for submitting and tracking Hadoop jobs, but our experience in Phase I shows that there were technical challenges that are not easily resolved. For Phase II, we envision a potential solution by having instances behave independently and processing chunks of data one at a time. The raw data would be downloaded from S3 and when the work is complete, the result would be uploaded back to S3. This would allow the instance to make full use of the SSDs as a temporary workspace and use S3 to share output.  For X-ray tomography, the image correction step is embarrassingly parallel and each image can be processed independently of all the other images. The entire dataset would be divided into smaller files stored in S3 and the processing nodes would only need to download the part that the node will process. The corrected images would then be uploaded back into S3 where all the nodes would have access to them for constructing the sinograms. Again, this step can be parallelized by having each node start with a different section of the sinogram. The reconstruction and ring correction steps would follow the same pattern.  Visualization is another area that can greatly benefit from cloud computing. If we can view and tweak the reconstruction without having to transfer it to a local workstation first, that would enable faster discovery since large datasets would not need to be constantly transferred between a computer for viewing and a cluster for heavy computing. With AWS and streaming visualization using the graphics processor enabled configurations, we would be able to have both.   The results from this Phase I project have shown the feasibility and potential for cloud computing to address some of the "Big Data" problems that scientists will face as they begin to generate ever increasing amounts of data. With our experience from Phase I, we have a clear path for what needs to be done in Phase II to develop a working solution for X-ray tomography.          Last Modif...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
