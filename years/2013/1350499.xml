<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: DrCloud: Drill-Ready Cloud Computing</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>05/01/2014</AwardEffectiveDate>
<AwardExpirationDate>04/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>279944.00</AwardTotalIntnAmount>
<AwardAmount>449350</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Marilyn McClure</SignBlockName>
<PO_EMAI>mmcclure@nsf.gov</PO_EMAI>
<PO_PHON>7032925197</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Cloud computing is pervasive, but cloud service outages still take place.  This proposal addresses how to ensure that failure recovery will work robustly in many deployment scenarios.  To address this important question, this project proposes drill-ready cloud computing (DrCloud), a new dependability paradigm that advocates cloud systems to routinely perform "failure drills" in real deployments (i.e., deliberately schedule real failures rather than waiting for unexpected real failures to happen).  This practice can unearth in-production recovery issues and prevent real outages. &lt;br/&gt;&lt;br/&gt;This project will create five building blocks of drill-ready cloud computing:  methodology, safety, efficiency, usability, and generality. Specifically, these five sub-projects will substantiate a new research methodology via a formal study of hundreds of in-production recovery issues, devise mechanisms that guarantee safety (no data loss and performance disruptions) analogous to a proper fire drill preparation, develop techniques that maximize resource and monetary efficiency of drill execution, design a specification language and its runtime that simplifies drill usability, and finally boost drill generality beyond failure drills (e.g., supporting software upgrade and configuration change drills). &lt;br/&gt;&lt;br/&gt;The DrCloud project will enrich decades of research and literature in fault-tolerant computing.  The project will also bring many direct benefits to the society; users from many areas increasingly use large-scale storage and computation services, depending on high availability and predictability that drill-ready cloud computing will facilitate.  The project will also involve state-of-the-art scale-out cloud systems (Hadoop, Cassandra, HBase, etc.).  Adding drill-readiness to these systems will provide prototypes of next-generation reliable systems.</AbstractNarration>
<MinAmdLetterDate>12/11/2013</MinAmdLetterDate>
<MaxAmdLetterDate>04/26/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1350499</AwardID>
<Investigator>
<FirstName>Haryadi</FirstName>
<LastName>Gunawi</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Haryadi Gunawi</PI_FULL_NAME>
<EmailAddress>haryadi@cs.uchicago.edu</EmailAddress>
<PI_PHON>7737025772</PI_PHON>
<NSF_ID>000626546</NSF_ID>
<StartDate>12/11/2013</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Chicago</Name>
<CityName>Chicago</CityName>
<ZipCode>606372612</ZipCode>
<PhoneNumber>7737028669</PhoneNumber>
<StreetAddress>6054 South Drexel Avenue</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IL01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>005421136</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CHICAGO, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>005421136</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Chicago]]></Name>
<CityName>Chicago</CityName>
<StateCode>IL</StateCode>
<ZipCode>606375418</ZipCode>
<StreetAddress><![CDATA[1100 E 58th Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IL01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7354</Code>
<Text>CSR-Computer Systems Research</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~196385</FUND_OBLG>
<FUND_OBLG>2015~76645</FUND_OBLG>
<FUND_OBLG>2017~78754</FUND_OBLG>
<FUND_OBLG>2018~97566</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The DrCloud project has advanced research in cloud systems reliability with various works that reveal systems problems that can appear in large-scale deployments.&nbsp;</p> <p><br />In the Drill project [SoCC 14], we raised this fundamental question: how can we ensure that cloud services work robustly against many failure scenarios in real deployments? &nbsp;As many failure scenarios cannot be covered in offline testing, failures should be deliberately injected online in actual deployments. &nbsp;We name this method "failure drill". The principle here is to make failure a first-class citizen: rather than waiting for unplanned failures to happen, cloud services should plan and schedule failure drills from time to time (analogous to a routine exercise of fire drills), thereby unearthing in-production recovery issues early before they lead to major outages.&nbsp;</p> <p>In the CBS project [SoCC 14], we conducted a comprehensive study of development and deployment issues of six popular and important cloud systems (Hadoop MapReduce, HDFS, HBase, Cassandra, ZooKeeper and Flume). &nbsp;We reviewed in total 21,399 submitted issues within a three-year period (2011- 2014). Among these issues, we perform a deep analysis of 3655 "vital" issues (i.e., real issues affecting deployments) with a set of detailed classifications. &nbsp;</p> <p>In the COS project [SoCC 16], We conducted a cloud outage study (COS) of 32 popular Internet services. We analyzed 1247 headline news and public post-mortem reports that detail 597 unplanned outages that occurred within a 7-year span from 2009 to 2015. We analyzed outage duration, root causes, impacts, and fix procedures. This study reveals the broader availability landscape of modern cloud services and provides answers to why outages still take place even with pervasive redundancies. &nbsp;One valuable information in our dataset is the outage duration; 69% of outages are reported with downtime information. This allows us to quantify the overall availability of modern cloud services and analyze which root causes have longer impacts. &nbsp;</p> <p>In the Scalability Bugs project [HotOS 17], we highlighted the problem of scalability bugs, a new class of bugs that appear in "cloud-scale" distributed systems. Scalability bugs are latent bugs that are cluster-scale dependent, whose symptoms typically surface in large-scale deployments, but not in small or medium-scale deployments. The standard practice to test large distributed systems is to deploy them on a large number of machines ("real-scale testing"), which is difficult and expensive. New methods are needed to reduce developers&rsquo; burdens in finding, reproducing, and debugging scalability bugs. &nbsp;&nbsp;</p> <p>In the ScaleCheck Project [FAST 19], we built an approach for discovering scalability bugs (a new class of bug in large storage systems) and for democratizing large-scale testing. The tool employs a program analysis technique, for finding potential causes of scalability bugs, and a series of colocation techniques, for testing implementation code at real scales but doing so on just a commodity PC. ScaleCheck has been integrated to several large-scale storage systems, Cassandra, HDFS, Riak, and Voldemort, and successfully exposed known and unknown scalability bugs, up to 512-node scale on a 16-core PC.&nbsp;</p> <p>In the ETLIB project [in submission], we introduced an ecosystem of tail-latency mitigation with supports from library and other layers. &nbsp;ETLIB provides an end-to-end request abstraction that enables a uniform type of tail-mitigation capabilities, namely request cancellation and delay prediction, that can be stackable together across multiple resource layers. &nbsp;Our evaluation shows that with ETLIB, multi-resource cloud storge applications are faster by 5-70% starting at 90P (the 90th percentile) compared to popular practices such as speculative execution and is only 3% slower on average compared to a best-case (no tail) scenario.&nbsp;<br />With the COBE Finder project [manuscript in preparation], we attempt to prevent service downtimes in datacenter and mobile systems caused by cascading outage bugs ("CO bugs" in short), a new class of bugs that can cause simultaneous or cascades of failures to many or all system nodes/components.</p> <p>In this CObe project, we will: (1) study the anatomy of CO bugs, (2) develop CO-bug detection tools to unearth CO bugs, and (3) build CO-bug containment solutions to prevent CO bugs from causing an outage in deployment.&nbsp;<br /><br />Broader Impact: &nbsp;</p> <p><br />Users from many areas (science, healthcare, business, education, military, and government) increasingly demand for large-scale compute and storage. To serve their demands, systems must run at an extreme scale. &nbsp;Netflix is reported to run 50 Cassandra clusters of 500 machines each. &nbsp;Apple is also reported to run a total of 100,000 Cassandra nodes. &nbsp;At such an extreme scale, cloud-scale distributed systems must be stable. We believe our project helps improving the scalability, availability, and predictability of today's cloud systems.&nbsp;</p> <p>Our in-depth analysis and studies of various bugs, failures and outages in large-scale open-source and commercial datacenter and storage systems in the last six years have collectively been cited more than 300 times and whose datasets have been downloaded over 200 times.&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 06/05/2020<br>      Modified by: Haryadi&nbsp;Gunawi</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The DrCloud project has advanced research in cloud systems reliability with various works that reveal systems problems that can appear in large-scale deployments.    In the Drill project [SoCC 14], we raised this fundamental question: how can we ensure that cloud services work robustly against many failure scenarios in real deployments?  As many failure scenarios cannot be covered in offline testing, failures should be deliberately injected online in actual deployments.  We name this method "failure drill". The principle here is to make failure a first-class citizen: rather than waiting for unplanned failures to happen, cloud services should plan and schedule failure drills from time to time (analogous to a routine exercise of fire drills), thereby unearthing in-production recovery issues early before they lead to major outages.   In the CBS project [SoCC 14], we conducted a comprehensive study of development and deployment issues of six popular and important cloud systems (Hadoop MapReduce, HDFS, HBase, Cassandra, ZooKeeper and Flume).  We reviewed in total 21,399 submitted issues within a three-year period (2011- 2014). Among these issues, we perform a deep analysis of 3655 "vital" issues (i.e., real issues affecting deployments) with a set of detailed classifications.    In the COS project [SoCC 16], We conducted a cloud outage study (COS) of 32 popular Internet services. We analyzed 1247 headline news and public post-mortem reports that detail 597 unplanned outages that occurred within a 7-year span from 2009 to 2015. We analyzed outage duration, root causes, impacts, and fix procedures. This study reveals the broader availability landscape of modern cloud services and provides answers to why outages still take place even with pervasive redundancies.  One valuable information in our dataset is the outage duration; 69% of outages are reported with downtime information. This allows us to quantify the overall availability of modern cloud services and analyze which root causes have longer impacts.    In the Scalability Bugs project [HotOS 17], we highlighted the problem of scalability bugs, a new class of bugs that appear in "cloud-scale" distributed systems. Scalability bugs are latent bugs that are cluster-scale dependent, whose symptoms typically surface in large-scale deployments, but not in small or medium-scale deployments. The standard practice to test large distributed systems is to deploy them on a large number of machines ("real-scale testing"), which is difficult and expensive. New methods are needed to reduce developersâ€™ burdens in finding, reproducing, and debugging scalability bugs.     In the ScaleCheck Project [FAST 19], we built an approach for discovering scalability bugs (a new class of bug in large storage systems) and for democratizing large-scale testing. The tool employs a program analysis technique, for finding potential causes of scalability bugs, and a series of colocation techniques, for testing implementation code at real scales but doing so on just a commodity PC. ScaleCheck has been integrated to several large-scale storage systems, Cassandra, HDFS, Riak, and Voldemort, and successfully exposed known and unknown scalability bugs, up to 512-node scale on a 16-core PC.   In the ETLIB project [in submission], we introduced an ecosystem of tail-latency mitigation with supports from library and other layers.  ETLIB provides an end-to-end request abstraction that enables a uniform type of tail-mitigation capabilities, namely request cancellation and delay prediction, that can be stackable together across multiple resource layers.  Our evaluation shows that with ETLIB, multi-resource cloud storge applications are faster by 5-70% starting at 90P (the 90th percentile) compared to popular practices such as speculative execution and is only 3% slower on average compared to a best-case (no tail) scenario.  With the COBE Finder project [manuscript in preparation], we attempt to prevent service downtimes in datacenter and mobile systems caused by cascading outage bugs ("CO bugs" in short), a new class of bugs that can cause simultaneous or cascades of failures to many or all system nodes/components.  In this CObe project, we will: (1) study the anatomy of CO bugs, (2) develop CO-bug detection tools to unearth CO bugs, and (3) build CO-bug containment solutions to prevent CO bugs from causing an outage in deployment.   Broader Impact:     Users from many areas (science, healthcare, business, education, military, and government) increasingly demand for large-scale compute and storage. To serve their demands, systems must run at an extreme scale.  Netflix is reported to run 50 Cassandra clusters of 500 machines each.  Apple is also reported to run a total of 100,000 Cassandra nodes.  At such an extreme scale, cloud-scale distributed systems must be stable. We believe our project helps improving the scalability, availability, and predictability of today's cloud systems.   Our in-depth analysis and studies of various bugs, failures and outages in large-scale open-source and commercial datacenter and storage systems in the last six years have collectively been cited more than 300 times and whose datasets have been downloaded over 200 times.           Last Modified: 06/05/2020       Submitted by: Haryadi Gunawi]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
