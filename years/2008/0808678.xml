<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC-Large: Using the Internet without using the Eyes: Models of Online Transactions for Non-Visual Interaction</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>1582860.00</AwardTotalIntnAmount>
<AwardAmount>1623540</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The Internet has become the primary medium for accessing information and for conducting many types of online transactions, including shopping, paying bills, making travel plans, applying for college or employment, and participating in civic activities.  The primary mode of interaction over the Internet is via graphical browsers designed for visual navigation. This seriously limits the access of people with impaired vision or blindness, a population that is large and growing ever larger.  Existing assistive technology for non-visual Internet access typically forces users with visual impairments into an inefficient, sequential mode of information access.  To do better, two kinds of models are needed.  First, we need to build computational models to represent the structure of web pages and online transactions, and to present them effectively using non-visual modalities.  Second, we need to better understand how users' mental models for online transactions are built and utilized; we then need to align the computational models with the users' mental models, so as  to combine their strengths and significantly improve the efficiency of non-visual interactions.  In previous work, the PI developed the HearSay non-visual web browser, which permits users to perform basic non-visual web browsing and search, contextual browsing, and online form-filling.  However, HearSay does not take full advantage of the interaction context or the unique perceptual and processing strengths of people with visual impairments.  In the current project, the PI seeks to combine basic computational and psychological research designed to produce accessibility technology embodying the synergy of computational modeling and users' mental models.  In terms of computational research, the PI will: (i) automatically track the interaction context of user browsing actions; (ii) automatically build models for transactions that users perform online; and (iii) develop ways in which users can interact with transaction models through non-visual modalities efficiently and effectively.  In terms of psychological research, user studies will be conducted to examine (i) how people build mental models for online transactions, and (ii) how they use modality-specific cues and their own short-term memory to utilize these mental models.  The PI will incorporate the findings from these user studies into the computational models for online transaction processing, so as to align them with the users' mental models.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  The ultimate goal of the PI's research is to empower people with visual impairments to lead completely independent lives with the help of the Internet.  To this end, the PI has planned an extensive dissemination campaign involving workshops, collaborations with institutions that serve people who have visual impairments, and online dissemination of HearSay prototypes and HearSay component technologies.  HearSay will also provide a means, in principle, for anyone who wishes to have non-visual Internet access (e.g., listening to Internet content while driving).</AbstractNarration>
<MinAmdLetterDate>09/04/2008</MinAmdLetterDate>
<MaxAmdLetterDate>07/15/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0808678</AwardID>
<Investigator>
<FirstName>I.</FirstName>
<LastName>Ramakrishnan</LastName>
<PI_MID_INIT>V</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>I. V Ramakrishnan</PI_FULL_NAME>
<EmailAddress>ram@cs.stonybrook.edu</EmailAddress>
<PI_PHON>6316328451</PI_PHON>
<NSF_ID>000365929</NSF_ID>
<StartDate>09/04/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Susan</FirstName>
<LastName>Brennan</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Susan E Brennan</PI_FULL_NAME>
<EmailAddress>susan.brennan@stonybrook.edu</EmailAddress>
<PI_PHON>6316329145</PI_PHON>
<NSF_ID>000106933</NSF_ID>
<StartDate>09/04/2008</StartDate>
<EndDate>07/15/2011</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Amanda</FirstName>
<LastName>Stent</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Amanda Stent</PI_FULL_NAME>
<EmailAddress>amanda.stent@gmail.com</EmailAddress>
<PI_PHON>6316328447</PI_PHON>
<NSF_ID>000203133</NSF_ID>
<StartDate>09/04/2008</StartDate>
<EndDate>03/07/2011</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yevgen</FirstName>
<LastName>Borodin</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yevgen Borodin</PI_FULL_NAME>
<EmailAddress>borodin@charmtechlabs.com</EmailAddress>
<PI_PHON>5163137356</PI_PHON>
<NSF_ID>000532638</NSF_ID>
<StartDate>03/07/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Stony Brook</Name>
<CityName>Stony Brook</CityName>
<ZipCode>117940001</ZipCode>
<PhoneNumber>6316329949</PhoneNumber>
<StreetAddress>WEST 5510 FRK MEL LIB</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804878247</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Stony Brook]]></Name>
<CityName>Stony Brook</CityName>
<StateCode>NY</StateCode>
<ZipCode>117940001</ZipCode>
<StreetAddress><![CDATA[WEST 5510 FRK MEL LIB]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0116000</Code>
<Name>Human Subjects</Name>
</FoaInformation>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>1640</Code>
<Text>INFORMATION TECHNOLOGY RESEARC</Text>
</ProgramReference>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7925</Code>
<Text>LARGE PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9215</Code>
<Text>HIGH PERFORMANCE COMPUTING SYSTEMS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~1582860</FUND_OBLG>
<FUND_OBLG>2010~21000</FUND_OBLG>
<FUND_OBLG>2011~19680</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The World Wide Web &nbsp;has become the primary medium for accessing information and for conducting many types of online&nbsp;<em>transactions</em>, including shopping online, paying bills, making travel plans, applying for college or employment, and participating in civic activities. The primary mode of interaction with the Web is via graphical browsers designed for visual navigation.&nbsp;This seriously limits the access of people with impaired vision or blindness, a population that is large and growing ever larger.&nbsp; Just to give a sense of the size of the target population: in the U.S. alone, there are over 10 million visually impaired people, of whom approximately 1.3 million people are legally blind, as reported by the American Foundation for the Blind, and, according to the World Health Organization&rsquo;s 2003 report there are more than 175 million people with visual impairment worldwide (40 to 45 million blind and 135 million with low vision).</p> <p class="EHeading4">Blind people use screen readers to access the Web.&nbsp; Screen-readers and more generally non-visual web browsers typically read out the content of the screen, ignoring the graphics and layout of web pages while giving other audio or Braille feedback to help navigate web pages.</p> <p class="EHeading4">However a large gap remains between the ways sighted and blind users browse the Web due to the differences in their perception and the mode of interaction.&nbsp; Sighted users can quickly process web content by visually segmenting any web page into sections, classifying the whole page and its parts, finding patterns, filtering out irrelevant information, and quickly identifying possible steps and actions they can take on any web page (e.g.: log in, add to cart, etc.).&nbsp; On the other hand, blind users are forced to process information sequentially, as screen readers read the page in the order it is presented in web pages.&nbsp; While shortcuts are provided for skipping through text or reading it more rapidly, blind users still have to listen to much irrelevant content before reaching the content of interest as screen-readers provide almost no content-analysis to facilitate access to relevant information.&nbsp; When a blind user visits a web page for the first time, s/he cannot easily tell, without listening to all of it, how much information it contains.&nbsp; Navigating back and forth among pages, blind users often have to listen to redundant information, design strategies to find relevant content, or remember page structure to make web browsing more efficient. All of these difficulties make non-visual web browsing slow and difficult. In short, blind users can experience considerable information overload when using assistive tools. This is especially true of web transactions such as shopping, registrations, online banking and bill-payments, which often involve a number of steps spanning several web pages</p> <p>To significantly advance the state of the art in web accessibility technology for blind people the project proposed developing a computational model based on how blind users&nbsp; &nbsp;use the non-visual interface for browsing and search and on how they use non-visual contextual cues that are constructed and maintained during non-visual transactions. The computational model would facilitate repositioning of web content for doing efficient search and retrieval of information as well as for conducting online transactions with non-visual modalities, namely keyboards and audio. Development of such a model was the principal objective of this research project.</p> <p>A major outcome of the project is Hearsay,&nbsp;a multi-modal non-visual web browser incorporating such a model. Four years in the making HearSay is &nbsp;a working system. On the algorithmic side, it incorporates a number of robust and scalable techniques based on Information Retrieval and Machine Learning, including: ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The World Wide Web  has become the primary medium for accessing information and for conducting many types of online transactions, including shopping online, paying bills, making travel plans, applying for college or employment, and participating in civic activities. The primary mode of interaction with the Web is via graphical browsers designed for visual navigation. This seriously limits the access of people with impaired vision or blindness, a population that is large and growing ever larger.  Just to give a sense of the size of the target population: in the U.S. alone, there are over 10 million visually impaired people, of whom approximately 1.3 million people are legally blind, as reported by the American Foundation for the Blind, and, according to the World Health OrganizationÃ†s 2003 report there are more than 175 million people with visual impairment worldwide (40 to 45 million blind and 135 million with low vision). Blind people use screen readers to access the Web.  Screen-readers and more generally non-visual web browsers typically read out the content of the screen, ignoring the graphics and layout of web pages while giving other audio or Braille feedback to help navigate web pages. However a large gap remains between the ways sighted and blind users browse the Web due to the differences in their perception and the mode of interaction.  Sighted users can quickly process web content by visually segmenting any web page into sections, classifying the whole page and its parts, finding patterns, filtering out irrelevant information, and quickly identifying possible steps and actions they can take on any web page (e.g.: log in, add to cart, etc.).  On the other hand, blind users are forced to process information sequentially, as screen readers read the page in the order it is presented in web pages.  While shortcuts are provided for skipping through text or reading it more rapidly, blind users still have to listen to much irrelevant content before reaching the content of interest as screen-readers provide almost no content-analysis to facilitate access to relevant information.  When a blind user visits a web page for the first time, s/he cannot easily tell, without listening to all of it, how much information it contains.  Navigating back and forth among pages, blind users often have to listen to redundant information, design strategies to find relevant content, or remember page structure to make web browsing more efficient. All of these difficulties make non-visual web browsing slow and difficult. In short, blind users can experience considerable information overload when using assistive tools. This is especially true of web transactions such as shopping, registrations, online banking and bill-payments, which often involve a number of steps spanning several web pages  To significantly advance the state of the art in web accessibility technology for blind people the project proposed developing a computational model based on how blind users   use the non-visual interface for browsing and search and on how they use non-visual contextual cues that are constructed and maintained during non-visual transactions. The computational model would facilitate repositioning of web content for doing efficient search and retrieval of information as well as for conducting online transactions with non-visual modalities, namely keyboards and audio. Development of such a model was the principal objective of this research project.  A major outcome of the project is Hearsay, a multi-modal non-visual web browser incorporating such a model. Four years in the making HearSay is  a working system. On the algorithmic side, it incorporates a number of robust and scalable techniques based on Information Retrieval and Machine Learning, including: content analysis that partitions web pages into meaningful sections for ease of navigation; context-directed browsing that exploits the content surrounding a link to find relevant information as users move fro...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
