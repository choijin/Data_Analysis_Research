<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI-Small: Motion-induced Extrinsic Calibration of Rigid and Reconfigurable Networks of Sensors</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>389999.00</AwardTotalIntnAmount>
<AwardAmount>397999</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Richard Voyles</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Navigation sensors are found not only on robots, but also on passenger vehicles, portable devices, such as cell phones, and on wheel-chairs and white-canes that aid people with disabilities. For several applications, sensor-fusion algorithms are employed to combine measurements from multiple sensors rigidly attached to the same vehicle, or spatially dispersed and mobile. In either case, their relative spatial configuration must be known. This introduces the need for sensor-to-sensor extrinsic calibration. Moreover, in order for sensor measurements to be useful for high-precision navigation, their placement on the host robot must be determined, which brings about the problem of sensor-to-body calibration.&lt;br/&gt;&lt;br/&gt;To this day, very little is known about how to rigorously perform sensor-to-sensor and sensor-to-body extrinsic calibration. Typically, the transformations between the sensor and/or robot frames of interest are estimated using expensive calibration equipment, or found through approximate manual measurements and use of CAD plots.  The objectives of this research effort are to develop rigorous methods for motion-induced sensor-to-sensor and sensor-to-body calibration, and to conduct a detailed theoretical study of their performance.  The results of this work will significantly improve the quality and reduce the effort needed for multi-sensor calibration, thus providing valuable tools for designing and implementing navigation systems.</AbstractNarration>
<MinAmdLetterDate>07/22/2008</MinAmdLetterDate>
<MaxAmdLetterDate>04/22/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0811946</AwardID>
<Investigator>
<FirstName>Stergios</FirstName>
<LastName>Roumeliotis</LastName>
<PI_MID_INIT>I</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stergios I Roumeliotis</PI_FULL_NAME>
<EmailAddress>stergios@cs.umn.edu</EmailAddress>
<PI_PHON>6126267507</PI_PHON>
<NSF_ID>000487045</NSF_ID>
<StartDate>07/22/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Minnesota-Twin Cities</Name>
<CityName>Minneapolis</CityName>
<ZipCode>554552070</ZipCode>
<PhoneNumber>6126245599</PhoneNumber>
<StreetAddress>200 OAK ST SE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<StateCode>MN</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MN05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>555917996</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF MINNESOTA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>117178941</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Minnesota-Twin Cities]]></Name>
<CityName>Minneapolis</CityName>
<StateCode>MN</StateCode>
<ZipCode>554552070</ZipCode>
<StreetAddress><![CDATA[200 OAK ST SE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Minnesota</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MN05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~389999</FUND_OBLG>
<FUND_OBLG>2011~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="text-align: left;">For accurate autonomous navigation, mobile robots fuse information from multiple onboard navigation sensors such as cameras, laser scanners, wheel encoders, gyroscopes, and accelerometers that are rigidly attached at different locations on the robot&rsquo;s body. For example, the camera is located on an elevated platform to maximize its field of view, while the wheel encoders are attached to the wheels. Since each sensor obtains measurements in its own local frame of reference, it becomes necessary to determine the spatial configuration between these sensors in order to correctly fuse their measurements. This is known as sensor-to-sensor extrinsic calibration. Moreover, in order for sensor measurements to be useful for high-precision navigation, their placement on the host robot or their environment must be determined, which brings about the problems of sensor-to-body and sensor-to-world calibration. A more challenging version of these problems involves fusing sensors&rsquo; information from multiple mobile robots in a team, which requires determining the spatial configuration between the robots themselves.</p> <p style="text-align: left;">Until recently, very little was known about how to rigorously perform sensor-to-sensor and sensor-to-body extrinsic calibration. Typically, the unknown transformations between the sensors and/or robot frames of interest was estimated using expensive calibration equipment, which in most cases is available only inside a laboratory. However, since the calibration parameters change over time (e.g., due to vibrations), this approach is unsuitable for recalibration of robots deployed in the field, leading to performance degradation over time. Another approach for determining the extrinsic calibration was through approximate manual measurements and the use of CAD plots. However, the limited accuracy of this approach prohibits its use for high-precision navigation tasks such as obstacle avoidance.</p> <p style="text-align: left;">Therefore, to address these drawbacks, the objective of this research effort was to design extrinsic calibration algorithms that utilize the sensors&rsquo; own measurement information, while in motion, to accurately determine the calibration parameters.</p> <p style="text-align: left;">For this proposed motion-induced extrinsic calibration approach, our research effort focused &nbsp;on designing algorithms for a variety of extrinsic calibration problems and addressing fundamental issues such as determining the minimum amount of information and the motion profile necessary for correctly determining calibration parameters.</p> <p style="text-align: left;">In particular, we considered extrinsic calibration problems both in 2D and 3D using distance and/or bearing measurements. These include sensor-to-sensor (wheel encoder-camera, inertial measurement unit-camera, 3D laser scanner-spherical camera), sensor-to-body (camera-body), and sensor-to-world (camera-world) extrinsic calibration for individual robots and also robot-to-robot extrinsic calibration for mobile robot teams. Distance measurements are often provided by telecommunication equipment (e.g., based on the time of flight of the signal used for communication). On the other hand, cameras provide images from which we can extract the bearing (direction) towards an object of interest. Using these types of measurements, we have carried out detailed theoretical analysis and provided solutions to both deterministic and probabilistic problem formulations. For deterministic problems, which assume that the measurement noise is negligible, we have provided solutions for both the over-determined (typically, when the number of scalar measurements is greater than the number of unknowns) and minimal problems (typically, when the number of scalar measurements equals the number of unknowns). The advantage of over-determined problems is that they are ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[For accurate autonomous navigation, mobile robots fuse information from multiple onboard navigation sensors such as cameras, laser scanners, wheel encoders, gyroscopes, and accelerometers that are rigidly attached at different locations on the robotÆs body. For example, the camera is located on an elevated platform to maximize its field of view, while the wheel encoders are attached to the wheels. Since each sensor obtains measurements in its own local frame of reference, it becomes necessary to determine the spatial configuration between these sensors in order to correctly fuse their measurements. This is known as sensor-to-sensor extrinsic calibration. Moreover, in order for sensor measurements to be useful for high-precision navigation, their placement on the host robot or their environment must be determined, which brings about the problems of sensor-to-body and sensor-to-world calibration. A more challenging version of these problems involves fusing sensorsÆ information from multiple mobile robots in a team, which requires determining the spatial configuration between the robots themselves. Until recently, very little was known about how to rigorously perform sensor-to-sensor and sensor-to-body extrinsic calibration. Typically, the unknown transformations between the sensors and/or robot frames of interest was estimated using expensive calibration equipment, which in most cases is available only inside a laboratory. However, since the calibration parameters change over time (e.g., due to vibrations), this approach is unsuitable for recalibration of robots deployed in the field, leading to performance degradation over time. Another approach for determining the extrinsic calibration was through approximate manual measurements and the use of CAD plots. However, the limited accuracy of this approach prohibits its use for high-precision navigation tasks such as obstacle avoidance. Therefore, to address these drawbacks, the objective of this research effort was to design extrinsic calibration algorithms that utilize the sensorsÆ own measurement information, while in motion, to accurately determine the calibration parameters. For this proposed motion-induced extrinsic calibration approach, our research effort focused  on designing algorithms for a variety of extrinsic calibration problems and addressing fundamental issues such as determining the minimum amount of information and the motion profile necessary for correctly determining calibration parameters. In particular, we considered extrinsic calibration problems both in 2D and 3D using distance and/or bearing measurements. These include sensor-to-sensor (wheel encoder-camera, inertial measurement unit-camera, 3D laser scanner-spherical camera), sensor-to-body (camera-body), and sensor-to-world (camera-world) extrinsic calibration for individual robots and also robot-to-robot extrinsic calibration for mobile robot teams. Distance measurements are often provided by telecommunication equipment (e.g., based on the time of flight of the signal used for communication). On the other hand, cameras provide images from which we can extract the bearing (direction) towards an object of interest. Using these types of measurements, we have carried out detailed theoretical analysis and provided solutions to both deterministic and probabilistic problem formulations. For deterministic problems, which assume that the measurement noise is negligible, we have provided solutions for both the over-determined (typically, when the number of scalar measurements is greater than the number of unknowns) and minimal problems (typically, when the number of scalar measurements equals the number of unknowns). The advantage of over-determined problems is that they are often easier to solve and their solutions can be computed very fast. However, in the presence of measurement outliers (incorrect measurements), minimal solvers are preferable. On the other hand, when the measurement noise cannot be ignored, we ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
