<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:   Minimum Sobolov Norm Methods</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/15/2008</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>299600.00</AwardTotalIntnAmount>
<AwardAmount>307600</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Dmitri Maslov</SignBlockName>
<PO_EMAI>dmaslov@nsf.gov</PO_EMAI>
<PO_PHON>7032928910</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Collaborative Research: Minimum Sobolev Norm Methods&lt;br/&gt;&lt;br/&gt;The aim of this research project is to design fast and accurate&lt;br/&gt;numerical algorithms for the solution of large classes of mathematical&lt;br/&gt;equations that arise in engineering and science. In particular, the&lt;br/&gt;main concerns are the solution of integro-differential equations on&lt;br/&gt;complex domains and of signal and image processing problems. The&lt;br/&gt;approach is based on formulating the estimate of the solution of the&lt;br/&gt;equation at a point as the value of the smoothest solution (on&lt;br/&gt;average) at that point based on the given data. The resulting discrete&lt;br/&gt;equations can be shown to have specially structured matrices, which&lt;br/&gt;can be exploited to create fast solvers for these equations. The&lt;br/&gt;resulting methods have two main computational advantages. First, they&lt;br/&gt;can be designed to avoid gridding or triangulation of the complex&lt;br/&gt;domain. Second, these methods exhibit local convergence; that is, the&lt;br/&gt;rate at which the approximant converges to the solution at a point&lt;br/&gt;depends only on the local smoothness of the solution. These advantages&lt;br/&gt;enable the method to tackle equations with complicated singularity&lt;br/&gt;structures with relative ease.&lt;br/&gt;&lt;br/&gt;Let Hs denote a Sobolev Hilbert space whose elements have s &gt; 1&lt;br/&gt;fractional derivatives. Suppose an unknown function f in Hs satisfies&lt;br/&gt;the equation L(F) = g, where L is a linear operator and g is a known&lt;br/&gt;function. Let Ln denote n linear functionals on Hr. Let q denote a&lt;br/&gt;linear functional on Hs. Then the best minmax estimate for q(f) can be&lt;br/&gt;computed from the minimum Sobolev norm function p in Hs that satisfies&lt;br/&gt;the constraints Ln(L(p)) = Ln(g). This p can be computed very rapidly&lt;br/&gt;since the optimal p is given by a nice set of equations that has Fast&lt;br/&gt;Multipole Method (FMM) structure when written in the proper&lt;br/&gt;representation. Also, it is possible to work with Lp Sobolev spaces&lt;br/&gt;with p = 1. In these cases the optimization problem is more&lt;br/&gt;complicated and can be reduced to linear programming problems, for&lt;br/&gt;which fast solvers are being developed that exploit the underlying FMM&lt;br/&gt;structure of the constraint matrix. The theoretical work consists of&lt;br/&gt;studying the convergence of the solution as n gets bigger, and also in&lt;br/&gt;proving the FMM structure of the resulting discrete equations. The&lt;br/&gt;algorithmic work consists of designing fast algorithms for&lt;br/&gt;constructing the FMM representation and then designing fast algorithms&lt;br/&gt;for the direct (non-iterative) solution of these equations. The&lt;br/&gt;application work consists of applying these ideas to image&lt;br/&gt;segmentation and multi-rate signal processing. Also, mesh free,&lt;br/&gt;locally convergent schemes are being developed for the solution of&lt;br/&gt;integral equations and elliptic partial differential equations on&lt;br/&gt;complex domains in two dimensions.</AbstractNarration>
<MinAmdLetterDate>08/29/2008</MinAmdLetterDate>
<MaxAmdLetterDate>08/11/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0830764</AwardID>
<Investigator>
<FirstName>Ming</FirstName>
<LastName>Gu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ming Gu</PI_FULL_NAME>
<EmailAddress>mgu@math.berkeley.edu</EmailAddress>
<PI_PHON>5106423145</PI_PHON>
<NSF_ID>000205573</NSF_ID>
<StartDate>08/29/2008</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Berkeley</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947101749</ZipCode>
<PhoneNumber>5106433891</PhoneNumber>
<StreetAddress>Sponsored Projects Office</StreetAddress>
<StreetAddress2><![CDATA[1608 Fourth Street, Suite 220]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>124726725</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>REGENTS OF THE UNIVERSITY OF CALIFORNIA, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Berkeley]]></Name>
<CityName>BERKELEY</CityName>
<StateCode>CA</StateCode>
<ZipCode>947101749</ZipCode>
<StreetAddress><![CDATA[Sponsored Projects Office]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000912</Code>
<Name>Computer Science</Name>
</FoaInformation>
<ProgramElement>
<Code>2865</Code>
<Text>NUMERIC, SYMBOLIC &amp; GEO COMPUT</Text>
</ProgramElement>
<ProgramElement>
<Code>7796</Code>
<Text>Algorithmic Foundations</Text>
</ProgramElement>
<ProgramElement>
<Code>7933</Code>
<Text>NUM, SYMBOL, &amp; ALGEBRA COMPUT</Text>
</ProgramElement>
<ProgramReference>
<Code>7752</Code>
<Text>CDI NON SOLICITED RESEARCH</Text>
</ProgramReference>
<ProgramReference>
<Code>9218</Code>
<Text>BASIC RESEARCH &amp; HUMAN RESORCS</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>HPCC</Code>
<Text>HIGH PERFORMANCE COMPUTING &amp; COMM</Text>
</ProgramReference>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2008~119462</FUND_OBLG>
<FUND_OBLG>2010~107252</FUND_OBLG>
<FUND_OBLG>2011~80886</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><ol> <li><strong>&nbsp;&nbsp;</strong><strong>Fast and Stable Structured Matrix Computations. </strong>Partly through our NSF-supported work, structured matrix computations have become a very active area of research. Many matrices arising from scientific computing applications are naturally &ldquo;structured&rdquo; as they can be characterized by far fewer parameters than the number of entries in the matrix. There are diverse forms of &ldquo;structures,&rdquo; such as sparse (matrices with very large number of zero entries) and Toeplitz (matrices that are constant along each diagonal.) Often these structures are closely related and can be exploited for very significant savings in computational and storage costs. We have developed new structured algorithms based a particular matrix structure: semi-separability. Loosely speaking, a semi-separable matrix is one whose off-diagonal submatrices all have relatively low rank in a given precision. Surprisingly large classes of matrices can be represented as semi-separable matrices and therefore allow fast matrix computations. Partly through our work, fast structured algorithms are now an accepted approach to solving many very large sparse linear systems of equations. &nbsp;</li> <li><strong>Communication-Avoiding Matrix Factorizations</strong>. On modern architectures, data communication has overtaken numerical computation as the dominant cost in large-scale matrix computations.&nbsp; Largely due to the work of Demmel and his colleagues, communication avoidance has become the central theme in current numerical linear algebra research. In this work, we consider the LU factorization and rank-revealing QR factorization algorithms. These algorithms are the workhorse in matrix computations and yet their practical performance can be far from optimal due to their excessive communication costs. In this work, we develop communication-avoiding versions of these algorithms that require orders of magnitude less communication. In both cases, we have to design the algorithms carefully so as to not create numerical instability while introducing new flows of numerical computation.&nbsp;</li> <li><strong>Randomized Algorithms </strong>A classical problem in matrix computations is the efficient and reliable approximation of a given matrix by a matrix of&nbsp; much lower rank, with applications throughout wide areas of computational sciences and engineering.&nbsp; This problem becomes especially important today, as huge data sets are routinely processed with low-rank approximation techniques. Among the different approaches in the literature for computing low- rank approximations, randomized algorithms have attracted much of researchers&rsquo; recent attention due to their surprising reliability and computational efficiency in different application areas. Typically, such algorithms are shown to compute, with very high probability, low-rank approximations that are within a constant factor from optimal, and are known to perform even better in many practical situations. In this work [S1], we point out a close connection between randomized algorithms and the classical subspace iteration method in numerical linear algebra. Based on this connection, we provide strong new insight into both randomized algorithms and the subspace iteration method. This insight further allows us to develop a new class of condition number estimators that are far more reliable than any in existence.</li> </ol> <p>&nbsp;</p><br> <p>            Last Modified: 08/22/2013<br>      Modified by: Ming&nbsp;Gu</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Fast and Stable Structured Matrix Computations. Partly through our NSF-supported work, structured matrix computations have become a very active area of research. Many matrices arising from scientific computing applications are naturally "structured" as they can be characterized by far fewer parameters than the number of entries in the matrix. There are diverse forms of "structures," such as sparse (matrices with very large number of zero entries) and Toeplitz (matrices that are constant along each diagonal.) Often these structures are closely related and can be exploited for very significant savings in computational and storage costs. We have developed new structured algorithms based a particular matrix structure: semi-separability. Loosely speaking, a semi-separable matrix is one whose off-diagonal submatrices all have relatively low rank in a given precision. Surprisingly large classes of matrices can be represented as semi-separable matrices and therefore allow fast matrix computations. Partly through our work, fast structured algorithms are now an accepted approach to solving many very large sparse linear systems of equations.   Communication-Avoiding Matrix Factorizations. On modern architectures, data communication has overtaken numerical computation as the dominant cost in large-scale matrix computations.  Largely due to the work of Demmel and his colleagues, communication avoidance has become the central theme in current numerical linear algebra research. In this work, we consider the LU factorization and rank-revealing QR factorization algorithms. These algorithms are the workhorse in matrix computations and yet their practical performance can be far from optimal due to their excessive communication costs. In this work, we develop communication-avoiding versions of these algorithms that require orders of magnitude less communication. In both cases, we have to design the algorithms carefully so as to not create numerical instability while introducing new flows of numerical computation.  Randomized Algorithms A classical problem in matrix computations is the efficient and reliable approximation of a given matrix by a matrix of  much lower rank, with applications throughout wide areas of computational sciences and engineering.  This problem becomes especially important today, as huge data sets are routinely processed with low-rank approximation techniques. Among the different approaches in the literature for computing low- rank approximations, randomized algorithms have attracted much of researchersÃ† recent attention due to their surprising reliability and computational efficiency in different application areas. Typically, such algorithms are shown to compute, with very high probability, low-rank approximations that are within a constant factor from optimal, and are known to perform even better in many practical situations. In this work [S1], we point out a close connection between randomized algorithms and the classical subspace iteration method in numerical linear algebra. Based on this connection, we provide strong new insight into both randomized algorithms and the subspace iteration method. This insight further allows us to develop a new class of condition number estimators that are far more reliable than any in existence.           Last Modified: 08/22/2013       Submitted by: Ming Gu]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
