<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Workshop on Supporting Scientific Discovery through Norms and Practices for Software and Data Citation and Attribution</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2014</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>99935.00</AwardTotalIntnAmount>
<AwardAmount>99935</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Daniel Katz</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Scientific researchers, and particularly academic researchers, are embedded in a reputation economy. Tenure, promotion, and acclaim are achieved through influential research. There are few incentives for scientists to share data and software, and tenure and promotion decisions lack consideration of such activities; and to compound the problem, there are disincentives such as risking the loss of attribution. Some scientists distrust the public access model for software and data, and prefer to share data and software only by personal request, which assures attribution through personal contact and implicit social contract. There is also a lack of well-developed metrics with which to assess the impact and quality of scientific software and data.  New practices and incentives are needed in the research community for software and data citation and attribution, so that data producers, software and tool developers, and data curators are credited for their contributions.&lt;br/&gt; &lt;br/&gt;This workshop will facilitate a national, interdisciplinary exploration of new norms and practices for software and data citation and attribution, with the goal of informing the Science of Science and Innovation Policy (SciSIP) and Software Infrastructure for Sustained Innovation (SI2) NSF programs. Social and technical challenges facing current software development and data generation efforts will be identified and participants will explore viable methods and metrics to support software and data attribution in the scientific research community. This workshop will address registration of software and data, repositories for software and data, methods for tracking software and data usage, software and data annotation, collecting and curating metadata on software and data, ensuring appropriate attribution by software and data users, alternatives to traditional publication models for attribution, adaptation of commercial models for software and data attribution, proportioning attribution metrics to match degree of effort and role in software development and data generation, and establishing reward metrics for open science. Workshop outcomes will include actionable plans to enable the broader research community to implement the software and data attribution practices that are identified and advanced by the participants of the workshop.</AbstractNarration>
<MinAmdLetterDate>08/18/2014</MinAmdLetterDate>
<MaxAmdLetterDate>08/18/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1448360</AwardID>
<Investigator>
<FirstName>Stanley</FirstName>
<LastName>Ahalt</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stanley Ahalt</PI_FULL_NAME>
<EmailAddress>ahalt@renci.org</EmailAddress>
<PI_PHON>9194459641</PI_PHON>
<NSF_ID>000241232</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Thomas</FirstName>
<LastName>Carsey</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Thomas M Carsey</PI_FULL_NAME>
<EmailAddress>carsey@unc.edu</EmailAddress>
<PI_PHON>9199663411</PI_PHON>
<NSF_ID>000498404</NSF_ID>
<StartDate>08/18/2014</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of North Carolina at Chapel Hill</Name>
<CityName>CHAPEL HILL</CityName>
<ZipCode>275991350</ZipCode>
<PhoneNumber>9199663411</PhoneNumber>
<StreetAddress>104 AIRPORT DR STE 2200</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<StateCode>NC</StateCode>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NC04</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>608195277</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NORTH CAROLINA AT CHAPEL HILL</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>142363428</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of North Carolina at Chapel Hill]]></Name>
<CityName>Chapel Hill</CityName>
<StateCode>NC</StateCode>
<ZipCode>275991350</ZipCode>
<StreetAddress><![CDATA[104 Airport Drive, Suite 2200]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>North Carolina</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>04</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NC04</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7626</Code>
<Text>SciSIP-Sci of Sci Innov Policy</Text>
</ProgramElement>
<ProgramElement>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramElement>
<ProgramElement>
<Code>8022</Code>
<Text>STAR Metrics</Text>
</ProgramElement>
<ProgramReference>
<Code>7433</Code>
<Text>CyberInfra Frmwrk 21st (CIF21)</Text>
</ProgramReference>
<ProgramReference>
<Code>7556</Code>
<Text>CONFERENCE AND WORKSHOPS</Text>
</ProgramReference>
<ProgramReference>
<Code>8004</Code>
<Text>Software Institutes</Text>
</ProgramReference>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2014~99935</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Software is as essential as data in the modern practice of science. When scientists share with each other not only research results, but also data and software, it vastly amplifies the reach, relevance, and transparency of science. Yet there are substantial social, systemic, and technological barriers that prevent scientists from sharing data and software. Scientific researchers &ndash; particularly academics &ndash; are embedded in a reputation economy in which tenure, promotion, and acclaim are achieved through influential research results. Tenure and promotion decisions are typically blind to a researcher&rsquo;s contributions to shared data or software, despite the crucial role of these activities in the scientific endeavor. Compounding the problem, there are no standard practices for citing data and software, giving appropriate credit to contributors, or measuring the impact and value of data and software contributions. Although numerous data and software sharing repositories exist, each uses a slightly different approach and many scientists still distrust the public access model, preferring to share data and software only by personal request, which assures attribution through personal contact and implicit social contract but substantially limits the reach and benefit of shared data and software.</p> <p>The research community urgently needs new practices and incentives to ensure data producers, software and tool developers, and data curators are credited for their contributions. This National Science Foundation (NSF)-sponsored workshop facilitated a national, interdisciplinary discussion and exploration of new norms and practices for software and data citation and attribution to inform the Software Infrastructure for Sustained Innovation (SI2) and Science of Science and Innovation Policy (SciSIP) NSF programs. Participants identified social and technical challenges facing current software development and data generation efforts and explored viable methods and metrics to support software and data attribution in the scientific research community. A consensus throughout the workshop was a strong sentiment that it is time to move beyond discussion of the issues and begin to establish pilot projects that endeavor to implement and experiment with actionable ideas. Highlights among the list of actionable plans discussed at the workshop include:</p> <ul> <li>Request that publishers and repositories interlink      their platforms and processes so that article references and data set or      software citations cross-reference each other.</li> <li>Request that the research community develop a primary      consistent data and software citation record format to support data and      software citation.</li> <li>Request that an organization (as yet unidentified)      develop guidelines for trusted software repositories for science (similar      to trusted digital data repositories).</li> <li>Ask federal funding agencies to require every Principal      Investigator (PI) to have a permanent human identifier (e.g., ORCID, which      resolves critical issues of identifying individuals).</li> <li>Data and software repository landing pages should      describe the full provenance of the data using appropriate standards.</li> <li>Authors should be able to cite data and software in      their articles at an appropriate level of granularity.&nbsp;</li> <li>Federal funding agencies should support an effort to      convene key players to identify and harmonize standards on roles,      attribution, value, and transitive credit (in an extensible framework).      All key sponsors would be recognized.&nbsp;</li> <li>Agencies, publishers, societies, and foundations should      fund implementation grants to identify and measure data and software      impacts in a way that is relevant to stakeholders and research      communities.</li> </ul> <p>&nbsp;</p><br> <p>            Last Modified: 10/...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Software is as essential as data in the modern practice of science. When scientists share with each other not only research results, but also data and software, it vastly amplifies the reach, relevance, and transparency of science. Yet there are substantial social, systemic, and technological barriers that prevent scientists from sharing data and software. Scientific researchers &ndash; particularly academics &ndash; are embedded in a reputation economy in which tenure, promotion, and acclaim are achieved through influential research results. Tenure and promotion decisions are typically blind to a researcherÃ†s contributions to shared data or software, despite the crucial role of these activities in the scientific endeavor. Compounding the problem, there are no standard practices for citing data and software, giving appropriate credit to contributors, or measuring the impact and value of data and software contributions. Although numerous data and software sharing repositories exist, each uses a slightly different approach and many scientists still distrust the public access model, preferring to share data and software only by personal request, which assures attribution through personal contact and implicit social contract but substantially limits the reach and benefit of shared data and software.  The research community urgently needs new practices and incentives to ensure data producers, software and tool developers, and data curators are credited for their contributions. This National Science Foundation (NSF)-sponsored workshop facilitated a national, interdisciplinary discussion and exploration of new norms and practices for software and data citation and attribution to inform the Software Infrastructure for Sustained Innovation (SI2) and Science of Science and Innovation Policy (SciSIP) NSF programs. Participants identified social and technical challenges facing current software development and data generation efforts and explored viable methods and metrics to support software and data attribution in the scientific research community. A consensus throughout the workshop was a strong sentiment that it is time to move beyond discussion of the issues and begin to establish pilot projects that endeavor to implement and experiment with actionable ideas. Highlights among the list of actionable plans discussed at the workshop include:  Request that publishers and repositories interlink      their platforms and processes so that article references and data set or      software citations cross-reference each other. Request that the research community develop a primary      consistent data and software citation record format to support data and      software citation. Request that an organization (as yet unidentified)      develop guidelines for trusted software repositories for science (similar      to trusted digital data repositories). Ask federal funding agencies to require every Principal      Investigator (PI) to have a permanent human identifier (e.g., ORCID, which      resolves critical issues of identifying individuals). Data and software repository landing pages should      describe the full provenance of the data using appropriate standards. Authors should be able to cite data and software in      their articles at an appropriate level of granularity.  Federal funding agencies should support an effort to      convene key players to identify and harmonize standards on roles,      attribution, value, and transitive credit (in an extensible framework).      All key sponsors would be recognized.  Agencies, publishers, societies, and foundations should      fund implementation grants to identify and measure data and software      impacts in a way that is relevant to stakeholders and research      communities.           Last Modified: 10/23/2015       Submitted by: Stanley Ahalt]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
