<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Evolutionary Data Systems</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>02/15/2015</AwardEffectiveDate>
<AwardExpirationDate>01/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>550000.00</AwardTotalIntnAmount>
<AwardAmount>550000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Maria Zemankova</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>We are entering the era of data deluge. Businesses, sciences and government organizations collect overwhelming amounts of data on a daily or even hourly basis. But data becomes useful information (knowledge) only after proper analysis. Data systems drive the analysis of data by servicing query requests. The way a system stores, accesses and processes data, defines the architecture of the data system. Over the past five decades the database research community and industry have designed numerous data systems architectures; a fundamental problem now is that there is no single data systems architecture that fits the ever-increasing kinds of data driven applications and scenarios. This is a critical problem for numerous reasons. First, anyone in need of a data system is confronted with numerous complex options, e.g., relational databases, NoSQL, NewSQL, column-stores, row-stores, and the list goes on. This overwhelming array of choices makes bootstrapping data-driven applications difficult and time consuming, requiring expertise often not accessible due to cost issues (e.g., to sciences or small businesses). Furthermore, many new kinds of data-driven applications are characterized by (a) varying workload patterns, i.e., varying patterns in data and queries and (b) new requirements in terms of interactive query processing and data exploration, i.e., when in need to quickly find useful knowledge in a big pile of data without a predefined exact goal. What these cases have in common is that one cannot tell a priori which data system architecture is the best fit as there is no predefined or fixed workload. An application may change over time and then a different architecture will be best. In short, this is a "one size does not fit all world" which sacrifices flexibility for performance.  However, as sciences and businesses increase their ability to collect more data and thus create more data-driven applications and scenarios, this model does not scale. The goal of this proposal is to maintain the performance characteristics of the "one size does not fit all" world, while adding the flexibility to effortlessly support emerging applications and varying workloads. &lt;br/&gt;&lt;br/&gt;The researchers will study a new class of database architectures, evolutionary data systems, which evolve continuously to match the application's needs. The way data is stored and the way data is accessed changes to accommodate the data and query patterns. There is no need to make a priori complex set-up decisions, locking an application to a fixed system architecture that may soon become suboptimal due to workload changes. Instead, one starts using an evolutionary system simply by identifying the data. Then, as new data and queries come in, the system evolves such that its architecture matches the properties of the incoming workload. At any given point in time, an evolutionary system may employ multiple competing solutions down at the low level of database architectures such as using various combinations of data layouts, access methods and execution strategies. Then "the fittest wins" and becomes the dominant architecture until the environment (workload) changes. The researchers will demonstrate solutions that can seamlessly evolve (back and forth) between different architectures, i.e., from key-value stores to column-stores, from read-optimized to write-optimized systems and from structured to schema-free systems. In addition, the researchers will develop solutions for several key challenges: how systems evolve, management of overheads, how to extend an evolutionary system as well as comparison with brute force approaches.&lt;br/&gt;&lt;br/&gt;For further information see the project web site at:  http://daslab.seas.harvard.edu/evosys</AbstractNarration>
<MinAmdLetterDate>01/23/2015</MinAmdLetterDate>
<MaxAmdLetterDate>01/31/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1452595</AwardID>
<Investigator>
<FirstName>Stratos</FirstName>
<LastName>Idreos</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Stratos Idreos</PI_FULL_NAME>
<EmailAddress>stratos@seas.harvard.edu</EmailAddress>
<PI_PHON>6174955501</PI_PHON>
<NSF_ID>000669659</NSF_ID>
<StartDate>01/23/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard SEAS]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021382933</ZipCode>
<StreetAddress><![CDATA[33 Oxford Street, 139 MD]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7364</Code>
<Text>Info Integration &amp; Informatics</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7364</Code>
<Text>INFO INTEGRATION &amp; INFORMATICS</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~100883</FUND_OBLG>
<FUND_OBLG>2016~110574</FUND_OBLG>
<FUND_OBLG>2017~114600</FUND_OBLG>
<FUND_OBLG>2018~109971</FUND_OBLG>
<FUND_OBLG>2019~113972</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><div class="page" title="Page 2"> <div class="layoutArea"> <div class="column"> <p><span>What do analytics, machine learning, data science, and big data systems have in common? What is the major common component for astronomy, biology, neuroscience, and all other data-driven and computational sciences? Data structures.</span></p> <div class="page" title="Page 2"> <div class="layoutArea"> <div class="column"> <p><span>Data structures are one of the most fundamental areas of computer science.&nbsp;</span>Since the early days of computer science, dozens of new data structures are published every year. The pace has increased over the last decade, with 50-80 new data structures yearly. This is because of 1) the growth of data, 2) the increasing number of data-driven applications, 3) more fields moving to a computational paradigm where data collection, storage, and analysis become critical, and 4) hardware changes that require a complete redesign of data structures and algorithms.&nbsp;</p> <div class="page" title="Page 3"> <div class="layoutArea"> <div class="column"> <p><span>There are so many di</span><span>ff</span><span>erent ways to design a data structure and so many moving targets that it has become a notoriously hard problem; it takes several months even for experts to design and optimize a new structure.&nbsp;</span>For data-driven fields without computer science expertise, these low-level choices are impossible to make. The only viable solution is using suboptimal off-the-shelf designs or hiring expensive experts. Data science pipelines in astronomy, biology, neuroscience, chemistry, and other emerging data-driven scientific fields exhibit exactly those characteristics. Similarly, for both established companies and data-driven startups, this complexity leads to a slow design process and has severe cost side-effects. Time to market is of extreme importance, so data structure design stops when a design &ldquo;is due&rdquo; and only rarely when it &ldquo;is ready&rdquo;. Finally, in today&rsquo;s cloud-based world even slightly sub-optimal designs, e.g., by 1%, translate to a massive loss in energy utilization for the cloud provider and cloud expenses for the users/applications.</p> <p>Our work in this project takes a fundamentally different approach to data structure design. There are four critical outcomes:&nbsp;</p> <p>1) We introduced a design engine, the Data Calculator, which enables interactive and semi-automated design of data structures. It brings two innovations. First, it offers a set of fine-grained design primitives that capture the first principles of data layout design: how data structure nodes lay data out, and how they are positioned relative to each other. This allows for a structured description of the universe of possible data structure designs that can be synthesized as combinations of those primitives. The second innovation is the computation of performance using learned cost models. These models are trained on diverse hardware and data profiles and capture the cost properties of fundamental data access primitives. With these models, we synthesize the performance cost of complex operations on arbitrary data structure designs without having to: 1) implement the data structure, 2) run the workload, or even 3) access the target hardware.&nbsp;</p> <p>2) We organized the existing and possible design space of data structures into the Periodic Table of Data Structures. This can be used as a tool to accelerate research (similarly to the periodic table in Chemistry).&nbsp;</p> <div class="page" title="Page 1"> <div class="section"> <div class="layoutArea"> <div class="column"> <p><span>3) To accelerate automated search for the best possible design among the vast space of data structure designs formed by the first principles, we introduced the concept of design continuums</span>. A design continuum unifies major distinct data structure designs under the same model. The critical insight and potential long-term impact is that such unifying models 1) render what we consider up to now as fundamentally different data structures to be seen as &ldquo;views&rdquo; of the very same overall design space, and 2) allow &ldquo;seeing&rdquo; new data structure designs with performance properties that are not feasible by existing designs. We show how to construct, evaluate, and expand, design continuums, and we also present the first continuum that unifies major data structure designs.</p> <p>4) Using the formalization of the design space, we have discovered a series of new data structure designs for NoSQL stores, which are critical for a massive number of everyday applications in science and business.&nbsp;</p> </div> </div> </div> </div> <p>Overall, we demonstrate that the formalization of the design space of data structures and the Data Calculator can assist data structure designers and researchers by accurately answering rich what-if design questions on the order of a few seconds or minutes, i.e., computing how the performance (response time) of a given data structure design is impacted by variations in the: 1) design, 2) hardware, 3) data, and 4) query workloads. This makes it effortless to test numerous designs and ideas before embarking on lengthy implementation, deployment, and hardware acquisition steps. We also demonstrate that the Data Calculator can synthesize entirely new designs, auto-complete partial designs, and detect suboptimal design choices.&nbsp;</p> <p>&nbsp;</p> </div> </div> </div> </div> </div> </div> <p><span><br /></span></p> </div> </div> </div><br> <p>            Last Modified: 03/13/2020<br>      Modified by: Stratos&nbsp;Idreos</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    What do analytics, machine learning, data science, and big data systems have in common? What is the major common component for astronomy, biology, neuroscience, and all other data-driven and computational sciences? Data structures.     Data structures are one of the most fundamental areas of computer science. Since the early days of computer science, dozens of new data structures are published every year. The pace has increased over the last decade, with 50-80 new data structures yearly. This is because of 1) the growth of data, 2) the increasing number of data-driven applications, 3) more fields moving to a computational paradigm where data collection, storage, and analysis become critical, and 4) hardware changes that require a complete redesign of data structures and algorithms.      There are so many different ways to design a data structure and so many moving targets that it has become a notoriously hard problem; it takes several months even for experts to design and optimize a new structure. For data-driven fields without computer science expertise, these low-level choices are impossible to make. The only viable solution is using suboptimal off-the-shelf designs or hiring expensive experts. Data science pipelines in astronomy, biology, neuroscience, chemistry, and other emerging data-driven scientific fields exhibit exactly those characteristics. Similarly, for both established companies and data-driven startups, this complexity leads to a slow design process and has severe cost side-effects. Time to market is of extreme importance, so data structure design stops when a design "is due" and only rarely when it "is ready". Finally, in todayâ€™s cloud-based world even slightly sub-optimal designs, e.g., by 1%, translate to a massive loss in energy utilization for the cloud provider and cloud expenses for the users/applications.  Our work in this project takes a fundamentally different approach to data structure design. There are four critical outcomes:   1) We introduced a design engine, the Data Calculator, which enables interactive and semi-automated design of data structures. It brings two innovations. First, it offers a set of fine-grained design primitives that capture the first principles of data layout design: how data structure nodes lay data out, and how they are positioned relative to each other. This allows for a structured description of the universe of possible data structure designs that can be synthesized as combinations of those primitives. The second innovation is the computation of performance using learned cost models. These models are trained on diverse hardware and data profiles and capture the cost properties of fundamental data access primitives. With these models, we synthesize the performance cost of complex operations on arbitrary data structure designs without having to: 1) implement the data structure, 2) run the workload, or even 3) access the target hardware.   2) We organized the existing and possible design space of data structures into the Periodic Table of Data Structures. This can be used as a tool to accelerate research (similarly to the periodic table in Chemistry).       3) To accelerate automated search for the best possible design among the vast space of data structure designs formed by the first principles, we introduced the concept of design continuums. A design continuum unifies major distinct data structure designs under the same model. The critical insight and potential long-term impact is that such unifying models 1) render what we consider up to now as fundamentally different data structures to be seen as "views" of the very same overall design space, and 2) allow "seeing" new data structure designs with performance properties that are not feasible by existing designs. We show how to construct, evaluate, and expand, design continuums, and we also present the first continuum that unifies major data structure designs.  4) Using the formalization of the design space, we have discovered a series of new data structure designs for NoSQL stores, which are critical for a massive number of everyday applications in science and business.       Overall, we demonstrate that the formalization of the design space of data structures and the Data Calculator can assist data structure designers and researchers by accurately answering rich what-if design questions on the order of a few seconds or minutes, i.e., computing how the performance (response time) of a given data structure design is impacted by variations in the: 1) design, 2) hardware, 3) data, and 4) query workloads. This makes it effortless to test numerous designs and ideas before embarking on lengthy implementation, deployment, and hardware acquisition steps. We also demonstrate that the Data Calculator can synthesize entirely new designs, auto-complete partial designs, and detect suboptimal design choices.                       Last Modified: 03/13/2020       Submitted by: Stratos Idreos]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
