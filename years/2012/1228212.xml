<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Data-Driven Inverse Sensitivity Analysis for Predictive Coastal Ocean Modeling</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2012</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>189647.00</AwardTotalIntnAmount>
<AwardAmount>189647</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Andrew Pollington</SignBlockName>
<PO_EMAI>adpollin@nsf.gov</PO_EMAI>
<PO_PHON>7032924878</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The goal of this project is to improve the predictive capabilities of computational models of the coastal ocean, by combining a novel measure-theoretic approach for inverse sensitivity with experimental data.  Advanced computer models of the coastal ocean, such as the Advanced Circulation (ADCIRC) model, can be used in predictive mode to estimate storm surge as hurricanes approach landfall for the purposes of emergency evacuation and response.  However, the accuracy of ADCIRC, and other computer models, relies on the painstaking process of model calibration based on uncertain input parameters.  The investigators study the estimation and model sensitivity for certain critical parameters, in particular bathymetry, bottom friction, and wind stress.  Applying the solution to the inverse problem for prediction is complicated by two issues.  First, the map from the input data and parameter space to the observable space generally reduces the dimension which implies the inverse problem has set-valued solutions.  Second, even though the models considered in this project provide deterministic physical descriptions, all of the data available is subject to natural stochastic variability as well as experimental/observational error and uncertainty generally described stochastically.  The measure-theoretic algorithm computes a probability measure over the entire parameter space from which an ensemble of model selections may be chosen to deliver reliable predictions of critical quantities of interest such as maximum water elevation along the coast.  The PIs study various mathematical issues including estimation of various sources of error inherent in a non-intrusive implementation of the measure-theoretic approach.  The use of experimental data and the ADCIRC model creates a unique opportunity for verification and validation of proposed methods.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Quantitative predictions of coastal ocean conditions is central to long-range studies of coastal sustainability, the development of priorities and policies for the restoration and maintenance of coastal ecosystems, enhancing the economic vitality of coastal communities, and assessing risk of coastal populations to natural disasters.  While coastal predictions of various complexity have been under development and used routinely for decades now, a series of events over the past seven years has driven a revolution.  Namely, Hurricane Katrina (2005), in devastating fashion, demonstrated the perils of underestimating the vulnerability of coastal communities to storm surge.  Following on the heels of Katrina were hurricanes Rita (2005), Gustav (2008) and Ike (2008), which all caused tremendous damage to communities along the northern Gulf of Mexico, and more recently the Deepwater Horizon Oil Spill, which occurred off the coast of Louisiana and threatened the entire Gulf ecosystem.  These events spurred a serious and sustained effort to improve the ability to predict coastal ocean conditions.  However, the prediction of coastal conditions beyond what can be observed, e.g. predicting future maximum storm surge from current and near past coastal observation data in real-time, is an exceedingly challenging mathematical, statistical, and computational problem.  In this project, the investigators study and apply state-of-the-art techniques in order to improve the predictive capabilities of coastal ocean models used to predict storm surge.  The computational methodology and tools developed under this project are applicable to other problems in coastal engineering, marine science, material science and other engineering disciplines. Technology transfer of the mathematical and numerical methodologies developed under this project will occur with the coastal ocean modeling community, and with agencies such as the U.S. Army Corps of Engineers, NOAA, the Department of Homeland Security, state and local agencies, industry, and other universities in the U.S. and abroad.</AbstractNarration>
<MinAmdLetterDate>08/31/2012</MinAmdLetterDate>
<MaxAmdLetterDate>08/31/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1228212</AwardID>
<Investigator>
<FirstName>Joannes</FirstName>
<LastName>Westerink</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Joannes J Westerink</PI_FULL_NAME>
<EmailAddress>jjw@nd.edu</EmailAddress>
<PI_PHON>5746316475</PI_PHON>
<NSF_ID>000181253</NSF_ID>
<StartDate>08/31/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Notre Dame</Name>
<CityName>NOTRE DAME</CityName>
<ZipCode>465565708</ZipCode>
<PhoneNumber>5746317432</PhoneNumber>
<StreetAddress>940 Grace Hall</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<StateCode>IN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>IN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>824910376</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF NOTRE DAME DU LAC</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>048994727</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Notre Dame]]></Name>
<CityName/>
<StateCode>IN</StateCode>
<ZipCode>465565602</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Indiana</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>IN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~189647</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="western">Uncertainty plays an important role in our ability to make scientific predictions with physics-based computational models.  There is often uncertainty associated with the accuracy of measured data that describes or drives the physical system and in the measured responses. In the context of the storm-surge problem considered in this project, these inputs include ocean depth measurements, land and ocean bottom cover and the associated frictional resistance , wind data, air-sea momentum transfer, and the available measured water surface elevations, wave conditions, currents, and inundation extent of the floodplain. Often, scientists and engineers have a limited ability to determine if the input and comparison data uncertainty is the dominant source of overall prediction error for a given simulation.  Therefore, it is important to develop tools to both quantify the impact of the uncertainty and understand how it interacts with the model physics. The strategy for accomplishing this centers around solving the inverse problem, i.e. using the output predictions of the model to determine the inputs.  Since this cannot be done directly for complex problems, it requires a large number (thousands) of model simulations, each with varied inputs, to be performed in order to infer how changes in the inputs effect the predicted result.  This means large amounts of computing resources (thousands of CPUs) need to be utilized for long periods of time (several hours per run) for today&rsquo;s most accurate models.  As a result, the efficiency of the code used to perform the simulations is vital to the feasibility of solving the inverse problem.  Our group has worked to improve the efficiency of the ocean circulation code suite ADCIRC, which is used to provide storm surge inundation predictions so it can be used in conjunction with uncertainty quantification techniques.  We have focused on three areas: implementation of high-order accurate solution methods, code restructuring, and improvements to the parallel performance of the code.</p> <p class="western">&nbsp;</p> <p class="western">The first area refers to researching the application of discontinuous Galerkin (DG) finite element methods to storm surge prediction problems.  This algorithm allows for solutions to be computed with greater numerical accuracy than what is currently achievable in ADCIRC.  This means larger element sizes can be used to provide results with the same level of numerical error.  Since using small element sizes leads to greater computational expense, the ability of the DG method to use large elements with improved accuracy allows for significant efficiency gains.  However, the greater accuracy also means more attention must be paid to the way in which certain aspects of the model domain are described because these larger elements cannot capture small-scale details as effectively as finer elements.  Our main focus has been to remedy this through the incorporation of curved element edge along the domain boundaries as well as developing methods to more accurately model the variation of the depth within the element.</p> <p class="western">&nbsp;</p> <p class="western">We have also worked to restructure the computations in the code to take advantage of the advancements in CPU technology.  Today's computer chips have specialized units which can preform the same calculation on several values at once.  These chips also have improved performance when calculations involving the same data are grouped as close together as possible.  To fully benefit from these factors, our DG code structure was reorganized to improve the efficiency of the calculations.</p> <p class="western">&nbsp;</p> <p class="western">Another consideration for increasing the computational efficiency of the DG method is to improve it's performance on parallel HPC systems.  This means ensuring that as little information as possible is comm...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Uncertainty plays an important role in our ability to make scientific predictions with physics-based computational models.  There is often uncertainty associated with the accuracy of measured data that describes or drives the physical system and in the measured responses. In the context of the storm-surge problem considered in this project, these inputs include ocean depth measurements, land and ocean bottom cover and the associated frictional resistance , wind data, air-sea momentum transfer, and the available measured water surface elevations, wave conditions, currents, and inundation extent of the floodplain. Often, scientists and engineers have a limited ability to determine if the input and comparison data uncertainty is the dominant source of overall prediction error for a given simulation.  Therefore, it is important to develop tools to both quantify the impact of the uncertainty and understand how it interacts with the model physics. The strategy for accomplishing this centers around solving the inverse problem, i.e. using the output predictions of the model to determine the inputs.  Since this cannot be done directly for complex problems, it requires a large number (thousands) of model simulations, each with varied inputs, to be performed in order to infer how changes in the inputs effect the predicted result.  This means large amounts of computing resources (thousands of CPUs) need to be utilized for long periods of time (several hours per run) for todayÃ†s most accurate models.  As a result, the efficiency of the code used to perform the simulations is vital to the feasibility of solving the inverse problem.  Our group has worked to improve the efficiency of the ocean circulation code suite ADCIRC, which is used to provide storm surge inundation predictions so it can be used in conjunction with uncertainty quantification techniques.  We have focused on three areas: implementation of high-order accurate solution methods, code restructuring, and improvements to the parallel performance of the code.   The first area refers to researching the application of discontinuous Galerkin (DG) finite element methods to storm surge prediction problems.  This algorithm allows for solutions to be computed with greater numerical accuracy than what is currently achievable in ADCIRC.  This means larger element sizes can be used to provide results with the same level of numerical error.  Since using small element sizes leads to greater computational expense, the ability of the DG method to use large elements with improved accuracy allows for significant efficiency gains.  However, the greater accuracy also means more attention must be paid to the way in which certain aspects of the model domain are described because these larger elements cannot capture small-scale details as effectively as finer elements.  Our main focus has been to remedy this through the incorporation of curved element edge along the domain boundaries as well as developing methods to more accurately model the variation of the depth within the element.   We have also worked to restructure the computations in the code to take advantage of the advancements in CPU technology.  Today's computer chips have specialized units which can preform the same calculation on several values at once.  These chips also have improved performance when calculations involving the same data are grouped as close together as possible.  To fully benefit from these factors, our DG code structure was reorganized to improve the efficiency of the calculations.   Another consideration for increasing the computational efficiency of the DG method is to improve it's performance on parallel HPC systems.  This means ensuring that as little information as possible is communicated between each CPU involved in the calculation, since it is much slower to move information across the network than to do the computations themselves.   Therefore, we have also developed a way of overlapping some of the calculation...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
