<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CC-NIE Network Infrastructure: Advanced Connectivity for Texas A&amp;M University</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2012</AwardEffectiveDate>
<AwardExpirationDate>09/30/2014</AwardExpirationDate>
<AwardTotalIntnAmount>499609.00</AwardTotalIntnAmount>
<AwardAmount>499609</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Kevin Thompson</SignBlockName>
<PO_EMAI>kthompso@nsf.gov</PO_EMAI>
<PO_PHON>7032924220</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Texas A&amp;M University is establishing and exploiting advanced 100-Gb/s connectivity, focused on accessing Internet2's OS3E network for its researchers. It is furthering data-intensive discipline research and network research, and is contributing to national best practices and to advanced connectivity in support of data-intensive research throughout the LEARN community in Texas.&lt;br/&gt;&lt;br/&gt;The chief objective is to enhance the ability of A&amp;M researchers to engage in data-intensive science, including combined use of campus and national cyberinfrastructure and large data sets. Similarly, routinely using large VLAN-based, wide-area data flows will challenge Texas A&amp;M to grow technically. One example is the integration of these techniques into the LHC "data moving" area.&lt;br/&gt;&lt;br/&gt;A second example is exploring federated wide-area file systems as an alternative to the classic "data moving" paradigm, including performance and federated identity issues crucial to the success of this approach. This is a contribution to realizing the evolving Campus Bridging vision. In support of computational scientists at Texas A&amp;M, the project leverages connectivity with the Immersive Visualization Center to enable effective forms of local and remote visualization. Finally, the project enables network researchers to use the OpenFlow-based services of Internet2's OS3E as it evolves.&lt;br/&gt;&lt;br/&gt;The advanced connectivity, combined with Texas A&amp;M's ScienceDMZ, enables emerging sets of data-intensive scientists in areas that have not yet become prominent, destroying physical remoteness as a barrier to engaging in collaborative data-intensive science.</AbstractNarration>
<MinAmdLetterDate>09/06/2012</MinAmdLetterDate>
<MaxAmdLetterDate>09/06/2012</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1246097</AwardID>
<Investigator>
<FirstName>Guy</FirstName>
<LastName>Almes</LastName>
<PI_MID_INIT>T</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Guy T Almes</PI_FULL_NAME>
<EmailAddress>galmes@vprmail.tamu.edu</EmailAddress>
<PI_PHON>9798623982</PI_PHON>
<NSF_ID>000473910</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Pierce</FirstName>
<LastName>Cantrell</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME>Jr</PI_SUFX_NAME>
<PI_FULL_NAME>Pierce E Cantrell</PI_FULL_NAME>
<EmailAddress>p-cantrell@tamu.edu</EmailAddress>
<PI_PHON>9798453719</PI_PHON>
<NSF_ID>000374074</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Willis</FirstName>
<LastName>Marti</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Willis Marti</PI_FULL_NAME>
<EmailAddress>wmarti@tamu.edu</EmailAddress>
<PI_PHON>9798626450</PI_PHON>
<NSF_ID>000621257</NSF_ID>
<StartDate>09/06/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Texas A&amp;M University</Name>
<CityName>College Station</CityName>
<ZipCode>778454375</ZipCode>
<PhoneNumber>9798626777</PhoneNumber>
<StreetAddress>400 Harvey Mitchell Pkwy South</StreetAddress>
<StreetAddress2><![CDATA[Suite 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX17</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>020271826</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TEXAS A &amp; M UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042915991</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Texas A&M University Main Campus]]></Name>
<CityName/>
<StateCode>TX</StateCode>
<ZipCode>778431260</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>17</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX17</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1640</Code>
<Text>Information Technology Researc</Text>
</ProgramElement>
<ProgramElement>
<Code>8080</Code>
<Text>Campus Cyberinfrastructure</Text>
</ProgramElement>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~499609</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Texas A&amp;M University has built advanced 100 Gigabit per second (Gbps) computer network infrastructure to support data-intensive science and engineering research as well as computer networking research. We have connected the 100 Gbps network to a separate high-speed network on campus, known as a ScienceDMZ, which provides a &ldquo;fast-lane&rdquo; to the 100 Gbps off-campus network with 10 to 40 Gbps connections to researcher laboratories and campus cyberinfrastructure (e.g., High-Performance Computing (HPC) clusters, immersive visualization center, and specialized instruments). The ScienceDMZ also supports Software Defined Networking (SDN) protocols.</p> <p>Texas A&amp;M researchers in many fields of science and engineering utilize national high-performance computing resources, unique research infrastructure on- and off-campus, or sensor networks that require moving large amounts of data to and from campus. For example, our experimental particle physics group participates in the Compact Muon Solenoid (CMS) experiment, which is located at the Large Hadron Collider at CERN in Switzerland, and they are expecting to transfer 100 Terabytes (TB)/month of CMS processed data from CERN to campus in the coming years (1 Terabyte = 1,000 Gigabytes). Similarly, the Texas Center for Climate Studies has a research project in climate modeling that generated 60 TB of data on the NSF-funded Stampede supercomputer, located at the Texas Advanced Computer Center (TACC) at the University of Texas at Austin, which needed to be moved back to the A&amp;M campus for further analysis. Prior to the advanced networking infrastructure built as part of this grant, it took 30 days to transfer this data over the network. When high-performance computer interfaces and high-speed file transfer software are installed in the next few months, we expect this same transfer to take just a few hours. There are also instruments on campus such as the new Cryo Electron Microscope that generate large images that need to transferred to systems on- and off-campus, and the Synchrophasor Test Bed that is under development by the TEES SmartGrid Center will need to access real-time synchrophasor electric power grid data both nationally and internationally. The Synchrophasor Test Bed will also need to access A&amp;M supercomputers for real-time computations.</p> <p>Texas A&amp;M has computer networking research that will utilize the new networking infrastructure as well. Some of the networking research is data-intensive, such as the collection of data from millions of websites or the development of "big data" tools for extracting, mining, and visualizing data. Other networking research in high-performance networking protocols, SDN applications, cloud computing architectures that support large file transfers, or utilize the ExoGeni network will also make use of the new infrastructure.</p> <p>Utilizing the Lonestar Education and Research Network (LEARN) regional optical network, a 100 Gbps circuit was provisioned from the main campus in College Station, Texas to the LEARN/Internet2 Point-of-Presence in Houston, Texas, where it connects to Internet2&rsquo;s Advanced Layer 2/3 networks. As the first recipient of a NSF CC-NIE grant in Texas, we worked to build consensus among LEARN members for a strategy on connecting to Internet2 at 100 Gbps in both Houston and Dallas, which allows all participating LEARN members to benefit from the 10X increase in bandwidth, shares the cost among the users, and provides redundancy and performance benefits. The 2 X 100 Gbps Internet2 connections are also important to the other four NSF CC-NIE/CC*IIE grant recipients in Texas. Internet2 operates a 100 Gbps Research and Education (R&amp;E) network that connects to more than 250 U.S. universities as well as corporate and government R&amp;D laboratories. Internet2 also connects globally to more than 65 international R&amp;E partners that ...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Texas A&amp;M University has built advanced 100 Gigabit per second (Gbps) computer network infrastructure to support data-intensive science and engineering research as well as computer networking research. We have connected the 100 Gbps network to a separate high-speed network on campus, known as a ScienceDMZ, which provides a "fast-lane" to the 100 Gbps off-campus network with 10 to 40 Gbps connections to researcher laboratories and campus cyberinfrastructure (e.g., High-Performance Computing (HPC) clusters, immersive visualization center, and specialized instruments). The ScienceDMZ also supports Software Defined Networking (SDN) protocols.  Texas A&amp;M researchers in many fields of science and engineering utilize national high-performance computing resources, unique research infrastructure on- and off-campus, or sensor networks that require moving large amounts of data to and from campus. For example, our experimental particle physics group participates in the Compact Muon Solenoid (CMS) experiment, which is located at the Large Hadron Collider at CERN in Switzerland, and they are expecting to transfer 100 Terabytes (TB)/month of CMS processed data from CERN to campus in the coming years (1 Terabyte = 1,000 Gigabytes). Similarly, the Texas Center for Climate Studies has a research project in climate modeling that generated 60 TB of data on the NSF-funded Stampede supercomputer, located at the Texas Advanced Computer Center (TACC) at the University of Texas at Austin, which needed to be moved back to the A&amp;M campus for further analysis. Prior to the advanced networking infrastructure built as part of this grant, it took 30 days to transfer this data over the network. When high-performance computer interfaces and high-speed file transfer software are installed in the next few months, we expect this same transfer to take just a few hours. There are also instruments on campus such as the new Cryo Electron Microscope that generate large images that need to transferred to systems on- and off-campus, and the Synchrophasor Test Bed that is under development by the TEES SmartGrid Center will need to access real-time synchrophasor electric power grid data both nationally and internationally. The Synchrophasor Test Bed will also need to access A&amp;M supercomputers for real-time computations.  Texas A&amp;M has computer networking research that will utilize the new networking infrastructure as well. Some of the networking research is data-intensive, such as the collection of data from millions of websites or the development of "big data" tools for extracting, mining, and visualizing data. Other networking research in high-performance networking protocols, SDN applications, cloud computing architectures that support large file transfers, or utilize the ExoGeni network will also make use of the new infrastructure.  Utilizing the Lonestar Education and Research Network (LEARN) regional optical network, a 100 Gbps circuit was provisioned from the main campus in College Station, Texas to the LEARN/Internet2 Point-of-Presence in Houston, Texas, where it connects to Internet2Ã†s Advanced Layer 2/3 networks. As the first recipient of a NSF CC-NIE grant in Texas, we worked to build consensus among LEARN members for a strategy on connecting to Internet2 at 100 Gbps in both Houston and Dallas, which allows all participating LEARN members to benefit from the 10X increase in bandwidth, shares the cost among the users, and provides redundancy and performance benefits. The 2 X 100 Gbps Internet2 connections are also important to the other four NSF CC-NIE/CC*IIE grant recipients in Texas. Internet2 operates a 100 Gbps Research and Education (R&amp;E) network that connects to more than 250 U.S. universities as well as corporate and government R&amp;D laboratories. Internet2 also connects globally to more than 65 international R&amp;E partners that connect over 100 countries. Outside of Texas, Internet2 will provide the high-speed connections to...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
