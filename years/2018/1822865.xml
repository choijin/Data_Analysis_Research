<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Designing and Evaluating a Naturalistic Platform for Collaborative Learning About Spatial Reasonings</AwardTitle>
<AwardEffectiveDate>09/01/2018</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>749849.00</AwardTotalIntnAmount>
<AwardAmount>781849</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Chia Shen</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Spatial reasoning and computational thinking are essential knowledge for future workers in Science, Technology, Engineering and Mathematics (STEM). Researchers in this project will design and study an intelligent multimodal interface to support elementary and middle school students to learn these cognitive skills on a collaborative digital platform based on Minecraft. The new interface will include audio and speech, gaze with eye tracking, and tangible physical objects. The combination of these human-computer interaction modalities may support learning for a broader diversity of learners and facilitate collaboration between peers in a more naturalistic and productive setting. Enabling interacting with computers not only through keyboard commands but also through speech, physical blocks and eye gaze, learners will have the opportunity to practice and improve their spatial reasoning and computational thinking skills in an informal, flexible and accessible context. More broadly, this project investigates the future of multimodal interfaces. Some of the key components of the next-generation multimodal, voice-enabled interface that this project explores include the ability to interpret everyday human language and to express an idea using speech or gesture during collaboration. In addition to developing new technologies, this project examines how to structure different digital learning experiences in ways that promote spatial reasoning and computational thinking. Several hypotheses related to spatial reasoning and computational thinking will be tested through laboratory studies and afterschool learning club studies in partnership with the Evanston Public School District in Illinois. This project is funded by the Cyberlearning for Work at the Human-Technology Frontier program, which seeks to fund exploratory and synergistic research in learning technologies to prepare learners to excel in work at the human-technology frontier.&lt;br/&gt;&lt;br/&gt;The research project will investigate four important research questions. (1) In what ways does a multimodal interface enable or promote increased participation in the afterschool game-based learning club among current non-participants, especially younger students, people from underrepresented groups and girls? (2) How might a multimodal interface change existing patterns of behavior among current learners; specifically, in what ways do multimodal interfaces introduce, support, and disrupt collaborative learning and building practices? (3) Do these changes in participation and in-game practices correlate with the development of spatial thinking, computational thinking and the pursuit of computing careers? (4) What design principles and technological and/or algorithmic developments are needed to support collaborative work through multimodal interfaces? Researchers will employ an iterative design process. Design modifications will be informed by mixed-method analyses of video data, surveys, learning assessments, user activity logs, and multimodal sensory data on how learners engage in multimodal, collaborative problem solving . This research has the potential to discover learning pathways to STEM careers for underrepresented young learners. Around 400 students will participate and experience the proposed multimodal learning system during the three years. The resulting software and design documents will be open source for the public in order to reach a much larger number of institutes to broadening participation of STEM learning.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/27/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/02/2020</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1822865</AwardID>
<Investigator>
<FirstName>David</FirstName>
<LastName>Uttal</LastName>
<EmailAddress>duttal@northwestern.edu</EmailAddress>
<StartDate>08/27/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Marcelo</FirstName>
<LastName>Worsley</LastName>
<EmailAddress>marcelo.worsley@northwestern.edu</EmailAddress>
<StartDate>08/27/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Northwestern University</Name>
<CityName>Chicago</CityName>
<ZipCode>606114579</ZipCode>
<PhoneNumber>3125037955</PhoneNumber>
<StreetAddress>750 N. Lake Shore Drive</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Illinois</StateName>
<StateCode>IL</StateCode>
</Institution>
<ProgramElement>
<Code>1536</Code>
<Text>S-STEM-Schlr Sci Tech Eng&amp;Math</Text>
</ProgramElement>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>063Z</Code>
<Text>FW-HTF Futr Wrk Hum-Tech Frntr</Text>
</ProgramReference>
<ProgramReference>
<Code>7218</Code>
<Text>RET SUPP-Res Exp for Tchr Supp</Text>
</ProgramReference>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
</Appropriation>
<Appropriation>
<Code>0120</Code>
</Appropriation>
<Appropriation>
<Code>13XX</Code>
</Appropriation>
</Award>
</rootTag>
