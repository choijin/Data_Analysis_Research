<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Toward a common digital continuum platform for big data and extreme-scale computing (BDEC2)</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2018</AwardEffectiveDate>
<AwardExpirationDate>09/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>203406.00</AwardTotalIntnAmount>
<AwardAmount>203406</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alan Sussman</SignBlockName>
<PO_EMAI>alasussm@nsf.gov</PO_EMAI>
<PO_PHON>7032927563</PO_PHON>
</ProgramOfficer>
<AbstractNarration>By the end of this decade, the world's store of digital data is projected to reach 40 zetabytes (10 to the power 21 bytes), while the number of network-connected devices (sensors, actuators, instruments, computers, and data stores) is expected to reach 20 billion. While these devices vary dramatically in their capabilities and number, taken collectively they represent a vast "digital continuum" of computing power and prolific data generators that scientific, economic, social, governmental and military concerns of all kinds will want and need to utilize. The diverse set of powerful forces propelling the growth of this digital continuum are prompting calls from various quarters for a next generation network computing platform - a digital continuum platform (DCP) - for creating distributed services in a world permeated by devices and saturated by digital data. But experience shows how challenging the creation of such a future-defining platform is likely to be, especially if the goal is to maximize its acceptance and use, and thereby the size of the community of interoperability it supports. Focusing on the strategically important realm of scientific research, broadly conceived, this project is staging six international workshops (two each in the United States, Europe, and Asia over a two-year period) to enable transnational research communities in a wide range of disciplines to converge on a common DCP to meet this challenge. Building on a decade of leadership in cyberinfrastructure planning, the Big Data and Extreme-scale Computing (BDEC2) community is attacking this problem by pursuing three complementary objectives: &lt;br/&gt;1. Draft a design for a "digital continuum platform" (DCP) to serve as shared software infrastructure for the growing continuum of computing devices and data sources on which future science will rely; &lt;br/&gt;2. Organize and plan an international demonstration of the feasibility and potential of the DCP; and&lt;br/&gt;3. Develop a corresponding "shaping strategy" that addresses all relevant stakeholders and moves the community toward convergence on a standard DCP specification. &lt;br/&gt;Thus, the project serves the national interest, as stated by NSF's mission: to promote the progress of science and to secure the national defense.&lt;br/&gt;&lt;br/&gt;Creating a common digital continuum platform represents a grand challenge problem for the global cyberinfrastructure community. To address this monumental challenge and achieve its objectives, the BDEC2 community is organizing around four distinct but complementary activities:&lt;br/&gt;1. Surveying and analyzing the spectrum of application/workflow needs across diverse research and engineering communities who will use the digital continuum; &lt;br/&gt;2. Developing a reference design for a DCP system architecture that is able to manage the trade offs involved in using a widely shared infrastructure to satisfy diverse application community requirements; &lt;br/&gt;3. Strengthening cooperative and crosscutting efforts among stakeholders (e.g., research communities, commercial vendors, software developers, resource providers) in the "cyber ecosystem" of science and engineering; and &lt;br/&gt;4. Formulating a strategy for building community consensus on a common digital continuum within this same collection of stakeholders.&lt;br/&gt;To help researchers converge on critical problems for important user communities, foster and focus collaboration to solve those problems, and better coordinate software research and data sharing, the BDEC2 community process engages both international software research and data science communities. The process also includes inter-meeting working groups (e.g., for application/workflow analysis and DCP architecture). Combined with the work of the international meetings themselves, these working groups are intended to accelerate community-wide discussion and collaborative activities needed to address the multi-dimensional challenges of the emerging digital continuum. By achieving its goals, this project intends to supply the different stakeholder communities with the kind of well-defined vision and consensus building strategy necessary to realize a common, open, and interoperable DCP for digital continuum era. The project actively promotes participation by talented young scientists (up to 15 across the series) drawn from the academic community and with special attention to women and minorities.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/28/2018</MinAmdLetterDate>
<MaxAmdLetterDate>08/28/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1849625</AwardID>
<Investigator>
<FirstName>Jack</FirstName>
<LastName>Dongarra</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jack J Dongarra</PI_FULL_NAME>
<EmailAddress>dongarra@icl.utk.edu</EmailAddress>
<PI_PHON>8659748295</PI_PHON>
<NSF_ID>000299281</NSF_ID>
<StartDate>08/28/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Daniel</FirstName>
<LastName>Reed</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Daniel A Reed</PI_FULL_NAME>
<EmailAddress>dan.reed@utah.edu</EmailAddress>
<PI_PHON>8015815057</PI_PHON>
<NSF_ID>000356566</NSF_ID>
<StartDate>08/28/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Geoffrey</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Geoffrey C Fox</PI_FULL_NAME>
<EmailAddress>gcf@indiana.edu</EmailAddress>
<PI_PHON>8128567977</PI_PHON>
<NSF_ID>000231257</NSF_ID>
<StartDate>08/28/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Peter</FirstName>
<LastName>Beckman</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Peter H Beckman</PI_FULL_NAME>
<EmailAddress>beckman@uchicago.edu</EmailAddress>
<PI_PHON>6302529020</PI_PHON>
<NSF_ID>000298331</NSF_ID>
<StartDate>08/28/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Tennessee Knoxville</Name>
<CityName>Knoxville</CityName>
<ZipCode>379163801</ZipCode>
<PhoneNumber>8659743466</PhoneNumber>
<StreetAddress>1331 CIR PARK DR</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<StateCode>TN</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TN02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>003387891</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF TENNESSEE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003387891</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Tennessee Knoxville]]></Name>
<CityName/>
<StateCode>TN</StateCode>
<ZipCode>379960003</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Tennessee</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TN02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramReference>
<Code>026Z</Code>
<Text>NSCI: National Strategic Computing Initi</Text>
</ProgramReference>
<ProgramReference>
<Code>062Z</Code>
<Text>Harnessing the Data Revolution</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~203406</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-e9337359-7fff-ac5f-7e33-f9cca5188d53"> </span></p> <p dir="ltr"><span>Between 2018 and 2020, the Big Data and Extreme-scale Computing (BDEC) project carried out its most recent set of meetings in a series of four pre-pandemic workshops and three post-pandemic virtual seminars. This final series represents the latest phase in a twelve-year effort by an international group of leaders in the high-performance computing (HPC) community to respond strategically to the momentous waves of change that have engulfed scientific computing in this century. Beginning with the International Exascale Software Project (IESP) (2009&ndash; 2012) and working through the first series of BDEC workshops (2013&ndash;2018), this community has tracked and helped drive the development of cyberinfrastructure for science and engineering research throughout this period. Its mission is to </span><span>foster the co-design of shared software infrastructure for extreme-scale science that draws on international cooperation and supports a broad spectrum of research domains</span><span>.&nbsp;</span></p> <p dir="ltr"><span>The three stages in the life of this ongoing project correspond to the three significant waves of change in the world of cyberinfrastructure for research. The first wave to arrive (arriving circa 2006) was the multi-core, many-core, and accelerator-based transformation in processor and system design that brought with it a host of closely related problems of software complexity and energy usage. The second wave (arriving circa 2010) was the explosive emergence of &ldquo;big data&rdquo; and large-scale data analysis as increasingly powerful drivers for a wide range of scientific and engineering domains.&nbsp;</span></p> <p dir="ltr"><span>The third wave (arriving circa 2016) is the current revolution in artificial intelligence (AI) that, through the new methods and tools of machine learning (ML) and especially deep learning (DL), is automating empirical phases of the scientific reasoning process in ways that are destined to have profound and wide-ranging effects in nearly every field of science and technology. The use of ML and DL has already been gathering strength for many years, but arguably the combination of the first two waves&mdash;enormous volumes of data to train on and super fast systems to do the training&mdash;have at last thrown open the doors to its widespread use. The second series of BDEC workshops (BDEC2) was conceived to develop a more concrete, strategic plan for addressing the second-wave issues, but the excitement and urgency surrounding developments in AI made it a dominant theme in many of the discussions. These increasingly focused on the transformative impact of new, AI-driven methods and the data resources they require for scientific computing, especially in the context of large international research communities and geo-distributed data ecosystems.</span></p> <p dir="ltr"><span>The ideas developed by the in-person workshop and the virtual meetings throw two essential points into sharp relief. First, it is clear that the absence of a shared, interoperable platform for supporting scientific data ecosystems and deploying generic data services will become increasingly problematic as the AI4Sci revolution moves forward. Realizing the potential of AI4Sci will require us to change the way data are processed and collected. The community must find ways to automate a whole series of activities that would otherwise present obstacles to adequately describing data and properly organizing it for subsequent AI training. Defining and developing automated pipelines that produce data that makes sense to machines, and not necessarily or just to people, changes the focus of the work.</span></p> <p dir="ltr"><span>Second, the shared data ecosystem platform will need to combine processing, storage/buffering, and communication to create integrated services that can be deployed across the &ldquo;digital continuum,&rdquo; from the data center to the network edge and across national boundaries. To acquire the kind of &ldquo;converged&rdquo; services that emerging research programs require, the scientific community has increasingly been turning to commercial cloud providers, but that strategy comes with risks and opportunities that need to be carefully assessed. The critical question confronting the AI-hungry scientific computing community is how to integrate and leverage the resources of commercial cloud computing providers, while at the same time maintaining some autonomy and control over a software and research ecosystem and also enabling applications that can scale up to the kind of extreme performance we are now achieving on the world&rsquo;s fastest supercomputers.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 01/29/2021<br>      Modified by: Jack&nbsp;J&nbsp;Dongarra</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Between 2018 and 2020, the Big Data and Extreme-scale Computing (BDEC) project carried out its most recent set of meetings in a series of four pre-pandemic workshops and three post-pandemic virtual seminars. This final series represents the latest phase in a twelve-year effort by an international group of leaders in the high-performance computing (HPC) community to respond strategically to the momentous waves of change that have engulfed scientific computing in this century. Beginning with the International Exascale Software Project (IESP) (2009&ndash; 2012) and working through the first series of BDEC workshops (2013&ndash;2018), this community has tracked and helped drive the development of cyberinfrastructure for science and engineering research throughout this period. Its mission is to foster the co-design of shared software infrastructure for extreme-scale science that draws on international cooperation and supports a broad spectrum of research domains.  The three stages in the life of this ongoing project correspond to the three significant waves of change in the world of cyberinfrastructure for research. The first wave to arrive (arriving circa 2006) was the multi-core, many-core, and accelerator-based transformation in processor and system design that brought with it a host of closely related problems of software complexity and energy usage. The second wave (arriving circa 2010) was the explosive emergence of "big data" and large-scale data analysis as increasingly powerful drivers for a wide range of scientific and engineering domains.  The third wave (arriving circa 2016) is the current revolution in artificial intelligence (AI) that, through the new methods and tools of machine learning (ML) and especially deep learning (DL), is automating empirical phases of the scientific reasoning process in ways that are destined to have profound and wide-ranging effects in nearly every field of science and technology. The use of ML and DL has already been gathering strength for many years, but arguably the combination of the first two waves&mdash;enormous volumes of data to train on and super fast systems to do the training&mdash;have at last thrown open the doors to its widespread use. The second series of BDEC workshops (BDEC2) was conceived to develop a more concrete, strategic plan for addressing the second-wave issues, but the excitement and urgency surrounding developments in AI made it a dominant theme in many of the discussions. These increasingly focused on the transformative impact of new, AI-driven methods and the data resources they require for scientific computing, especially in the context of large international research communities and geo-distributed data ecosystems. The ideas developed by the in-person workshop and the virtual meetings throw two essential points into sharp relief. First, it is clear that the absence of a shared, interoperable platform for supporting scientific data ecosystems and deploying generic data services will become increasingly problematic as the AI4Sci revolution moves forward. Realizing the potential of AI4Sci will require us to change the way data are processed and collected. The community must find ways to automate a whole series of activities that would otherwise present obstacles to adequately describing data and properly organizing it for subsequent AI training. Defining and developing automated pipelines that produce data that makes sense to machines, and not necessarily or just to people, changes the focus of the work. Second, the shared data ecosystem platform will need to combine processing, storage/buffering, and communication to create integrated services that can be deployed across the "digital continuum," from the data center to the network edge and across national boundaries. To acquire the kind of "converged" services that emerging research programs require, the scientific community has increasingly been turning to commercial cloud providers, but that strategy comes with risks and opportunities that need to be carefully assessed. The critical question confronting the AI-hungry scientific computing community is how to integrate and leverage the resources of commercial cloud computing providers, while at the same time maintaining some autonomy and control over a software and research ecosystem and also enabling applications that can scale up to the kind of extreme performance we are now achieving on the worldâ€™s fastest supercomputers.          Last Modified: 01/29/2021       Submitted by: Jack J Dongarra]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
