<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>The intertwined roles of vision and sensorimotor adaptation on reach-to-grasp movements</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2018</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>523550.00</AwardTotalIntnAmount>
<AwardAmount>523550</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>When we perform mundane daily actions like picking up a cup of coffee, our brain needs to figure out the location of the cup, its shape and weight (which changes depending on how full it is). We tend to think that our perceptual experience of the cup is what determines our interaction with it. However, several studies over the past two decades have repeatedly shown that a perceptual task, like judging the size of an object or its weight, is processed by a different part of the brain than an action task, like reaching to lift the object. This project takes an alternate view, in which the brain processes the visual scene, but this process may be subject to errors (like overestimating the size of a cup or its weight). These errors are immediately detected while an action is unfolding and the subsequent movements toward the object are quickly corrected. This research plan will explore the nature of these complex corrections. It will also examine certain circumstances in which our perception is faulty while our actions are accurate. This knowledge could help people rapidly learn new visuomotor skills, such as interacting in virtual-reality environments and teleoperation. Indeed, a more comprehensive understanding of visually guided action could inform the development of these emerging technologies. Finally, discoveries from this proposal could help improve the lives of individuals with neurological disorders, which often lead to a profound loss of motor ability that significantly impairs activities of daily living. &lt;br/&gt;&lt;br/&gt;This research project uses state-of-the-art virtual reality environments to test 1) to what extent humans can adjust their motor actions "on the fly" to compensate for inaccuracies in visual perception and 2) whether visual perception changes when smooth movement coordination cannot be achieved through motor adjustments. Three mechanistic hypotheses will be tested in which sensory-prediction errors - signals produced when sensory feedback does not match one's expectations - enhance the accuracy of action during repeated visuomotor interactions. At the core of each hypothesis is the idea that when biases in perception lead to inaccurate movements, sensory-prediction errors will drive adaptive changes across the sensorimotor system. Three non-mutually exclusive mechanisms of adaptive change will be tested: (1) Rapid re-alignment of the motor output with the physical world, (2) Changes in calibration of visual perception, and/or (3) Selective changes in the contribution of specific aspects of visual information to action. To test these hypotheses, an integrated set of behavioral experiments will be conducted, in tandem with the development of computational models to mechanistically explain the closely intertwined roles of perception and action.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/11/2018</MinAmdLetterDate>
<MaxAmdLetterDate>09/11/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1827550</AwardID>
<Investigator>
<FirstName>Fulvio</FirstName>
<LastName>Domini</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Fulvio Domini</PI_FULL_NAME>
<EmailAddress>Fulvio_Domini@Brown.edu</EmailAddress>
<PI_PHON>4018631356</PI_PHON>
<NSF_ID>000487044</NSF_ID>
<StartDate>09/11/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brown University</Name>
<CityName>Providence</CityName>
<ZipCode>029129002</ZipCode>
<PhoneNumber>4018632777</PhoneNumber>
<StreetAddress>BOX 1929</StreetAddress>
<StreetAddress2><![CDATA[350 Eddy Street]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<StateCode>RI</StateCode>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>RI01</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001785542</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BROWN UNIVERSITY IN PROVIDENCE IN THE STATE OF RHODE ISLAND AND PROVIDENCE PLANTATIONS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001785542</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Brown University]]></Name>
<CityName>Providence</CityName>
<StateCode>RI</StateCode>
<ZipCode>029129093</ZipCode>
<StreetAddress><![CDATA[Office of Sponsored Projects]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Rhode Island</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>01</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>RI01</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1397</Code>
<Text>Cross-Directorate  Activities</Text>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>063Z</Code>
<Text>FW-HTF Futr Wrk Hum-Tech Frntr</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~523550</FUND_OBLG>
</Award>
</rootTag>
