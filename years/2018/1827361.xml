<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Temporal dynamics of phonetic perceptual organization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2018</AwardEffectiveDate>
<AwardExpirationDate>08/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>536227.00</AwardTotalIntnAmount>
<AwardAmount>545797</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Scientists and engineers have studied speech to understand why this form of communication is so effective. They have also sought to create speaking and listening devices that approach the accuracy and ease of everyday communication, with modest success. The research problem has been easy to define: English is composed of more than 100,000 words created from over 16,000 different syllables and syllables are composed from a small inventory of several dozen consonants and vowels. Automatic speech recognition would be remarkably easy if these linguistic properties - words, syllables, consonants, vowels -produced uniformity in the sounds that talkers actually make. In fact, each utterance is also physically unique, whether in its sound pattern or in the visible movements of the speaker's face.  Different vocal anatomy in men, women and children causes complex variation in sound production even when the linguistic message is the same. Moreover, aspects of a talker's productions may express the dialect and speaking style of their family and linguistic community. Human listeners readily attend to the acoustic hints of these individual and social markers while also listening for the message. This project will examine how these different sources of perceptual information for speech are organized, how they are integrated over time, and how they allow perceptual tuning to the speech of individual talkers. Ultimately, a more complete account of the perception of speech can lead to improvement in recognition technology and to the creation of assistive devices.&lt;br/&gt;&lt;br/&gt;&lt;br/&gt;Three experimental projects will be performed: 1) to estimate the temporal dynamics of auditory sensory integration; 2) to determine the dimensions of exposure-based perceptual tuning to the characteristics of individual talkers; and, 3) to describe and model the intrinsic differences in auditory and visual temporal sensitivity and persistence that affect audiovisual speech perception. In each instance, the perceptual sensitivity to linguistic properties, talker characteristics, and language general features of spoken language will be assayed using discriminating and robust measures of auditory and audiovisual resolution. The studies explore the versatility of perceptual faculties applied to speech and provide an opportunity to identify the principles underlying the remarkably robust perceptual abilities that support and sustain communication. The overall goal is a formal and functional characterization of the cognitive resources that insure the perceptual stability of spoken communication in natural environments, whether the source of speech is visible or not, whether the talker is familiar or not, and whether the quality of the sensory samples of speech is natural or not.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>08/21/2018</MinAmdLetterDate>
<MaxAmdLetterDate>07/06/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1827361</AwardID>
<Investigator>
<FirstName>Robert</FirstName>
<LastName>Remez</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Robert E Remez</PI_FULL_NAME>
<EmailAddress>rremez@barnard.edu</EmailAddress>
<PI_PHON>2128544247</PI_PHON>
<NSF_ID>000177674</NSF_ID>
<StartDate>08/21/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Barnard College</Name>
<CityName>New York</CityName>
<ZipCode>100276598</ZipCode>
<PhoneNumber>2128542708</PhoneNumber>
<StreetAddress>3009 BROADWAY</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>068119601</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BARNARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>068119601</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Barnard College]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276598</ZipCode>
<StreetAddress><![CDATA[3009 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~536227</FUND_OBLG>
<FUND_OBLG>2019~9570</FUND_OBLG>
</Award>
</rootTag>
