<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Sustaining and Extending the Open Science Grid: Science Innovation on a PetaScale Nationwide Facility</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2006</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>13927000.00</AwardTotalIntnAmount>
<AwardAmount>15692445</AwardAmount>
<AwardInstrument>
<Value>Cooperative Agreement</Value>
</AwardInstrument>
<Organization>
<Code>03010000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>PHY</Abbreviation>
<LongName>Division Of Physics</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Shank</SignBlockName>
<PO_EMAI>jshank@nsf.gov</PO_EMAI>
<PO_PHON>7032924516</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The stated goal of distributed Grid Computing is to create a network of interconnected computers that can act as one.  This proposal for the "Open Science Grid" (OSG) is a component of a U.S. effort to create a truly seamless system where scientists and students distributed nationwide and worldwide can effectively collaborate.  &lt;br/&gt;&lt;br/&gt;Physicists' demand for computing power is being spurred by the flood of data that will pour out of the Large Hadron Collider (LHC), the next-generation particle collider at CERN, the European particle physics laboratory near Geneva, as well as LIGO, the Laser Interferometer Gravitational-Wave Observatory in the U.S.  These projects will produce dozens of petabytes (millions of billions of bytes) of data a year, or the equivalent of millions of DVDs, which physicists will store and sift through for at least a couple of decades in search of new phenomena.  To put this in perspective, current estimates of the annual production of information on the planet are on the order of a few thousand petabytes, so these projects will be producing nearly 1% of that total.  Some 100,000's of today's fastest personal computers "with accompanying tape and disk storage and high-speed networking equipment "will be needed to work together to analyse all of this data. &lt;br/&gt;&lt;br/&gt;A goal of the OSG is to enable dozens of other projects in other sciences to reap the benefits of Grid Computing&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/21/2006</MinAmdLetterDate>
<MaxAmdLetterDate>09/16/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>CoopAgrmnt</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0621704</AwardID>
<Investigator>
<FirstName>Miron</FirstName>
<LastName>Livny</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Miron Livny</PI_FULL_NAME>
<EmailAddress>miron@cs.wisc.edu</EmailAddress>
<PI_PHON>6083164336</PI_PHON>
<NSF_ID>000340383</NSF_ID>
<StartDate>09/21/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Paul</FirstName>
<LastName>Avery</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Paul R Avery</PI_FULL_NAME>
<EmailAddress>avery@phys.ufl.edu</EmailAddress>
<PI_PHON>3523929264</PI_PHON>
<NSF_ID>000182353</NSF_ID>
<StartDate>09/21/2006</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ian</FirstName>
<LastName>Foster</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ian Foster</PI_FULL_NAME>
<EmailAddress>foster@uchicago.edu</EmailAddress>
<PI_PHON>6302524619</PI_PHON>
<NSF_ID>000234022</NSF_ID>
<StartDate>09/21/2006</StartDate>
<EndDate>02/23/2009</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Albert</FirstName>
<LastName>Lazzarini</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Albert J Lazzarini</PI_FULL_NAME>
<EmailAddress>lazz@ligo.caltech.edu</EmailAddress>
<PI_PHON>6263958444</PI_PHON>
<NSF_ID>000463786</NSF_ID>
<StartDate>09/21/2006</StartDate>
<EndDate>08/25/2007</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ruth</FirstName>
<LastName>Pordes</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ruth Pordes</PI_FULL_NAME>
<EmailAddress>ruth@fnal.gov</EmailAddress>
<PI_PHON>6308403921</PI_PHON>
<NSF_ID>000101580</NSF_ID>
<StartDate>09/21/2006</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Wisconsin-Madison</Name>
<CityName>MADISON</CityName>
<ZipCode>537151218</ZipCode>
<PhoneNumber>6082623822</PhoneNumber>
<StreetAddress>21 North Park Street</StreetAddress>
<StreetAddress2><![CDATA[Suite 6401]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<StateCode>WI</StateCode>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WI02</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>161202122</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WISCONSIN SYSTEM</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>041188822</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Wisconsin-Madison]]></Name>
<CityName>MADISON</CityName>
<StateCode>WI</StateCode>
<ZipCode>537151218</ZipCode>
<StreetAddress><![CDATA[21 North Park Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Wisconsin</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>02</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WI02</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<FoaInformation>
<Code>0000099</Code>
<Name>Other Applications NEC</Name>
</FoaInformation>
<ProgramElement>
<Code>4095</Code>
<Text>SPECIAL PROJECTS IN NET RESEAR</Text>
</ProgramElement>
<ProgramElement>
<Code>7245</Code>
<Text>PHYSICS GRID COMPUTING</Text>
</ProgramElement>
<ProgramElement>
<Code>7316</Code>
<Text>EAPSI</Text>
</ProgramElement>
<ProgramElement>
<Code>7683</Code>
<Text>SOFTWARE DEVELOPEMENT FOR CI</Text>
</ProgramElement>
<ProgramElement>
<Code>7684</Code>
<Text>CESER-Cyberinfrastructure for</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>5914</Code>
<Text>WESTERN EUROPE, OTHER</Text>
</ProgramReference>
<ProgramReference>
<Code>5937</Code>
<Text>SWEDEN</Text>
</ProgramReference>
<ProgramReference>
<Code>5980</Code>
<Text>WESTERN EUROPE PROGRAM</Text>
</ProgramReference>
<ProgramReference>
<Code>7245</Code>
<Text>PHYSICS GRID COMPUTING</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~2927000</FUND_OBLG>
<FUND_OBLG>2007~2750000</FUND_OBLG>
<FUND_OBLG>2008~2750000</FUND_OBLG>
<FUND_OBLG>2009~3500000</FUND_OBLG>
<FUND_OBLG>2010~3197823</FUND_OBLG>
<FUND_OBLG>2011~567622</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p style="text-align: left;"><strong>Project Goals</strong><br /><br />The Open Science Grid (OSG) project&rsquo;s goal was to stimulate new discoveries by providing scientists with effective and dependable access to an unprecedented national distributed computational facility.&nbsp; We proposed to achieve this through the work of the OSG Consortium: a unique hands-on multi-disciplinary collaboration of scientists, software developers and providers of computing resources.</p> <p style="text-align: left;"><br /><strong>Project Outcomes &amp; Findings</strong><br /><br />The OSG has evolved into an internationally recognized key element of the U.S. national cyberinfrastructure.The OSG has been expanding the reach of high throughput computing (HTC) to a growing number of science communities. <br /><br />&nbsp;The largest OSG science stakeholder has been the Large Hadron Collider (LHC) program at the European Organization for Nuclear Research (CERN), comprising the U.S. ATLAS, U.S. CMS and ALICE-USA contributions to the World Wide LHC Computing Grid (WLCG). The global shared computing infrastructure enabled by the OSG services has facilitated a transformation in the delivery of results from the LHC and other advanced experimental facilities &ndash; enabling the public presentation of results days or weeks after the data is acquired, rather than the months or years it took before.&nbsp; The U.S. LHC scientific program embraces OSG as a major strategic partner in developing, deploying and operating their novel and cost effective DHTC infrastructure.&nbsp; <br /><br />The OSG provides an intellectual hub for the entire DHTC community and drives the development of novel frameworks, educates and trains, forms new collaborative efforts, supports the development of new tools and serves as a testing and evaluation laboratory. We contribute to the NSF eXtreme Digital (XD) program as a Service Provider (SP) to the Extreme Science and Engineering Discovery Environment (XSEDE) project and to the DOE Scientific Discovery through Advanced Computing (SciDAC) program as a promoter and adopter of advanced computational technologies and methods. <br /><br /><em>Figure 1: OSG's Fabric of Services &amp; Community Focused Architecture</em><br /><br />Today, the OSG fabric of services is composed of three groups&mdash;software services, support services like education,&nbsp; training,&nbsp; consulting&nbsp; in the best practices of DHTC, and an infrastructure of DHTC services (referred to as production services) for those who would like to join the OSG&nbsp; DHTC&nbsp; environment&nbsp; (Figure 1). Services in the first two groups serve the broader community that builds and operates their own DHTC environments (e.g. LIGO), as well as supporting the DHTC environment of the OSG.<br /><br />High Throughput Computing technology created and incorporated by the OSG and its contributing partners has now advanced to the point that scientific user communities (VOs) are simultaneously utilizing more geographically distributed HTC resources than ever before. Typical VOs now utilize ~20 resources with some routinely using as many as ~40 simultaneous resources. The overall usage of OSG has grown steadily over the life of this project and now reaches ~70M hours per month as shown in Figure 2. <br />&nbsp;<br /><br /><em>Figure 2: OSG Usage (hours/month) from July 2007 to August 2014</em><br /><br />Key results from this project include: an effective DHTC infrastructure providing single sign-on for use of its services; round-the-clock, dependable services that facilitate effective and secure sharing of the resources; a high quality DHTC software stack; and a &ldquo;home&rdquo; to the DHTC community which pioneered the concept of federated national grids and distributed resource management overlays. Other accomplishments include joint activities with science groups, engagement with end-users and training of s...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[Project Goals  The Open Science Grid (OSG) projectÃ†s goal was to stimulate new discoveries by providing scientists with effective and dependable access to an unprecedented national distributed computational facility.  We proposed to achieve this through the work of the OSG Consortium: a unique hands-on multi-disciplinary collaboration of scientists, software developers and providers of computing resources.  Project Outcomes &amp; Findings  The OSG has evolved into an internationally recognized key element of the U.S. national cyberinfrastructure.The OSG has been expanding the reach of high throughput computing (HTC) to a growing number of science communities.    The largest OSG science stakeholder has been the Large Hadron Collider (LHC) program at the European Organization for Nuclear Research (CERN), comprising the U.S. ATLAS, U.S. CMS and ALICE-USA contributions to the World Wide LHC Computing Grid (WLCG). The global shared computing infrastructure enabled by the OSG services has facilitated a transformation in the delivery of results from the LHC and other advanced experimental facilities &ndash; enabling the public presentation of results days or weeks after the data is acquired, rather than the months or years it took before.  The U.S. LHC scientific program embraces OSG as a major strategic partner in developing, deploying and operating their novel and cost effective DHTC infrastructure.    The OSG provides an intellectual hub for the entire DHTC community and drives the development of novel frameworks, educates and trains, forms new collaborative efforts, supports the development of new tools and serves as a testing and evaluation laboratory. We contribute to the NSF eXtreme Digital (XD) program as a Service Provider (SP) to the Extreme Science and Engineering Discovery Environment (XSEDE) project and to the DOE Scientific Discovery through Advanced Computing (SciDAC) program as a promoter and adopter of advanced computational technologies and methods.   Figure 1: OSG's Fabric of Services &amp; Community Focused Architecture  Today, the OSG fabric of services is composed of three groups&mdash;software services, support services like education,  training,  consulting  in the best practices of DHTC, and an infrastructure of DHTC services (referred to as production services) for those who would like to join the OSG  DHTC  environment  (Figure 1). Services in the first two groups serve the broader community that builds and operates their own DHTC environments (e.g. LIGO), as well as supporting the DHTC environment of the OSG.  High Throughput Computing technology created and incorporated by the OSG and its contributing partners has now advanced to the point that scientific user communities (VOs) are simultaneously utilizing more geographically distributed HTC resources than ever before. Typical VOs now utilize ~20 resources with some routinely using as many as ~40 simultaneous resources. The overall usage of OSG has grown steadily over the life of this project and now reaches ~70M hours per month as shown in Figure 2.     Figure 2: OSG Usage (hours/month) from July 2007 to August 2014  Key results from this project include: an effective DHTC infrastructure providing single sign-on for use of its services; round-the-clock, dependable services that facilitate effective and secure sharing of the resources; a high quality DHTC software stack; and a "home" to the DHTC community which pioneered the concept of federated national grids and distributed resource management overlays. Other accomplishments include joint activities with science groups, engagement with end-users and training of students through the residential summer school .    In 2012, 474 scientific papers were published that depended on use of OSG services and software, many of which are LHC results and 20% of which are non-physics. The number of users of the OSG has risen substantially over the past five years, with more than 2000 end-users accessing the OSG comput...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
