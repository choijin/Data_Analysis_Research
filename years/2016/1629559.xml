<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>XPS: FULL: Broad-Purpose, Aggressively Asynchronous and Theoretically Sound Parallel Large-scale Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>625379.00</AwardTotalIntnAmount>
<AwardAmount>625379</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Wei Ding</SignBlockName>
<PO_EMAI>weiding@nsf.gov</PO_EMAI>
<PO_PHON>7032928017</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many artificial intelligence (AI) applications such as image understanding and natural language processing rely on Machine Learning (ML) methods to automatically extract valuable knowledge from Big Data (Big Learning). Efficient ML requires not only expertise in advanced mathematical models and algorithms, but also experiences with large computer clusters where issues such as machine failures, memory/network bottlenecks, inter-machine latencies must be properly handled through complex system programming. Such demand on "dual skill" often prevents democratizing large-scale AI to wide user communities, and necessitates a new framework that bridges ML and the distributed computing environment of a cluster with a single-machine-like simple interface, allowing ML practitioners to be agnostic about the backend details, and able to quickly prototype or deploy ML programs on clusters. Solutions to such a need remain rare. In this project the PIs develop a new general purpose framework for ML on distributed systems, offering highly efficient and theoretically justified protocols (e.g. communication, scheduling, and partitioning functions) to orchestrate a heterogeneous computer cluster to become programmable and act like a single big computer, and execute distributed ML programs correctly and at a speed orders of magnitude faster than current systems such as Hadoop and Spark. With this new framework, data scientists will be able to conduct ML analytics with complex models on massive data without the need for dedicated engineering and infrastructure teams, allowing Big Learning more readily accessible to society.&lt;br/&gt; &lt;br/&gt;Specifically, over a four year span, the proposed research focuses on three technical aims: (1) Building a System Framework for Big Learning, by developing a new architecture that supports both data- and model-parallel execution of large ML programs, using intelligent scheduler, parameter server, and consistency controller that are configurable to provide flexible options for model/data parallelization, synchronization schemes, load balance, fault tolerance, and multi-instance tenancy; (2) Building a Multi-Level-Abstraction Programming Interface, which supports easy parallel programming of both basic and advanced ML algorithms for large-scale applications; and (3)Conducting theoretical analysis of distributed ML algorithms on the proposed system, based on unique insights such as block consistency and error-tolerance under bounded synchronism. The goal is to develop a system framework to achieve general, automatic, and effective parallelization of ML programs.</AbstractNarration>
<MinAmdLetterDate>08/24/2016</MinAmdLetterDate>
<MaxAmdLetterDate>02/06/2018</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1629559</AwardID>
<Investigator>
<FirstName>Garth</FirstName>
<LastName>Gibson</LastName>
<PI_MID_INIT>A</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Garth A Gibson</PI_FULL_NAME>
<EmailAddress>garth@cs.cmu.edu</EmailAddress>
<PI_PHON>4122685890</PI_PHON>
<NSF_ID>000471285</NSF_ID>
<StartDate>08/24/2016</StartDate>
<EndDate>02/06/2018</EndDate>
<RoleCode>Former Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Eric</FirstName>
<LastName>Xing</LastName>
<PI_MID_INIT>P</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Eric P Xing</PI_FULL_NAME>
<EmailAddress>epxing@cs.cmu.edu</EmailAddress>
<PI_PHON>4122682559</PI_PHON>
<NSF_ID>000195787</NSF_ID>
<StartDate>08/24/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Carnegie-Mellon University</Name>
<CityName>PITTSBURGH</CityName>
<ZipCode>152133815</ZipCode>
<PhoneNumber>4122688746</PhoneNumber>
<StreetAddress>5000 Forbes Avenue</StreetAddress>
<StreetAddress2><![CDATA[WQED Building]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<StateCode>PA</StateCode>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>PA18</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>052184116</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>CARNEGIE MELLON UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>052184116</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Carnegie Mellon University]]></Name>
<CityName>Pittsburgh</CityName>
<StateCode>PA</StateCode>
<ZipCode>152133815</ZipCode>
<StreetAddress><![CDATA[5000 Forbes Avenue]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Pennsylvania</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>18</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>PA18</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8283</Code>
<Text>Exploiting Parallel&amp;Scalabilty</Text>
</ProgramElement>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2016~625379</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Recent innovations in machine learning (ML) have led to an explosion in model and data size, with the largest ML models having 100s of millions to billions of parameters. To cope with the computational cost of training such large models and data, software systems have been created to parallelize training algorithms over multiple CPUs or GPUs and multiple computing devices over a network. In this project, we develop a computer system interface, with effective and easy-to-program parallel ML building blocks, justified by rigorous theoretical analysis, to achieve general, automatic, and effective parallelization of ML programs. We have focus on the following aims.</p> <p>Aim 1: Building a System Framework for Big ML, by developing a new architecture that supports both data- and model-parallel execution of large ML programs, using intelligent scheduler, parameter server, consistency controller, etc. that are configurable to provides flexible options for model/data parallelization, synchronization schemes, load balance, fault tolerance, and multi-instance tenancy.</p> <p>Aim 2: Building a Multi-Level-Abstraction Programming Interface, which supports easy parallel programming of both basic and advanced ML algorithms for large-scale applications, by users of different training backgrounds.</p> <p>Aim 3: Theoretical Analysis of Distributed ML Algorithms on Our System, where we study both finite-time and asymptotic behaviors of our parallelization schemes, based on unique insights such as block consistency, error-tolerance under bounded synchronism.</p> <p>Supported by this award, a team of graduate students at Carnegie Mellon University led by PI Professors Eric Xing and Garth Gibson have carried out a comprehensive and in-depth investigation of the problems listed above, via highly interdisciplinary research that integrates innovations in distributed systems design, ML algorithm design, related theoretical analyses, and industrial applications. These research works have led to a significant array of scientific, educational, and utility outcomes: 1) It has produced a large body of scientific results in the form of new systems and algorithms for distributed machine learning, which have led to more than 20 publications in top peer-reviewed conferences in Systems (e.g., Eurosys, ATC, SoCC), Data Mining (e.g., KDD), and Machine Learning (e.g., NeurIPS, ICML, ICLR). 2) It lays the system-for-ML (i.e., SysML) foundation of a vibrant research lab &ndash; the SAILING Lab, at CMU, which is now hosting about 10-15 Ph.D. students, and many postdocs at any time. In particular, this grant has either completely or partially sponsored the work of about 6 Ph.D. students overall. 3) It has also resulted in a big collection of open source software available to the research community. And the results have also been disseminated through tutorials and keynotes lectures in various research workshops, conferences, and university colloquium.</p> <p>The following systems have been developed and made publicly available: 1) The Bosen parameter service system for parallel ML with large data, 2) the Poseidon system for training large ML models using hybrid communication protocols with higher efficiency, 3) The Orpheus system for a new synchronization strategy with low burden on message congestion, 4) The Litz system for elastic scheduling of computing jobs on computer cluster, and 5) The CAVS for parallelizing dynamic deep learning models.</p> <p>Our work represents the latest development in defining a new filed known as SysML, the interface of machine learning and operating systems. Our work offers insights to how system design can leverage knowledge of machine learning analysis and principles to achieve better performance, and how to make AI algorithms more accessible. The techniques we developed are general purpose and application agonistic and can be used for many different application areas such as: Natural Language Processing, Computer Vision, and Healthcare applications.</p> <p>Our work has also contributed to the founding of an AI unicorn startup Petuum, that is working on productizing and commercializing distributed machine learning technology for standardizing and democratizing large-scale machine learning.</p> <p>Parallel to the research outcome, we have also pursued an education agenda that promote close interaction between systems and machine learning research. So far, this goal has been achieved well. The methodological advancements resultant from this grant have been well integrated into three courses taught at CMU: Graduate Machine Learning, Advanced Machine Learning, Graphical Models, which over years have been attended by hundreds of graduate students, and have had far reaching influences on the students&rsquo; research, and on later teachers of these courses.</p> <p>In summary, with funding from this award from NSF, we have achieved the original goals proposed in our proposal, and have made satisfactory contribution in scientific discovery, methodology development, tool production, and education outreach. We would like to thank NSF for the strong support throughout the duration of this project.</p><br> <p>            Last Modified: 11/10/2020<br>      Modified by: Eric&nbsp;P&nbsp;Xing</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Recent innovations in machine learning (ML) have led to an explosion in model and data size, with the largest ML models having 100s of millions to billions of parameters. To cope with the computational cost of training such large models and data, software systems have been created to parallelize training algorithms over multiple CPUs or GPUs and multiple computing devices over a network. In this project, we develop a computer system interface, with effective and easy-to-program parallel ML building blocks, justified by rigorous theoretical analysis, to achieve general, automatic, and effective parallelization of ML programs. We have focus on the following aims.  Aim 1: Building a System Framework for Big ML, by developing a new architecture that supports both data- and model-parallel execution of large ML programs, using intelligent scheduler, parameter server, consistency controller, etc. that are configurable to provides flexible options for model/data parallelization, synchronization schemes, load balance, fault tolerance, and multi-instance tenancy.  Aim 2: Building a Multi-Level-Abstraction Programming Interface, which supports easy parallel programming of both basic and advanced ML algorithms for large-scale applications, by users of different training backgrounds.  Aim 3: Theoretical Analysis of Distributed ML Algorithms on Our System, where we study both finite-time and asymptotic behaviors of our parallelization schemes, based on unique insights such as block consistency, error-tolerance under bounded synchronism.  Supported by this award, a team of graduate students at Carnegie Mellon University led by PI Professors Eric Xing and Garth Gibson have carried out a comprehensive and in-depth investigation of the problems listed above, via highly interdisciplinary research that integrates innovations in distributed systems design, ML algorithm design, related theoretical analyses, and industrial applications. These research works have led to a significant array of scientific, educational, and utility outcomes: 1) It has produced a large body of scientific results in the form of new systems and algorithms for distributed machine learning, which have led to more than 20 publications in top peer-reviewed conferences in Systems (e.g., Eurosys, ATC, SoCC), Data Mining (e.g., KDD), and Machine Learning (e.g., NeurIPS, ICML, ICLR). 2) It lays the system-for-ML (i.e., SysML) foundation of a vibrant research lab &ndash; the SAILING Lab, at CMU, which is now hosting about 10-15 Ph.D. students, and many postdocs at any time. In particular, this grant has either completely or partially sponsored the work of about 6 Ph.D. students overall. 3) It has also resulted in a big collection of open source software available to the research community. And the results have also been disseminated through tutorials and keynotes lectures in various research workshops, conferences, and university colloquium.  The following systems have been developed and made publicly available: 1) The Bosen parameter service system for parallel ML with large data, 2) the Poseidon system for training large ML models using hybrid communication protocols with higher efficiency, 3) The Orpheus system for a new synchronization strategy with low burden on message congestion, 4) The Litz system for elastic scheduling of computing jobs on computer cluster, and 5) The CAVS for parallelizing dynamic deep learning models.  Our work represents the latest development in defining a new filed known as SysML, the interface of machine learning and operating systems. Our work offers insights to how system design can leverage knowledge of machine learning analysis and principles to achieve better performance, and how to make AI algorithms more accessible. The techniques we developed are general purpose and application agonistic and can be used for many different application areas such as: Natural Language Processing, Computer Vision, and Healthcare applications.  Our work has also contributed to the founding of an AI unicorn startup Petuum, that is working on productizing and commercializing distributed machine learning technology for standardizing and democratizing large-scale machine learning.  Parallel to the research outcome, we have also pursued an education agenda that promote close interaction between systems and machine learning research. So far, this goal has been achieved well. The methodological advancements resultant from this grant have been well integrated into three courses taught at CMU: Graduate Machine Learning, Advanced Machine Learning, Graphical Models, which over years have been attended by hundreds of graduate students, and have had far reaching influences on the studentsâ€™ research, and on later teachers of these courses.  In summary, with funding from this award from NSF, we have achieved the original goals proposed in our proposal, and have made satisfactory contribution in scientific discovery, methodology development, tool production, and education outreach. We would like to thank NSF for the strong support throughout the duration of this project.       Last Modified: 11/10/2020       Submitted by: Eric P Xing]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
