<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: Inverse Origami: Generalized Pose-Normalization for Large-Scale Fine-Grained Recognition</AwardTitle>
<AwardEffectiveDate>06/01/2017</AwardEffectiveDate>
<AwardExpirationDate>05/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>512578.00</AwardTotalIntnAmount>
<AwardAmount>520578</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
</ProgramOfficer>
<AbstractNarration>This project seeks to create a computational platform capable of performing fine-grained visual categorization (FGVC) with accuracy comparable to a highly-trained human domain expert.  This ability to visually classify an object amongst thousands of subtly-differing subordinate categories has numerous real-world applications in biology, forestry and agriculture, homeland security and medical pathology.  While the research conducted in this project is applicable to many domains and applications, it targets insects as the next frontier in fine-grained recognition with specific applications of assessing and conserving biodiversity, and protecting forests and agricultural crops from known invasive pests.  The project's impact extends beyond the advancements in computer vision research, tightly integrating biological fieldwork and citizen science activities such as a field-deployable autonomous recognition system, a mobile recognition app, and an interactive museum exhibit to inspire future scientists.&lt;br/&gt;&lt;br/&gt;The primary research goal of this project is to develop a recognition system that can learn to recognize new fine-grained subcategories using as few as one image.  The developed pose-normalization framework explicitly factorizes an object's representation into a domain-level geometric model and a subcategory-specific appearance model.  The geometric domain model is learned automatically by observation of live subjects.  A unified pose-estimation model determines the location and orientation of the one-, two- and three-dimensional parts of the geometric model, leveraging an innovative form of convolutional neural network.  The subcategory labels and complex part annotations required to train the model are acquired efficiently using physical museum specimens and a multi-view annotation framework.  A large 2500-category dataset of insect images is being constructed for the project and for release to the computer vision research community.</AbstractNarration>
<MinAmdLetterDate>03/09/2017</MinAmdLetterDate>
<MaxAmdLetterDate>05/08/2019</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1651832</AwardID>
<Investigator>
<FirstName>Ryan</FirstName>
<LastName>Farrell</LastName>
<EmailAddress>farrell@cs.byu.edu</EmailAddress>
<StartDate>03/09/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Brigham Young University</Name>
<CityName>Provo</CityName>
<ZipCode>846021231</ZipCode>
<PhoneNumber>8014223360</PhoneNumber>
<StreetAddress>A-285 ASB</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Utah</StateName>
<StateCode>UT</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
</Award>
</rootTag>
