<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Efficient control and transmission of digital puppetry</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2013</AwardExpirationDate>
<AwardTotalIntnAmount>199754.00</AwardTotalIntnAmount>
<AwardAmount>219754</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Thyagarajan Nandagopal</SignBlockName>
<PO_EMAI>tnandago@nsf.gov</PO_EMAI>
<PO_PHON>7032924550</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Digital puppetry refers to the interactive control of virtual characters. This approach can be useful in any virtual experience where a synthetic character?s interactivity must surpass the current capabilities of AI. Examples of this include teacher training; projecting remote docents into museums, science centers and networked virtual environments; and preparing soldiers for their first tours of duty in foreign lands. The objective of this project is to improve the quality and breadth of experiences that can be provided by the puppeteering paradigm, especially when the puppeteer is working from a remote location such as his or her home and the experience is taking place in a location with limited network connectivity. The intent is to reduce latency and bandwidth requirements; cost and complexity of experience creation, capture and delivery; and cognitive load on puppeteers ? while still providing support for complex behaviors. &lt;br/&gt;&lt;br/&gt;This exploratory research project focuses on micro-poses that can be recognized and assembled into more complex actions. The PI seeks to rapidly identify these poses via the fusion of multiple inexpensive sources of sensor data and the physical constraints appropriate for our virtual characters. The PI approaches the tension between precision and network demand by using these micro-poses to reduce cost, footprint of the puppetry motion capture, and networking demands, while simultaneously increasing the effectiveness and accessibility of the puppeteering paradigm. The focus on accessibility leads to the development of techniques that can be run at interactive rates on in-home tabletop systems, facilitating connections between mentors and mentees across the globe.</AbstractNarration>
<MinAmdLetterDate>08/07/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/17/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1051067</AwardID>
<Investigator>
<FirstName>Charles</FirstName>
<LastName>Hughes</LastName>
<PI_MID_INIT>E</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charles E Hughes</PI_FULL_NAME>
<EmailAddress>ceh@cs.ucf.edu</EmailAddress>
<PI_PHON>4078232341</PI_PHON>
<NSF_ID>000099660</NSF_ID>
<StartDate>08/07/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>The University of Central Florida Board of Trustees</Name>
<CityName>Orlando</CityName>
<ZipCode>328168005</ZipCode>
<PhoneNumber>4078230387</PhoneNumber>
<StreetAddress>4000 CNTRL FLORIDA BLVD</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>150805653</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CENTRAL FLORIDA BOARD OF TRUSTEES, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[The University of Central Florida Board of Trustees]]></Name>
<CityName>Orlando</CityName>
<StateCode>FL</StateCode>
<ZipCode>328168005</ZipCode>
<StreetAddress><![CDATA[4000 CNTRL FLORIDA BLVD]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7363</Code>
<Text>Networking Technology and Syst</Text>
</ProgramElement>
<ProgramElement>
<Code>J265</Code>
<Text/>
</ProgramElement>
<ProgramReference>
<Code>7484</Code>
<Text>IIS SPECIAL PROJECTS</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~219754</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p class="Default">The major focus of this project is to develop, test, generalize and deploy network protocols that support the control of remote virtual characters (typically called <em><strong>avatars</strong></em>). Here we will use the term <em><strong>inhabiter</strong></em> to denote any human who is controlling the behaviors of one or more avatars. In this paradigm, each such controlled avatar represents an alter ego of the inhabiter. The persons who interact with the avatars are called <strong><em>users</em></strong> or participants. The intent is that the users perceive interaction with the avatars as natural and engaging, and that the inhabiter feels transported to the remote location that its avatars share with these users. Making this possible involves many intellectual challenges, including the need for reliable and responsive communication between the inhabiter and user sites, and the desirability of an inhabiter control paradigm that is flexible enough to easily represent a broad range of behaviors. Such a paradigm should allow one human inhabiter to control multiple avatars in a manner that keeps cognitive and physical demands low enough so the operation is seamless enough that observers of the avatars assume that each avatar is independently controlled.</p> <p>The approach taken in this project diverges from the two common extremes of inhabitation (human control) of virtual characters &ndash; motion capture and selection of pre-scripted behaviors &ndash; although it does employ some aspects of each of these. The primary innovation is the use of weighted microposes. Each micropose represents a body position or facial expression. In weighted combinations, these microposes provide a large set of non-verbal expressions, tailored to the personality and capabilities of each individual character. For example, a smile and frown can be combined to create a sneer. Control of these expressions is done through a gesturing interface tailored to the individual inhabiter, so that physical and cognitive loads are manageable. Transmission of the intents of the inhabiter is done through a series of weights for each micropose. These weights are then used to render the avatar&rsquo;s appearance at the receiving and the sending ends. This approach requires very little network capacity; moreover pose blending makes pose transitions appear natural. Additional control is available through transmission of detailed head tracking information and through selection of genres of behaviors (generally used when the inhabiter wants to change focus to another character, while instructing the current character to automatically display some behaviors that are appropriate continuations of those which were just carried out under inhabiter control).</p> <p>The plasticity of the inhabiter control paradigm developed here supports a wide range of applications as evidenced by existing projects involving teacher rehearsal, cross cultural communication, protective strategies (for both self and others), debriefing practice, and interpersonal communication skills development, including for those with autism.&nbsp; The most widely deployed of these applications is the <em><strong>TLE TeachLivE&trade; Lab</strong></em>. The environment delivers a classroom simulation, complete with virtual students, intended to enhance teacher development in targeted skills, including those associated with classroom management, new pedagogical approaches and evolving content. Teachers have the opportunity to experiment with new ideas in the Lab without presenting any danger to the learning of &ldquo;real&rdquo; students in a classroom. Moreover, if a teacher has a less than successful session, he or she can reenter the virtual classroom to try a modified approach to teach the same students the same concepts or skills.&nbsp; The existing impact of TeachLivE is best evidenced by its current and growing use (33 univer...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[The major focus of this project is to develop, test, generalize and deploy network protocols that support the control of remote virtual characters (typically called avatars). Here we will use the term inhabiter to denote any human who is controlling the behaviors of one or more avatars. In this paradigm, each such controlled avatar represents an alter ego of the inhabiter. The persons who interact with the avatars are called users or participants. The intent is that the users perceive interaction with the avatars as natural and engaging, and that the inhabiter feels transported to the remote location that its avatars share with these users. Making this possible involves many intellectual challenges, including the need for reliable and responsive communication between the inhabiter and user sites, and the desirability of an inhabiter control paradigm that is flexible enough to easily represent a broad range of behaviors. Such a paradigm should allow one human inhabiter to control multiple avatars in a manner that keeps cognitive and physical demands low enough so the operation is seamless enough that observers of the avatars assume that each avatar is independently controlled.  The approach taken in this project diverges from the two common extremes of inhabitation (human control) of virtual characters &ndash; motion capture and selection of pre-scripted behaviors &ndash; although it does employ some aspects of each of these. The primary innovation is the use of weighted microposes. Each micropose represents a body position or facial expression. In weighted combinations, these microposes provide a large set of non-verbal expressions, tailored to the personality and capabilities of each individual character. For example, a smile and frown can be combined to create a sneer. Control of these expressions is done through a gesturing interface tailored to the individual inhabiter, so that physical and cognitive loads are manageable. Transmission of the intents of the inhabiter is done through a series of weights for each micropose. These weights are then used to render the avatarÃ†s appearance at the receiving and the sending ends. This approach requires very little network capacity; moreover pose blending makes pose transitions appear natural. Additional control is available through transmission of detailed head tracking information and through selection of genres of behaviors (generally used when the inhabiter wants to change focus to another character, while instructing the current character to automatically display some behaviors that are appropriate continuations of those which were just carried out under inhabiter control).  The plasticity of the inhabiter control paradigm developed here supports a wide range of applications as evidenced by existing projects involving teacher rehearsal, cross cultural communication, protective strategies (for both self and others), debriefing practice, and interpersonal communication skills development, including for those with autism.  The most widely deployed of these applications is the TLE TeachLivE&trade; Lab. The environment delivers a classroom simulation, complete with virtual students, intended to enhance teacher development in targeted skills, including those associated with classroom management, new pedagogical approaches and evolving content. Teachers have the opportunity to experiment with new ideas in the Lab without presenting any danger to the learning of "real" students in a classroom. Moreover, if a teacher has a less than successful session, he or she can reenter the virtual classroom to try a modified approach to teach the same students the same concepts or skills.  The existing impact of TeachLivE is best evidenced by its current and growing use (33 universities and two school districts as of November 2013).          Last Modified: 11/28/2013       Submitted by: Charles E Hughes]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
