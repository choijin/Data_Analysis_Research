<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Comparative Vision and Attention</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2010</AwardEffectiveDate>
<AwardExpirationDate>09/30/2012</AwardExpirationDate>
<AwardTotalIntnAmount>88966.00</AwardTotalIntnAmount>
<AwardAmount>88966</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040500</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Many researchers study how nonhuman animals see the world. To date, however, only certain types of comparative studies have been possible, given the limitations of animal learning and response capabilities. With funding from the National Science Foundation, Drs. Nakayama and Pepperberg at Harvard and Brandeis Universities will address questions about visual processing in Grey parrots.  Taking advantage of Grey parrots' ability to mimic human speech, Pepperberg was able to train a Grey parrot to verbally respond to simple optical illusions (e.g., the Müller-Lyer illusion, in which two lines appear to humans to vary in length but in reality do not).  The parrot?s responses indicated that it also perceived the illusions.  Drs. Nakayama and Pepperberg will train additional birds to learn to label various colors and shapes using the sounds of English speech. The current project will then examine whether parrots, like people, can (a) complete the shape of a partially covered object (e.g., see a square partially occluded by a circle as still being a square), and (b) "see" objects that aren't actually there, like a triangle that seems to appear (to humans and primates) between three pac-man-like partial circles that are arranged in a triangular manner, something formally known as an "illusory contour" or "Kanizsa figure." One might expect a parrot to be able, for example, to infer the presence of a predator that isn't fully observable, but no one has been able to ask any nonhuman such questions directly. Future research will involve more complex tasks designed to study how birds pay attention to objects in their visual environment. &lt;br/&gt;&lt;br/&gt;The underlying long-term goal of this research is to determine whether the tasks reveal differences in perceptual processing between birds and humans. Success in training the parrots will enable Drs. Nakayama and Pepperberg eventually to examine a broad range of visual tasks to determine which perceptual abilities share the same underlying mechanisms in birds and humans and which do not.  Comparisons between these two species with very different brain sizes will allow us to understand which components of perception can be implemented with smaller scale neural architecture and which require greater brain size and/or complexity. The data will guide future comparisons with other species and provide insights into the structure and function of the human brain and may provide useful insights for the design of artificial visual processors.&lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>09/21/2010</MinAmdLetterDate>
<MaxAmdLetterDate>09/21/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1026256</AwardID>
<Investigator>
<FirstName>Ken</FirstName>
<LastName>Nakayama</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ken Nakayama</PI_FULL_NAME>
<EmailAddress>k2ibo@berkeley.edu</EmailAddress>
<PI_PHON>5106425292</PI_PHON>
<NSF_ID>000152801</NSF_ID>
<StartDate>09/21/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Irene</FirstName>
<LastName>Pepperberg</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Irene M Pepperberg</PI_FULL_NAME>
<EmailAddress>impepper@media.mit.edu</EmailAddress>
<PI_PHON>7817362195</PI_PHON>
<NSF_ID>000153565</NSF_ID>
<StartDate>09/21/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Harvard University</Name>
<CityName>Cambridge</CityName>
<ZipCode>021385369</ZipCode>
<PhoneNumber>6174955501</PhoneNumber>
<StreetAddress>1033 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[5th Floor]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>082359691</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>PRESIDENT AND FELLOWS OF HARVARD COLLEGE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001963263</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Harvard University]]></Name>
<CityName>Cambridge</CityName>
<StateCode>MA</StateCode>
<ZipCode>021385369</ZipCode>
<StreetAddress><![CDATA[1033 MASSACHUSETTS AVE]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramElement>
<Code>7659</Code>
<Text>Animal Behavior</Text>
</ProgramElement>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~88966</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Cross-species comparisons permit identification of critical, core elements preserved across divergent cognitive and visual architectures as well as those that may be unique to humans. Despite their enormous potential, interpretations of cross-species studies are often complicated by differences between methods used for humans and nonverbal nonhumans. However, one avian subject, a Grey parrot, Alex, had cognitive abilities and verbal response capabilities that allowed testing on <em>identical</em> experiments with human and avian subjects. Alex has died, but he was not unique; only his training was unique. His final data have now been published, demonstrating that his numerical abilities were more like those of children than other nonhumans. Like both children and nonhumans, he could sum two Arabic numerals correctly, but like children and unlike nonhumans tested to date, he derived the successor function: after deriving the order of the first few numerals in his repertoire (i.e., deriving a number line), he figured out that each new numeral was worth exactly one more entity than the previous numeral in the number line (Fig. 1).</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Two other parrots&mdash;Griffin and Arthur&mdash;have had training comparable if not fully equivalent to that of Alex; Griffin in particular has already demonstrated similar abilities and, with further instruction, will be capable of being similarly tested. In the meantime, he and Arthur (as well as two pet parrots) have demonstrated that they can reason by exclusion: Deducing the presence of an object after being show where it is not (see Fig. 2).</p> <p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Griffin participates in experiments much as do humans, <em>without</em> training on stimuli later to be used in testing. Precisely because test stimuli presented in tasks can be completely novel and he responds vocally, he potentially can validate previous animal studies and extend them to more complex visual tasks, avoiding issues of stimulus generalization. Initial tasks on the M&uuml;ller-Lyer illusion with Alex and substantial data from Griffin on modal and amodal completion (Fig. 3) have already established Grey parrots&rsquo; unique viability as subjects; subsequent experiments will allow us to compare the relative allocation of attention resources to spatial and temporal processing in parrot and human.</p> <p><strong><em>Intellectual merit</em></strong><em>: </em>These are clearly first steps, but as part of an exceedingly rare set of nonhuman test subjects with advanced cognitive and vocal response capabilities, Griffin (and to some extent Arthur) present a remarkable opportunity to evaluate visual processing in a very different species with tests <em>directly</em> comparable to those used with humans. <strong><em>Broader impact</em></strong>: These comparisons will help us understand what aspects of human cognition and attention are specific to human neural architecture and what other aspects are driven by environmental factors to common solutions in very different brains. Phenomena to be eventually examined, such as crowding, may have far-reaching affects, from facility in reading to TSA searches. Information about human cognitive architecture will help guide new strategies for teaching, training, and learning in humans that are matched to the strengths and limits of visual factors that are uniquely human.</p><br> <p>            Last Modified: 12/09/2012<br>      Modified by: Irene&nbsp;M&nbsp;Pepperberg</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="con...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[             Cross-species comparisons permit identification of critical, core elements preserved across divergent cognitive and visual architectures as well as those that may be unique to humans. Despite their enormous potential, interpretations of cross-species studies are often complicated by differences between methods used for humans and nonverbal nonhumans. However, one avian subject, a Grey parrot, Alex, had cognitive abilities and verbal response capabilities that allowed testing on identical experiments with human and avian subjects. Alex has died, but he was not unique; only his training was unique. His final data have now been published, demonstrating that his numerical abilities were more like those of children than other nonhumans. Like both children and nonhumans, he could sum two Arabic numerals correctly, but like children and unlike nonhumans tested to date, he derived the successor function: after deriving the order of the first few numerals in his repertoire (i.e., deriving a number line), he figured out that each new numeral was worth exactly one more entity than the previous numeral in the number line (Fig. 1).              Two other parrots&mdash;Griffin and Arthur&mdash;have had training comparable if not fully equivalent to that of Alex; Griffin in particular has already demonstrated similar abilities and, with further instruction, will be capable of being similarly tested. In the meantime, he and Arthur (as well as two pet parrots) have demonstrated that they can reason by exclusion: Deducing the presence of an object after being show where it is not (see Fig. 2).             Griffin participates in experiments much as do humans, without training on stimuli later to be used in testing. Precisely because test stimuli presented in tasks can be completely novel and he responds vocally, he potentially can validate previous animal studies and extend them to more complex visual tasks, avoiding issues of stimulus generalization. Initial tasks on the M&uuml;ller-Lyer illusion with Alex and substantial data from Griffin on modal and amodal completion (Fig. 3) have already established Grey parrotsÆ unique viability as subjects; subsequent experiments will allow us to compare the relative allocation of attention resources to spatial and temporal processing in parrot and human.  Intellectual merit: These are clearly first steps, but as part of an exceedingly rare set of nonhuman test subjects with advanced cognitive and vocal response capabilities, Griffin (and to some extent Arthur) present a remarkable opportunity to evaluate visual processing in a very different species with tests directly comparable to those used with humans. Broader impact: These comparisons will help us understand what aspects of human cognition and attention are specific to human neural architecture and what other aspects are driven by environmental factors to common solutions in very different brains. Phenomena to be eventually examined, such as crowding, may have far-reaching affects, from facility in reading to TSA searches. Information about human cognitive architecture will help guide new strategies for teaching, training, and learning in humans that are matched to the strengths and limits of visual factors that are uniquely human.       Last Modified: 12/09/2012       Submitted by: Irene M Pepperberg]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
