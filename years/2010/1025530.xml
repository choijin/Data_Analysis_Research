<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Doctoral Dissertation Research: Perception and Processing Rates in American Sign Language</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2012</AwardExpirationDate>
<AwardTotalIntnAmount>11960.00</AwardTotalIntnAmount>
<AwardAmount>11960</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Joan Maling</SignBlockName>
<PO_EMAI>jmaling@nsf.gov</PO_EMAI>
<PO_PHON>7032928046</PO_PHON>
</ProgramOfficer>
<AbstractNarration>With NSF support and collaboration among researchers at the University of Maryland and Gallaudet University, doctoral student So-One Hwang will examine the perception and production of American Sign Language (ASL) as compared to spoken English, in order to investigate the contribution of modality and linguistic representations in language processing. Psycholinguistic experiments will be conducted to investigate the effect of visual processing in determining the time windows of integrating linguistic information, using behavioral methodology similar to that used to study speech. Although processing speech often feels like a seamless, continuous experience, studies have shown that the acoustic input is actually analyzed piece-meal, according to time windows that are closely linked to linguistic units. This has been shown by the cognitive restoration of locally-reversed speech, where sentences that are distorted in small chunks still remain intelligible. This phenomenon has provided insight into the temporal integration windows and the limits of the auditory system for language processing. Because signs in ASL take longer to produce than words of spoken English, and because the visual system can be more resilient to temporal distortions than the auditory system, it is hypothesized that significant differences will be found for the perception of ASL as compared to speech. The intelligibility of distorted ASL sentences will be determined by asking deaf participants to sign back what they can understand, and accuracy will be measured. &lt;br/&gt;&lt;br/&gt;In the second part of this project, archived recordings of ASL and English will be used to compare production rates of languages that use different articulators. Using the most current understanding of the features of linguistic units in ASL and English, this study will lead to a better understanding of the differences and similarities in the production rates of languages using different modalities. In addition to contributions based on its findings, this research promotes multidisciplinary collaboration among investigators at two institutions, and will lead to an increased awareness of the importance of diversity and cross-modal approaches in language and cognitive science research.</AbstractNarration>
<MinAmdLetterDate>08/27/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/27/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1025530</AwardID>
<Investigator>
<FirstName>William</FirstName>
<LastName>Idsardi</LastName>
<PI_MID_INIT>J</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>William J Idsardi</PI_FULL_NAME>
<EmailAddress>idsardi@umd.edu</EmailAddress>
<PI_PHON>3023974984</PI_PHON>
<NSF_ID>000166350</NSF_ID>
<StartDate>08/27/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>So-One</FirstName>
<LastName>Hwang</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>So-One K Hwang</PI_FULL_NAME>
<EmailAddress>soone@umd.edu</EmailAddress>
<PI_PHON>3014058140</PI_PHON>
<NSF_ID>000557803</NSF_ID>
<StartDate>08/27/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Maryland, College Park</Name>
<CityName>College Park</CityName>
<ZipCode>207425141</ZipCode>
<PhoneNumber>3014056269</PhoneNumber>
<StreetAddress>3112 LEE BLDG 7809 Regents Drive</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>790934285</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF MARYLAND, COLLEGE PARK</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>003256088</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Maryland, College Park]]></Name>
<CityName>College Park</CityName>
<StateCode>MD</StateCode>
<ZipCode>207425141</ZipCode>
<StreetAddress><![CDATA[3112 LEE BLDG 7809 Regents Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1311</Code>
<Text>Linguistics</Text>
</ProgramElement>
<ProgramReference>
<Code>1311</Code>
<Text>LINGUISTICS</Text>
</ProgramReference>
<ProgramReference>
<Code>9179</Code>
<Text>GRADUATE INVOLVEMENT</Text>
</ProgramReference>
<ProgramReference>
<Code>SMET</Code>
<Text>SCIENCE, MATH, ENG &amp; TECH EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~11960</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Building a model for how humans process language requires examination of a diverse range of languages. This dissertation work compared signed and spoken languages to better understand the time constraints of language processing. Underlying the seemingly effortless process of language comprehension is the perceiver&rsquo;s knowledge about the rate at which the input unfolds in time to create meaning. Rather than analyzing the sounds or signs smoothly and continuously over time, the perceiver integrates the input in time chunks or windows of certain durations. This project was the first to investigate temporal integration windows for sign language processing. Size of temporal integration windows was tested by measuring the intelligibility of sentences that were locally-reversed in different time increments. When these increments are short in duration, the sentences are easy to understand, but as these increments get longer, intelligibility falls sharply and reflects limitations of the perceptual systems. By comparing results from English and American Sign Language, this study&rsquo;s findings show that the time durations over which we chunk the linguistic input depend on whether the input is coming through our ears (~50-60 ms) or our eyes (~250-300 ms). In both cases, however, temporal integration windows are affected by the rate of the input and become shorter proportionately when the rate is faster and the size of the linguistic units themselves become shorter. Thus, integrating the input according to the rate at which the sounds or signs unfold in time to create meaning is a universal mechanism of language processing. However, individuals who learn the language later in their lives do not have the same perceptual flexibility to accurately track natural fluctuations in the input. Finally, data from word, sign, morpheme, and syllable rates from natural production suggest that while the rate of words and signs can vary from language to language, the relationship between the rate of syllables and morphemes is relatively consistent among typologically diverse languages.</p> <p>The project provided many training opportunities for Ph.D. candidate So-One Hwang in different aspects of language research and professional development, such as mentoring, networking, and public-outreach. As a result, Hwang created opportunities for other students get more research training experience. Furthermore, it brought together researchers from Gallaudet University and the University of Maryland to pursue interdisciplinary collaborations for training and research. Sharing the project&rsquo;s findings spreads awareness of the importance of diversity and cross-modal approaches in language and cognitive science research.</p><br> <p>            Last Modified: 10/01/2012<br>      Modified by: So-One&nbsp;K&nbsp;Hwang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Building a model for how humans process language requires examination of a diverse range of languages. This dissertation work compared signed and spoken languages to better understand the time constraints of language processing. Underlying the seemingly effortless process of language comprehension is the perceiverÆs knowledge about the rate at which the input unfolds in time to create meaning. Rather than analyzing the sounds or signs smoothly and continuously over time, the perceiver integrates the input in time chunks or windows of certain durations. This project was the first to investigate temporal integration windows for sign language processing. Size of temporal integration windows was tested by measuring the intelligibility of sentences that were locally-reversed in different time increments. When these increments are short in duration, the sentences are easy to understand, but as these increments get longer, intelligibility falls sharply and reflects limitations of the perceptual systems. By comparing results from English and American Sign Language, this studyÆs findings show that the time durations over which we chunk the linguistic input depend on whether the input is coming through our ears (~50-60 ms) or our eyes (~250-300 ms). In both cases, however, temporal integration windows are affected by the rate of the input and become shorter proportionately when the rate is faster and the size of the linguistic units themselves become shorter. Thus, integrating the input according to the rate at which the sounds or signs unfold in time to create meaning is a universal mechanism of language processing. However, individuals who learn the language later in their lives do not have the same perceptual flexibility to accurately track natural fluctuations in the input. Finally, data from word, sign, morpheme, and syllable rates from natural production suggest that while the rate of words and signs can vary from language to language, the relationship between the rate of syllables and morphemes is relatively consistent among typologically diverse languages.  The project provided many training opportunities for Ph.D. candidate So-One Hwang in different aspects of language research and professional development, such as mentoring, networking, and public-outreach. As a result, Hwang created opportunities for other students get more research training experience. Furthermore, it brought together researchers from Gallaudet University and the University of Maryland to pursue interdisciplinary collaborations for training and research. Sharing the projectÆs findings spreads awareness of the importance of diversity and cross-modal approaches in language and cognitive science research.       Last Modified: 10/01/2012       Submitted by: So-One K Hwang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
