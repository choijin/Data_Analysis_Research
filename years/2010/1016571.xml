<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Fast First-Order Methods for Large-Scale Structured and Sparse Optimization</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2010</AwardEffectiveDate>
<AwardExpirationDate>08/31/2014</AwardExpirationDate>
<AwardTotalIntnAmount>450000.00</AwardTotalIntnAmount>
<AwardAmount>450000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Junping Wang</SignBlockName>
<PO_EMAI>jwang@nsf.gov</PO_EMAI>
<PO_PHON>7032924488</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Algorithms for large-scale optimization have traditionally exploited&lt;br/&gt;sparsity and structure in problem data. Many important optimization &lt;br/&gt;problems today, such as those that arise in  statistical machine &lt;br/&gt;learning (ML) and in compressive sensing (CS) are extremely large-scale convex&lt;br/&gt;problems with completely dense and/or unstructured problem data. &lt;br/&gt;However, there is often sparsity and structure in the solutions to &lt;br/&gt;these problems. The goal of this research project is the development of&lt;br/&gt;first-order algorithms, including gradient methods for non-smooth functions,&lt;br/&gt;smoothed penalty methods for constrained problems, multiple splitting methods,&lt;br/&gt;alternating-direction augmented-Lagrangian methods, and&lt;br/&gt;block coordinate descent methods, for extremely large-scale convex &lt;br/&gt;optimization problems that take advantage of solution structure and/or &lt;br/&gt;sparsity. Rigorous convergence analysis for these methods will be provided and&lt;br/&gt;robust software implementations will be developed. Although these &lt;br/&gt;methods are expected to have wide applicability, the focus will be on &lt;br/&gt;applications in CS and ML. Specifically, the investigators propose to &lt;br/&gt;develop  and analyze new scalable algorithms for (i) CS signal &lt;br/&gt;recovery, including algorithms that are able to exploit more detailed &lt;br/&gt;a priori knowledge in addition to sparsity; (ii) matrix rank minimization, the &lt;br/&gt;matrix analog of CS, and its variants; and (iii) a broad array of ML problems that exploit &lt;br/&gt;the special sparsity/structure of the solutions to these &lt;br/&gt;problems.&lt;br/&gt;&lt;br/&gt;The research that is proposed under this grant is focused on the development of &lt;br/&gt;algorithms with provable performance guarantees that are capable of &lt;br/&gt;solving extremely large scale optimization problems whose solutions are &lt;br/&gt;either sparse or have special structure.  Such problems arise under the paradigm of &lt;br/&gt;compressive sensing, which allows signals (e.g., radar) and images &lt;br/&gt;(e.g., CT and MRI scans) to be obtained with far fewer measurements &lt;br/&gt;than predicted by traditional theory, various extensions of CS, and in &lt;br/&gt;a broad array of problems in machine learning. All of these problems are &lt;br/&gt;aimed at extracting a "sparse" or low-dimensional true model from a &lt;br/&gt;high dimensional or dense empirical model or data. They have important applications &lt;br/&gt;in extracting information from surveillance video and hyper-spectral images, face &lt;br/&gt;recognition, medical imaging and data mining,as well as many other areas &lt;br/&gt;of strategic interest such as national security and biotechnology.</AbstractNarration>
<MinAmdLetterDate>08/16/2010</MinAmdLetterDate>
<MaxAmdLetterDate>08/16/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1016571</AwardID>
<Investigator>
<FirstName>Donald</FirstName>
<LastName>Goldfarb</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Donald Goldfarb</PI_FULL_NAME>
<EmailAddress>goldfarb@columbia.edu</EmailAddress>
<PI_PHON>2128548011</PI_PHON>
<NSF_ID>000118999</NSF_ID>
<StartDate>08/16/2010</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Garud</FirstName>
<LastName>Iyengar</LastName>
<PI_MID_INIT>N</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Garud N Iyengar</PI_FULL_NAME>
<EmailAddress>garud@ieor.columbia.edu</EmailAddress>
<PI_PHON>2128544594</PI_PHON>
<NSF_ID>000487025</NSF_ID>
<StartDate>08/16/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Katya</FirstName>
<LastName>Scheinberg</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Katya Scheinberg</PI_FULL_NAME>
<EmailAddress>katyas@cornell.edu</EmailAddress>
<PI_PHON>9178737981</PI_PHON>
<NSF_ID>000544723</NSF_ID>
<StartDate>08/16/2010</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Columbia University</Name>
<CityName>NEW YORK</CityName>
<ZipCode>100276902</ZipCode>
<PhoneNumber>2128546851</PhoneNumber>
<StreetAddress>2960 Broadway</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY10</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>049179401</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF COLUMBIA UNIVERSITY IN THE CITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>049179401</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Columbia University]]></Name>
<CityName>NEW YORK</CityName>
<StateCode>NY</StateCode>
<ZipCode>100276902</ZipCode>
<StreetAddress><![CDATA[2960 Broadway]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>10</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY10</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1271</Code>
<Text>COMPUTATIONAL MATHEMATICS</Text>
</ProgramElement>
<ProgramElement>
<Code>5514</Code>
<Text>OPERATIONS RESEARCH</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2010~450000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Intellectual Merit.<br /><br />Algorithms for large-scale optimization have traditionally leveraged problem structure by explicitly tailoring the algorithm to the underlying functions and efficiently handling sparsity in problem data. The optimization problems arising in&nbsp; machine learning (ML)&nbsp; and in compressive sensing (CS) are extremely large-scale convex problems with completely dense problem data, but&nbsp; often with sparsity and structure in the solutions. In this&nbsp; project we&nbsp; developed first-order algorithms for extremely large-scale convex optimization problems that take advantage of solution structure and&nbsp; sparsity. We provided rigorous convergence analysis for these methods, and produced robust software implementations. In particular we focused on optimization problems arising in CS and ML. Examples include sparse signal recovery, collaborative filtering, linear classification, graphical models, image denoising, just to name a few important large-scale applications. <br /><br />&nbsp; We applied compressed sensing and random matrix theory to show that one can recover quadratic interpolation models of functions whose Hessian are sparse using fewer interpolation points than was previously considered necessary. We have applied these ideas in the context of derivative-free optimization and developed an algorithm and a Matlab implementation. <br /><br />&nbsp;&nbsp;&nbsp; We developed a second-order algorithm for large-scale sparse optimization, which constructs Lasso-type models of the objective function using L-BFGS Hessian approximations and minimizes the models inexactly using coordinate descent. We have provided convergence rates. Our C++ and Matlab implementations outperform state-of-the-art software for sparse logistic regression and sparse inverse covariance selection.<br /><br />&nbsp; We&nbsp; developed a new non-convex model for risk-parity portfolio selection which we solve very efficiently by&nbsp; a sequence of convex quadratic optimization problems. We proved convergence and provided convergence rates&nbsp; for these new methods. <br /><br />&nbsp;&nbsp;&nbsp; We have combined a fixed-point continuation (FPC) method with active set identification and subspace optimization to develop an extremely effective and efficient method for the basis pursuit problem. We proved global R-linear convergence and showed that the method recovers signals with very large dynamic ranges in CS applications. We have also developed an FPC method and a Bregman iterative algorithm for matrix rank minimization problems and analyzed their ability to recover low-rank solutions. We have developed accelerated versions of the linearized Bregman method, proved that their iteration complexity is reduced from O(1/&epsilon;) to O(1/&radic;&epsilon;) and applied them to CS and matrix completion problems.<br /><br />&nbsp;&nbsp;&nbsp; We developed alternating linearization methods (ALMs) for solving convex optimizaion problems that often arise as tight convex relaxations of nonconvex structured optimization problems. Our methods solve problems of the form: min{F(x,y)&equiv;f(x)+g(y): Ax+y=b}. Under the assumption that both f and g are convex functions with Lipschitz continuous gradients, we proved that our methods require O(1/&epsilon;) iterations to obtain an &epsilon;-optimal solution (O(1/&radic;&epsilon;), for accelerated versions),&nbsp; while requiring essentially the same computational effort at each iteration. We developed specialized versions of these algorithms to solve various high dimensional problems in ML. Specifically, we applied them to Gaussian graphical models and to overlapping group LASSO problems involving appropriate sparsity-inducing norm regularizers. We developed an&nbsp; efficient block-coordinate descent approach for&nbsp; group LASSO problems, line search versions of our accelerated ALMs and the fast prox-gradient FISTA method t...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Intellectual Merit.  Algorithms for large-scale optimization have traditionally leveraged problem structure by explicitly tailoring the algorithm to the underlying functions and efficiently handling sparsity in problem data. The optimization problems arising in  machine learning (ML)  and in compressive sensing (CS) are extremely large-scale convex problems with completely dense problem data, but  often with sparsity and structure in the solutions. In this  project we  developed first-order algorithms for extremely large-scale convex optimization problems that take advantage of solution structure and  sparsity. We provided rigorous convergence analysis for these methods, and produced robust software implementations. In particular we focused on optimization problems arising in CS and ML. Examples include sparse signal recovery, collaborative filtering, linear classification, graphical models, image denoising, just to name a few important large-scale applications.     We applied compressed sensing and random matrix theory to show that one can recover quadratic interpolation models of functions whose Hessian are sparse using fewer interpolation points than was previously considered necessary. We have applied these ideas in the context of derivative-free optimization and developed an algorithm and a Matlab implementation.       We developed a second-order algorithm for large-scale sparse optimization, which constructs Lasso-type models of the objective function using L-BFGS Hessian approximations and minimizes the models inexactly using coordinate descent. We have provided convergence rates. Our C++ and Matlab implementations outperform state-of-the-art software for sparse logistic regression and sparse inverse covariance selection.    We  developed a new non-convex model for risk-parity portfolio selection which we solve very efficiently by  a sequence of convex quadratic optimization problems. We proved convergence and provided convergence rates  for these new methods.       We have combined a fixed-point continuation (FPC) method with active set identification and subspace optimization to develop an extremely effective and efficient method for the basis pursuit problem. We proved global R-linear convergence and showed that the method recovers signals with very large dynamic ranges in CS applications. We have also developed an FPC method and a Bregman iterative algorithm for matrix rank minimization problems and analyzed their ability to recover low-rank solutions. We have developed accelerated versions of the linearized Bregman method, proved that their iteration complexity is reduced from O(1/&epsilon;) to O(1/&radic;&epsilon;) and applied them to CS and matrix completion problems.      We developed alternating linearization methods (ALMs) for solving convex optimizaion problems that often arise as tight convex relaxations of nonconvex structured optimization problems. Our methods solve problems of the form: min{F(x,y)&equiv;f(x)+g(y): Ax+y=b}. Under the assumption that both f and g are convex functions with Lipschitz continuous gradients, we proved that our methods require O(1/&epsilon;) iterations to obtain an &epsilon;-optimal solution (O(1/&radic;&epsilon;), for accelerated versions),  while requiring essentially the same computational effort at each iteration. We developed specialized versions of these algorithms to solve various high dimensional problems in ML. Specifically, we applied them to Gaussian graphical models and to overlapping group LASSO problems involving appropriate sparsity-inducing norm regularizers. We developed an  efficient block-coordinate descent approach for  group LASSO problems, line search versions of our accelerated ALMs and the fast prox-gradient FISTA method that preserve these methodsÃ†  fast iteration complexity (and improve performance) and specialized versions of ALMs and prox-gradient methods for  robust and stable principle component pursuit problems. We also developed  an extremely fast...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
