<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research: Building the Community for the Open Storage Network</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>06/15/2018</AwardEffectiveDate>
<AwardExpirationDate>05/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>432400.00</AwardTotalIntnAmount>
<AwardAmount>494918</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05090000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>OAC</Abbreviation>
<LongName>Office of Advanced Cyberinfrastructure (OAC)</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alejandro Suarez</SignBlockName>
<PO_EMAI>alsuarez@nsf.gov</PO_EMAI>
<PO_PHON>7032927092</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The scientific community is facing a major challenge dealing with the increasing amount of open scientific data emerging from research projects on all scales-- from large facilities to small research labs. Over the last five years the NSF has funded more than 200 high-speed connections to the Internet-2 backbone operating at 10-100Gbps speeds. The goal of this project is to develop a prototype module for a high performance distributed storage system that extends the usability of the existing high-speed interconnects. This project is a pilot for a potential national-scale storage infrastructure for open scientific data, which at full scale could serve hundred sites and many hundreds of Petabytes.  Many of the technologies associated with such a distributed system already exist; the key challenge in this project is social engineering: how can one design a simple enough yet robust storage node that can be easily replicated, is attractive for universities and research projects to adopt, is easy to manage and can support the various patterns for large scale scientific analyses?&lt;br/&gt;&lt;br/&gt;Many universities have several of the necessary pieces for Data Intensive Science in place-- reasonably sized computing clusters, a few PB of storage and even a high-speed connection-- yet performing the analyses of data intensive science is very painful and slow. Data is never there when needed, large storage systems often fail despite having massive RAID configurations, and moving data from disk-to-disk at the full network speed still requires complex skills. The project offers a broad community buy-in through the Big Data Hubs, a unique combination of skills, facilities and science challenges to test, evaluate and deploy different hardware and software combinations that can be used in the design of a much larger, national-scale system. The goal is to design and run detailed benchmarks for various test science projects requiring different combinations of data transfer, data processing and massive compute, and use the results to design and build a low-cost, scalable petascale appliance including inexpensive hardware nodes and a simple software stack that can be replicated across many universities, supercomputer centers and large NSF facilities. The proposed system could become an enormous multiplier on the existing NSF investments in high end computing and fast networks. It could also accelerate the pace of standardization of data storage across the nation. The public, open data products, often discussed in the Data Management Plans at the end of NSF proposals could find an easy-to-use home. Various educational projects could simply rely upon a robust storage infrastructure with a simple API, and build a variety of delivery services for the educational community.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>06/07/2018</MinAmdLetterDate>
<MaxAmdLetterDate>10/15/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1747490</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Norman</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael L Norman</PI_FULL_NAME>
<EmailAddress>mlnorman@ucsd.edu</EmailAddress>
<PI_PHON>8588225461</PI_PHON>
<NSF_ID>000235680</NSF_ID>
<StartDate>06/07/2018</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Christine</FirstName>
<LastName>Kirkpatrick</LastName>
<PI_MID_INIT>R</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Christine R Kirkpatrick</PI_FULL_NAME>
<EmailAddress>christine@sdsc.edu</EmailAddress>
<PI_PHON>8588223322</PI_PHON>
<NSF_ID>000699006</NSF_ID>
<StartDate>06/07/2018</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-San Diego</Name>
<CityName>La Jolla</CityName>
<ZipCode>920930934</ZipCode>
<PhoneNumber>8585344896</PhoneNumber>
<StreetAddress>Office of Contract &amp; Grant Admin</StreetAddress>
<StreetAddress2><![CDATA[9500 Gilman Drive, 0934]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA49</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>804355790</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, SAN DIEGO</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-San Diego]]></Name>
<CityName>La Jolla</CityName>
<StateCode>CA</StateCode>
<ZipCode>920930934</ZipCode>
<StreetAddress><![CDATA[9500 Gilman Drive]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>49</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA49</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7231</Code>
<Text>CYBERINFRASTRUCTURE</Text>
</ProgramElement>
<ProgramElement>
<Code>8074</Code>
<Text>EarthCube</Text>
</ProgramElement>
<ProgramReference>
<Code>062Z</Code>
<Text>Harnessing the Data Revolution</Text>
</ProgramReference>
<Appropriation>
<Code>0118</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2018~432400</FUND_OBLG>
<FUND_OBLG>2020~62518</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-2eaf71c2-7fff-d2d8-edb7-fcd1fe6af20d"> </span></p> <p><span id="docs-internal-guid-7e15b738-7fff-bedb-f597-28b33399e50b">&nbsp;</span></p> <p dir="ltr"><span>The Open Storage Network (OSN) created a distributed platform for data sharing and data reuse.&nbsp; The pilot arrived at a time when commercial cloud storage was cost prohibitive to medium and large research projects. The OSN platform attracted usage from research communities addressing natural disasters, dark matter, Earth Sciences, the Humanities, Computer Science, Materials Science, and more. Researchers reported satisfaction using the platform, and many of the teams integrated OSN into their research plan and utilized increased quotas. The OSN team represents a partnership between the nation&rsquo;s Advanced Computing centers and leading innovators. Rallying around the common goal of low-cost, high performance, distributed storage, the team devised a technical methodology, processes, and approach for the most efficient operations possible, concentrated in the OSN Command Center. The five-site OSN deployment that went into production in December 2019 is operating and improving, and capacity is expanding through engagement with sites investing in deploying new OSN pods. The initial OSN deployment has matured into a production service with robust and scalable hardware, software, policy, and practices. OSN is a boon to computing on the network, particularly for the growing needs of mid-scale research, which focuses on collaboration and open data, but also needs access to storage cyberinfrastructure within limited resources. OSN provides a solution that helps to fill a gap in high performance and big data applications, building in performance and cost effectiveness that is competitive with or exceeds current solutions.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>Researchers received allocations directly from the OSN, and later also through XSEDE following the platform's inclusion as a Level 2 XSEDE service provider. The project provided a roadmap of how other storage resources may be included alongside computation in the NSF XSEDE and future ACCESS cyberinfrastructures. The testing and selection of a reliable hardware configuration proved useful to the broader community, and multiple institutions are now acquiring pods to be connected to the OSN. OSN Pods feature low cost per terabyte and high bandwidth capability in a single, well-integrated rack, supported with automated provisioning.&nbsp;</span></p> <p>&nbsp;</p> <p dir="ltr"><span>In the last year of the award, the team conducted an assessment of related and companion projects. Results and lessons learned have been communicated through a series of webinars that covered the research drivers that motivated initial use of the OSN, national and international trends in large scale research storage, OSN outcomes to date, and visions for the future of data sharing and distributed storage in research. Accompanying the webinar series were three concept papers (see Attachments), highlighting research drivers, state of the art,&nbsp; synergistic projects, and OSN capabilities and features. Community input and engagement was high, and collaborators and colleagues agreed there is a need for greater investment in distributed storage infrastructure. It is inefficient for every medium to large research project to reinvent research cyberinfrastructure. Platforms like the OSN allow the project teams to focus on their domain research and to trust in a well-architected and maintained resource for storage and data sharing.</span></p> <p>&nbsp;</p> <p dir="ltr"><span>OSN is poised for continued engagement with the scientific computing community.&nbsp; It is currently cooperatively managed by a distributed team that is bound by shared governance, with representation from the community. The project</span><span> </span><span>has used input from stakeholders to ensure uniform global behavior and continues to build out and implement flexible governance structures and functions that will allow the OSN to scale and operate efficiently as a distributed, readily accessible infrastructure. The OSN has established policies (see Attachments) that form a foundation for governance and operation. </span><span>This work has informed the policy work of other projects and organizations through team member participation in project working groups.</span></p> <p dir="ltr"><span>&nbsp;</span></p> <p dir="ltr"><span>The OSN is uniquely positioned to solve challenges encountered in mid-scale research. It is geographically distributed in national supercomputer and advanced computing centers near compute resources; it is connected to high-speed,low latency networks and it can support horizontal scaling by allowing simultaneous reads and writes to multiple OSN pods. Mid-scale researchers are already using the OSN to exploit the high-speed research network on which the OSN pods are strategically distributed to stream OSN data to their compute resources.</span></p> <p>&nbsp;</p><br> <p>            Last Modified: 08/11/2021<br>      Modified by: Christine&nbsp;R&nbsp;Kirkpatrick</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[      The Open Storage Network (OSN) created a distributed platform for data sharing and data reuse.  The pilot arrived at a time when commercial cloud storage was cost prohibitive to medium and large research projects. The OSN platform attracted usage from research communities addressing natural disasters, dark matter, Earth Sciences, the Humanities, Computer Science, Materials Science, and more. Researchers reported satisfaction using the platform, and many of the teams integrated OSN into their research plan and utilized increased quotas. The OSN team represents a partnership between the nationâ€™s Advanced Computing centers and leading innovators. Rallying around the common goal of low-cost, high performance, distributed storage, the team devised a technical methodology, processes, and approach for the most efficient operations possible, concentrated in the OSN Command Center. The five-site OSN deployment that went into production in December 2019 is operating and improving, and capacity is expanding through engagement with sites investing in deploying new OSN pods. The initial OSN deployment has matured into a production service with robust and scalable hardware, software, policy, and practices. OSN is a boon to computing on the network, particularly for the growing needs of mid-scale research, which focuses on collaboration and open data, but also needs access to storage cyberinfrastructure within limited resources. OSN provides a solution that helps to fill a gap in high performance and big data applications, building in performance and cost effectiveness that is competitive with or exceeds current solutions.    Researchers received allocations directly from the OSN, and later also through XSEDE following the platform's inclusion as a Level 2 XSEDE service provider. The project provided a roadmap of how other storage resources may be included alongside computation in the NSF XSEDE and future ACCESS cyberinfrastructures. The testing and selection of a reliable hardware configuration proved useful to the broader community, and multiple institutions are now acquiring pods to be connected to the OSN. OSN Pods feature low cost per terabyte and high bandwidth capability in a single, well-integrated rack, supported with automated provisioning.     In the last year of the award, the team conducted an assessment of related and companion projects. Results and lessons learned have been communicated through a series of webinars that covered the research drivers that motivated initial use of the OSN, national and international trends in large scale research storage, OSN outcomes to date, and visions for the future of data sharing and distributed storage in research. Accompanying the webinar series were three concept papers (see Attachments), highlighting research drivers, state of the art,  synergistic projects, and OSN capabilities and features. Community input and engagement was high, and collaborators and colleagues agreed there is a need for greater investment in distributed storage infrastructure. It is inefficient for every medium to large research project to reinvent research cyberinfrastructure. Platforms like the OSN allow the project teams to focus on their domain research and to trust in a well-architected and maintained resource for storage and data sharing.    OSN is poised for continued engagement with the scientific computing community.  It is currently cooperatively managed by a distributed team that is bound by shared governance, with representation from the community. The project has used input from stakeholders to ensure uniform global behavior and continues to build out and implement flexible governance structures and functions that will allow the OSN to scale and operate efficiently as a distributed, readily accessible infrastructure. The OSN has established policies (see Attachments) that form a foundation for governance and operation. This work has informed the policy work of other projects and organizations through team member participation in project working groups.   The OSN is uniquely positioned to solve challenges encountered in mid-scale research. It is geographically distributed in national supercomputer and advanced computing centers near compute resources; it is connected to high-speed,low latency networks and it can support horizontal scaling by allowing simultaneous reads and writes to multiple OSN pods. Mid-scale researchers are already using the OSN to exploit the high-speed research network on which the OSN pods are strategically distributed to stream OSN data to their compute resources.          Last Modified: 08/11/2021       Submitted by: Christine R Kirkpatrick]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
