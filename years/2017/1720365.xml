<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative: BystanderBots: Automated Bystander Intervention for Cyberbullying Mitigation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>99993.00</AwardTotalIntnAmount>
<AwardAmount>99993</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05050000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CNS</Abbreviation>
<LongName>Division Of Computer and Network Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Balakrishnan Prabhakaran</SignBlockName>
<PO_EMAI>bprabhak@nsf.gov</PO_EMAI>
<PO_PHON>7032924847</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Bullying has lasting negative psychological and physical effects on victims, bystanders, and bullies alike; online settings can magnify both the scale and impact of these effects, as anonymity can embolden people to make hostile posts about individuals or groups.  This project aims to reduce the prevalence of such posts through the design of active, automated "bystander interventions" in online comment threads.  Bystander interventions, in which one or more witnesses to a bullying incident pressures the bully to stop, are often effective in schoolyards, but people are often reluctant to intervene in online scenarios.  Instead, a computer program could post comments that contain these interventions, potentially reducing follow-on aggression from the original poster or others who might pile on -- if bullies perceive these posts as coming from human bystanders, and if bullies under the cover of pseudonyms react to bystander interventions as they do in in-person confrontations.   The project will proceed in three main stages.  The first stage involves improving cyberbullying detection through better detection of non-standard language use associated with bullying in a particular commenting system.  The second stage involves developing a dialogue system that acts like a human bystander, creating messages that look appropriate in the context of given a comment thread and that contain psychologically-valid bystander interventions.  The third stage involves deploying the tool in a large video sharing site and monitoring its ability to detect and, through interventions, mitigate further bullying.  If successful, the project could have real impacts in reducing online aggression in social media systems while reducing the need for (and possible harms to) human moderators; the tools will also be released to the community to support other kinds of research around how chatbots and humans might interact in online comments.&lt;br/&gt;&lt;br/&gt;The work on detection aims to advance natural language processing (NLP) and computational pragmatics, particularly around non-canonical language use, because state-of-the-art bullying detection schemes typically use bag-of-words approaches that do not consider the linguistic and structural features of cyberbullying.  The team will explore how to identify both explicit indicators of bullying, by developing topic models based on complex features where particular topics are more often associated with bullying, and implicit indicators, through looking for words whose use in a given context diverges from their location in other contexts. The context will be represented as a subspace of words, where the words themselves occur as low-dimensional word embeddings. The dialogue generation portion of the project will characterize and represent properties of effective bystander interventions from the psychology literature.  This representation will drive a dialogue manager designed to generate bystander responses automatically so that the responses contain features that are both believable and are known to be effective in reducing bullying online.  These components will first be evaluated through offline testing, using comment data labeled for bullying content and human ratings of the generated dialogue.  Once a reasonably effective pipeline has been built, it will be evaluated in a series of online experiments in which comment threads are monitored and automated bystander responses generated for some, but not all, threads detected as containing bullying.  The software will log the monitored threads and any generated responses, along with behavior both before and after the automated bystander response in a particular thread; these data will allow the team to evaluate the impact of the bystander intervention on bullying incidents later in the thread.</AbstractNarration>
<MinAmdLetterDate>08/07/2017</MinAmdLetterDate>
<MaxAmdLetterDate>08/07/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1720365</AwardID>
<Investigator>
<FirstName>Dorothy</FirstName>
<LastName>Espelage</LastName>
<PI_MID_INIT>L</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dorothy L Espelage</PI_FULL_NAME>
<EmailAddress>espelage@ufl.edu</EmailAddress>
<PI_PHON>2177666413</PI_PHON>
<NSF_ID>000245990</NSF_ID>
<StartDate>08/07/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>Gainesville</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[1 University of Florida]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8060</Code>
<Text>Secure &amp;Trustworthy Cyberspace</Text>
</ProgramElement>
<ProgramReference>
<Code>025Z</Code>
<Text>SaTC: Secure and Trustworthy Cyberspace</Text>
</ProgramReference>
<ProgramReference>
<Code>065Z</Code>
<Text>Human factors for security research</Text>
</ProgramReference>
<ProgramReference>
<Code>7434</Code>
<Text>CNCI</Text>
</ProgramReference>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8225</Code>
<Text>SaTC Special Projects</Text>
</ProgramReference>
<ProgramReference>
<Code>9102</Code>
<Text>WOMEN, MINORITY, DISABLED, NEC</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~99993</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-b11d02b5-7fff-84ec-e3ab-ed24035f059a"> <p dir="ltr"><span>Cyberbullying, defined as &ldquo;willful and repeated harm inflicted through the use of computers, cell phone, or other electronic devices&rdquo; (Hinduja &amp; Patchin, 2009), is a worldwide emerging public health concern (Tokunga, 2010). According to research findings, 41% of the people in the U.S. reported experiences of some form of online harassment, and 66% reported online harassment as a serious problem (PEW Research Center, 2017). More specifically, cyberbullying is consistently associated with various deleterious psychological and behavioral outcomes including anxiety, depression, substance use, suicidal ideation (Kowalski, Giumetti, Schroeder, &amp; Lattanner, 2014).In turn, more prevention science scholarship is being focused on prevention of cyberbullying and its adverse consequences (Van Geel, Vedder, &amp; Tanilon, 2014).</span></p> <p dir="ltr"><span>Previous literature findings about traditional bullying&nbsp; have demonstrated that the action or inaction of bystanders can alter the consequences of bullying (Polanin, Espelage, &amp; Pigot, 2012). Bystander intervention happens when an individual that witnesses bullying decides to intervene (Polanin, Espelage, &amp; Pigott, 2012) and it has been found to be an effective way of mitigating other forms of bullying (Salmivalli, 2014). Yet, literature findings indicate that the likelihood of bystander intervention is very low in cyberbullying incidents (Huang &amp; Chou, 2010). Thus, there is an emerging focus on promoting bystander intervention in bullying in digital platforms as a way of preventing cyberbullying&nbsp; (</span><span>DiFranzo, Taylor, Kazerooni, Wherry, &amp; Bazarova, 2018</span><span>). Although, research findings indicate that there are some strategies that are commonly observed in individuals&rsquo; way of intervening when they are bystanders in bullying&nbsp; incidents (Freis &amp; Gurung, 2013; Shultz, Heilman, &amp; Hart, 2014) little is known about the effectiveness of the intervention strategies that are used by bystanders in the virtual world.</span></p> <p dir="ltr"><span>The present study is part of a project funded by the National Science Foundation, that aims to contribute to building a healthier online ecosystem by creating an internet chat bot (BystanderBots) that automates the detection of toxic words and intervenes in cyberbullying incidents by automatically posting prosocial comments as if it were a real person. Our study was divided in 5 phases: (1) YouTube data collection and cyberbullying annotations; (2) experimental development of a machine learning detector of abusive language; (3) data collection and annotation of prosocial comments found in the wild; (4) generation of new prosocial comments; and (5) testing prosocial comments generated using a randomized experimental design to evaluate the efficacy of prosocial comments to reduce the incidence of cyberbullying.&nbsp;</span></p> <p dir="ltr"><span>Previous Results:</span><span> During </span><span>Phase 1</span><span>, human annotators assigned cyberbullying labels (1, Yes; 0, No) to 11,540 YouTube comments extracted from 253 different channels publishing anti-feminist content. Anotators classified&nbsp; 27.5% of the comments as abusive and 20.2% of the sentences were identified as containing cyberbullying language. During </span><span>Phase 2</span><span>, the team developed novel neural network techniques to detect abusive language. The detector uses a novel embedding-based neural network approach to disambiguate the different senses in which toxic words can be used, since many swear words can be used both in an aggressive and a friendly sense. The embedding-based iterative approach achieved comparable performance to human annotators and allowed us to compile a quality profanity lexicon without reliance on external linguistic resources. During </span><span>Phase 3</span><span>, </span><span>the team collected a sample of prosocial comments found in the wild which is used to inform the following phases.</span></p> <p dir="ltr"><span>The aim of </span><span>Phase 4</span><span>, the current stage, is to create a set of prosocial comments that will be used by BystanderBots in the automated intervention response and in </span><span>Phase 5</span><span>&nbsp; the effectiveness of the created prosocial comments on mitigating cyberbullying will be tested. To do that, the present study aims to generate prosocial comments that are likely to&nbsp; have an impact on mitigating cyberbullying as well as increasing other individuals&rsquo; willingness to intervene on YouTube comment threads. In the light of  previous literature findings, six categories of prosocial comments (empathy-provoking, promoting constructive conversation, changing the subject, addressing the topic not the person, removing yourself from the audience, and complimenting/telling jokes) were created. The corpus of prosocial comments will be posted to 100 comment threads on YouTube. Another set of 100 comments will be used as the control condition. The prosocial intervention group will be compared with the control group using the number of cyberbullying comments posted within a period of 30 days from the publishing date of the original prosocial comment and a matching comment from the control group. Additionally, the follow-up comments posted after the prosocial comments will be coded qualitatively in different domains (e.g. effectiveness, appropriateness, unintended effects). The mixed methods results will allow us to understand the complexity of online toxic discourse and to identify and describe which categories of comments have the potential to reduce the incidence of online cyberbullying. The results from Phase 4 and 5 will be available during the next few months, and the presentation will be based on those findings.</span></p> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 01/24/2020<br>      Modified by: Dorothy&nbsp;L&nbsp;Espelage</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Cyberbullying, defined as "willful and repeated harm inflicted through the use of computers, cell phone, or other electronic devices" (Hinduja &amp; Patchin, 2009), is a worldwide emerging public health concern (Tokunga, 2010). According to research findings, 41% of the people in the U.S. reported experiences of some form of online harassment, and 66% reported online harassment as a serious problem (PEW Research Center, 2017). More specifically, cyberbullying is consistently associated with various deleterious psychological and behavioral outcomes including anxiety, depression, substance use, suicidal ideation (Kowalski, Giumetti, Schroeder, &amp; Lattanner, 2014).In turn, more prevention science scholarship is being focused on prevention of cyberbullying and its adverse consequences (Van Geel, Vedder, &amp; Tanilon, 2014). Previous literature findings about traditional bullying  have demonstrated that the action or inaction of bystanders can alter the consequences of bullying (Polanin, Espelage, &amp; Pigot, 2012). Bystander intervention happens when an individual that witnesses bullying decides to intervene (Polanin, Espelage, &amp; Pigott, 2012) and it has been found to be an effective way of mitigating other forms of bullying (Salmivalli, 2014). Yet, literature findings indicate that the likelihood of bystander intervention is very low in cyberbullying incidents (Huang &amp; Chou, 2010). Thus, there is an emerging focus on promoting bystander intervention in bullying in digital platforms as a way of preventing cyberbullying  (DiFranzo, Taylor, Kazerooni, Wherry, &amp; Bazarova, 2018). Although, research findings indicate that there are some strategies that are commonly observed in individuals’ way of intervening when they are bystanders in bullying  incidents (Freis &amp; Gurung, 2013; Shultz, Heilman, &amp; Hart, 2014) little is known about the effectiveness of the intervention strategies that are used by bystanders in the virtual world. The present study is part of a project funded by the National Science Foundation, that aims to contribute to building a healthier online ecosystem by creating an internet chat bot (BystanderBots) that automates the detection of toxic words and intervenes in cyberbullying incidents by automatically posting prosocial comments as if it were a real person. Our study was divided in 5 phases: (1) YouTube data collection and cyberbullying annotations; (2) experimental development of a machine learning detector of abusive language; (3) data collection and annotation of prosocial comments found in the wild; (4) generation of new prosocial comments; and (5) testing prosocial comments generated using a randomized experimental design to evaluate the efficacy of prosocial comments to reduce the incidence of cyberbullying.  Previous Results: During Phase 1, human annotators assigned cyberbullying labels (1, Yes; 0, No) to 11,540 YouTube comments extracted from 253 different channels publishing anti-feminist content. Anotators classified  27.5% of the comments as abusive and 20.2% of the sentences were identified as containing cyberbullying language. During Phase 2, the team developed novel neural network techniques to detect abusive language. The detector uses a novel embedding-based neural network approach to disambiguate the different senses in which toxic words can be used, since many swear words can be used both in an aggressive and a friendly sense. The embedding-based iterative approach achieved comparable performance to human annotators and allowed us to compile a quality profanity lexicon without reliance on external linguistic resources. During Phase 3, the team collected a sample of prosocial comments found in the wild which is used to inform the following phases. The aim of Phase 4, the current stage, is to create a set of prosocial comments that will be used by BystanderBots in the automated intervention response and in Phase 5  the effectiveness of the created prosocial comments on mitigating cyberbullying will be tested. To do that, the present study aims to generate prosocial comments that are likely to  have an impact on mitigating cyberbullying as well as increasing other individuals’ willingness to intervene on YouTube comment threads. In the light of  previous literature findings, six categories of prosocial comments (empathy-provoking, promoting constructive conversation, changing the subject, addressing the topic not the person, removing yourself from the audience, and complimenting/telling jokes) were created. The corpus of prosocial comments will be posted to 100 comment threads on YouTube. Another set of 100 comments will be used as the control condition. The prosocial intervention group will be compared with the control group using the number of cyberbullying comments posted within a period of 30 days from the publishing date of the original prosocial comment and a matching comment from the control group. Additionally, the follow-up comments posted after the prosocial comments will be coded qualitatively in different domains (e.g. effectiveness, appropriateness, unintended effects). The mixed methods results will allow us to understand the complexity of online toxic discourse and to identify and describe which categories of comments have the potential to reduce the incidence of online cyberbullying. The results from Phase 4 and 5 will be available during the next few months, and the presentation will be based on those findings.           Last Modified: 01/24/2020       Submitted by: Dorothy L Espelage]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
