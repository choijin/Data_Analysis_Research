<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: INT: COLLAB: Integrated Modeling and Learning for Robust Grasping and Dexterous Manipulation with Adaptive Hands</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2017</AwardEffectiveDate>
<AwardExpirationDate>08/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>632500.00</AwardTotalIntnAmount>
<AwardAmount>632500</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>James Donlon</SignBlockName>
<PO_EMAI>jdonlon@nsf.gov</PO_EMAI>
<PO_PHON>7032928074</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Robots need to effectively interact with a large variety of objects&lt;br/&gt;that appear in warehouses and factories as well as homes and offices.&lt;br/&gt;This requires robust grasping and dexterous manipulation of everyday&lt;br/&gt;objects through low cost robots and low complexity solutions.&lt;br/&gt;Traditionally, robots use rigid hands and analytical models for such&lt;br/&gt;tasks, which often fail in the presence of even small errors. New&lt;br/&gt;compliant hands promise improved performance, while minimizing&lt;br/&gt;complexity, and increased robustness. Nevertheless, they are&lt;br/&gt;inherently difficult to sense and model. This project combines ideas&lt;br/&gt;from different robotics sub-fields to address this limitation. It&lt;br/&gt;utilizes progress in machine learning and builds on a strong tradition&lt;br/&gt;in robot modeling. The objective is to provide adaptive, compliant&lt;br/&gt;robots that are better in grasping objects in the presence of multiple&lt;br/&gt;unknown contact points and sliding or rolling objects in-hand. The&lt;br/&gt;broader impact will be strengthened by the open release of new or&lt;br/&gt;modified robot hand designs, improved control algorithms and software,&lt;br/&gt;as well as corresponding data sets. Furthermore, academic&lt;br/&gt;dissemination will be accompanied by educational outreach to&lt;br/&gt;undergraduate and high school students.&lt;br/&gt;&lt;br/&gt;Towards the above objective, the first step will be the definition of&lt;br/&gt;new hybrid models appropriate for adaptive, compliant hands.  This&lt;br/&gt;will happen by improving analytical solutions and extending them to&lt;br/&gt;allow adaptation based on data via novel, time-efficient learning&lt;br/&gt;methods. The objective is to capture model uncertainty inherent in&lt;br/&gt;real-world interactions; a process that suffers from data scarcity.&lt;br/&gt;In order to reduce the amount of data required for learning, different&lt;br/&gt;models will be tailored to specific tasks through an automated&lt;br/&gt;discovery of these tasks and of underlying motion primitives for each&lt;br/&gt;one of them. This task identification process will operate iteratively&lt;br/&gt;with learning and utilize improved models to discover new tasks. It&lt;br/&gt;can also provide feedback for improved hand design. Once these&lt;br/&gt;learning-based and task-focused models are available, they will be&lt;br/&gt;used to learn and synthesize controllers for grasping and in-hand&lt;br/&gt;manipulation. To learn controllers, this work will consider a&lt;br/&gt;model-based, reinforcement learning approach, which will be evaluated&lt;br/&gt;against alternatives. For controller synthesis, existing tools for&lt;br/&gt;this purpose will be integrated with task planning primitives and&lt;br/&gt;extended through learning processes to identify the preconditions&lt;br/&gt;under which different controllers can be chained together. The project&lt;br/&gt;involves extensive evaluation on a variety of novel adaptive hands and&lt;br/&gt;robotic arms designed in the PIs' labs. Modern vision-based solutions&lt;br/&gt;will be used to track grasped objects and provide feedback for&lt;br/&gt;learning and closed-loop control.  The evaluation will measure whether&lt;br/&gt;the developed hybrid models can significantly improve robustness of&lt;br/&gt;grasping and the effectiveness of dexterous manipulation.</AbstractNarration>
<MinAmdLetterDate>07/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1734190</AwardID>
<Investigator>
<FirstName>Aaron</FirstName>
<LastName>Dollar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Aaron Dollar</PI_FULL_NAME>
<EmailAddress>aaron.dollar@yale.edu</EmailAddress>
<PI_PHON>2034269122</PI_PHON>
<NSF_ID>000525237</NSF_ID>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Yale University</Name>
<CityName>New Haven</CityName>
<ZipCode>065208327</ZipCode>
<PhoneNumber>2037854689</PhoneNumber>
<StreetAddress>Office of Sponsored Projects</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 208327]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<StateCode>CT</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CT03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>043207562</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>YALE UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>043207562</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Yale University]]></Name>
<CityName>New Haven</CityName>
<StateCode>CT</StateCode>
<ZipCode>065116816</ZipCode>
<StreetAddress><![CDATA[15 Prospect Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Connecticut</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CT03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~632500</FUND_OBLG>
</Award>
</rootTag>
