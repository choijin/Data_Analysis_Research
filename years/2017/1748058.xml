<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EAGER: Collaborative Research: Interactive Dialog Agents for Social Language Development and Listening Comprehension</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2017</AwardEffectiveDate>
<AwardExpirationDate>06/30/2020</AwardExpirationDate>
<AwardTotalIntnAmount>96818.00</AwardTotalIntnAmount>
<AwardAmount>104818</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Practicing oral language skills supports children's later ability to process written text. Low socioeconomic status children receive lower levels of quality home language input, which negatively affects language development. Many children already suffer from a significant deficit in reading comprehension by the time they begin formal education, a deficit they may never overcome. This EArly Grant for Exploratory Research (EAGER) project will investigate improving children's home language environment with pedagogical learning agents, animated story characters and a child-like narrator who carry on a dialog with a child while telling a story. Story characters will talk directly to the child, tell the story from different perspectives, ask the child questions, phrase those questions in different ways using different vocabulary, and thus engage the child in understanding the story and improving their social language and listening comprehension skills.  Pedagogical learning agents are a cheap and easily deployable technology that could readily be provided to children in any socioeconomic group.&lt;br/&gt;&lt;br/&gt;This project will develop technology for expressive interactive storytelling agents that model human conversational storytelling with a high level of social engagement. This will require methods to effectively manage a spoken dialog with a child, using speech recognition and text-to-speech output. The project will scaffold from a deep computational representation of narrative to generate different dialogs around the same story. Novel algorithms for natural language generation will support asking different types of questions at different story points and generation of story events with first person and direct speech to allow the characters to talk.  The humanoid child narrator will model high immediacy nonverbal behaviors shown to increase engagement and positive affect. The project's prototype will be informed by regular evaluation by children at the UC Davis Reading and Academic Development Center, as the project assesses the promise of this exploratory technology to improve social language and comprehension skills.</AbstractNarration>
<MinAmdLetterDate>08/14/2017</MinAmdLetterDate>
<MaxAmdLetterDate>02/21/2019</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1748058</AwardID>
<Investigator>
<FirstName>Michael</FirstName>
<LastName>Neff</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Michael Neff</PI_FULL_NAME>
<EmailAddress>mpneff@ucdavis.edu</EmailAddress>
<PI_PHON>5307473838</PI_PHON>
<NSF_ID>000488920</NSF_ID>
<StartDate>08/14/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Emily</FirstName>
<LastName>Solari</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Emily Solari</PI_FULL_NAME>
<EmailAddress>ejsolari@ucdavis.edu</EmailAddress>
<PI_PHON>5307547700</PI_PHON>
<NSF_ID>000745704</NSF_ID>
<StartDate>08/14/2017</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956168562</ZipCode>
<StreetAddress><![CDATA[1 Shields Ave]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8020</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramElement>
<ProgramReference>
<Code>7916</Code>
<Text>EAGER</Text>
</ProgramReference>
<ProgramReference>
<Code>8045</Code>
<Text>Cyberlearn &amp; Future Learn Tech</Text>
</ProgramReference>
<ProgramReference>
<Code>8089</Code>
<Text>Understanding the Brain/Cognitive Scienc</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2017~96818</FUND_OBLG>
<FUND_OBLG>2019~8000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Listening comprehension is a critical early childhood skill that forms the basis for reading comprehension, a foundational requirement for educational achievement.&nbsp; The amount and quality of oral input children receive varies broadly, which can create inequity in students&rsquo; ability to achieve later learning.&nbsp; Therefore, significant impact may be achieved by providing quality verbal input to children that helps them develop listening comprehension skills.&nbsp; Software applications provide one way to achieve this and can be distributed at a large scale for a relatively low cost.&nbsp; It remains unclear, however, how to most effectively design these applications in order to achieve these goals.&nbsp; This grant supported research on developing animated listening comprehension experiences and evaluated the role of verbal and nonverbal behavior in these.</p> <p>An animated storytelling application was developed and tested with 33 children between the ages of 5 and 8.&nbsp; It evaluated both a narrator-driven approach to storytelling and a distributed version in which characters were given synthesized dialogue.&nbsp; It also compared simple gesture with more complex gesture that actively directed children&rsquo;s attention and requested input. &nbsp;An analysis of eye gaze indicates that children attend more to the story when a distributed storytelling model is used. Gesture prompts appear to encourage children to ask questions, something that children did, but at a relatively low rate. Interestingly, the children most frequently asked &ldquo;why" questions. Switching gaze between characters happened more quickly when the story characters began to speak than for narrator turns. These results will help inform the design of future agent-based storytelling system research.&nbsp; Furthermore, children were willing to actively engage with the system, suggesting such technology is applicable as a tool for improving listening comprehension.</p> <p>&nbsp;</p><br> <p>            Last Modified: 12/12/2020<br>      Modified by: Michael&nbsp;Neff</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Image         </div> <div class="galControls onePhoto" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation onePhoto" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2020/1748058/1748058_10514462_1607731680284_Experimental_Setup--rgov-214x142.jpg" original="/por/images/Reports/POR/2020/1748058/1748058_10514462_1607731680284_Experimental_Setup--rgov-800width.jpg" title="Storytelling application"><img src="/por/images/Reports/POR/2020/1748058/1748058_10514462_1607731680284_Experimental_Setup--rgov-66x44.jpg" alt="Storytelling application"></a> <div class="imageCaptionContainer"> <div class="imageCaption">This image shows the experimental storytelling application deployed on a laptop (left), and the video fees used to annotate gaze behavior (right).</div> <div class="imageCredit">Harrison Jesse Smith</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Michael&nbsp;Neff</div> <div class="imageTitle">Storytelling application</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Listening comprehension is a critical early childhood skill that forms the basis for reading comprehension, a foundational requirement for educational achievement.  The amount and quality of oral input children receive varies broadly, which can create inequity in students’ ability to achieve later learning.  Therefore, significant impact may be achieved by providing quality verbal input to children that helps them develop listening comprehension skills.  Software applications provide one way to achieve this and can be distributed at a large scale for a relatively low cost.  It remains unclear, however, how to most effectively design these applications in order to achieve these goals.  This grant supported research on developing animated listening comprehension experiences and evaluated the role of verbal and nonverbal behavior in these.  An animated storytelling application was developed and tested with 33 children between the ages of 5 and 8.  It evaluated both a narrator-driven approach to storytelling and a distributed version in which characters were given synthesized dialogue.  It also compared simple gesture with more complex gesture that actively directed children’s attention and requested input.  An analysis of eye gaze indicates that children attend more to the story when a distributed storytelling model is used. Gesture prompts appear to encourage children to ask questions, something that children did, but at a relatively low rate. Interestingly, the children most frequently asked "why" questions. Switching gaze between characters happened more quickly when the story characters began to speak than for narrator turns. These results will help inform the design of future agent-based storytelling system research.  Furthermore, children were willing to actively engage with the system, suggesting such technology is applicable as a tool for improving listening comprehension.          Last Modified: 12/12/2020       Submitted by: Michael Neff]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
