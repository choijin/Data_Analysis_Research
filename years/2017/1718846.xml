<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: Linguistic Structure in Neural Sequence Models</AwardTitle>
<AwardEffectiveDate>08/01/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2021</AwardExpirationDate>
<AwardTotalIntnAmount>395002.00</AwardTotalIntnAmount>
<AwardAmount>395002</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Over the past 25 years, the field of artificial intelligence has made great strides in the ability to automatically analyze and generate sequential data.  Much of this progress has come by building probabilistic models.  For example, mathematical descriptions of how words are typically used in context are based on a scientific understanding of the relationships among letters, sounds, words, and phrases, thanks to the field of linguistics.  Probabilistic models based on this understanding have allowed us to develop computational, data-driven methods for reasoning about the likely structure and meaning of sentences.  In the same way, probabilistic models of sequences of events have led to computational methods for predicting the unfolding of future events and reconstructing the ordering of past ones.  This project starts with sophisticated probabilistic models of linguistic structure and event sequences, and aims to make them more powerful, by using "deep learning" (neural networks) to increase their sensitivity to contextual effects.  Deep learning has already recently had a revolutionary impact on artificial intelligence.  This research will focus on using deep learning to enhance probabilistic models in settings where the model must discover structure that is not provided in its training data, such as the compositional units of language or the causal relations among events.&lt;br/&gt;&lt;br/&gt;The planned model design will not focus on hand-engineered features, but rather on broad representational choices.  The overall architectures are motivated by certain basic notions that linguists and other modelers have found indispensable in their analyses of empirical data as follows: (1) stick-breaking processes that respect duality of patterning, the linguistic notion that a word's internal form is not necessarily related to its external usage but is governed by separate rules or by chance; (2) finite-state transducers that can capture local editing that transforms an input sequence into an output sequence; (3) context-free grammars that can model hierarchical structure to help explain word sequences; and (4) temporal point processes that can capture process intensity, where different events are competing to occur next, and combinations of earlier events combine to elevate or suppress the rates of later events.  The project will infuse these probabilistic techniques with recurrent neural networks, in particular, long short-term memory (LSTM) networks.  In some cases, exact inference in the resulting models will not be tractable, necessitating the design of Monte Carlo or variational approximations.</AbstractNarration>
<MinAmdLetterDate>07/27/2017</MinAmdLetterDate>
<MaxAmdLetterDate>07/27/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1718846</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Eisner</LastName>
<EmailAddress>jason@cs.jhu.edu</EmailAddress>
<StartDate>07/27/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
</Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
</Award>
</rootTag>
