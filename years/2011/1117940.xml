<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>HCC: Small: Programming by Voice: Extending Initial Programming Environments for Children with Disabilities</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>315524.00</AwardTotalIntnAmount>
<AwardAmount>331524</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Ephraim Glinert</SignBlockName>
<PO_EMAI>eglinert@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The PI's objective in this research is to empower children with certain kinds of disabilities so they can participate fully in initial programming environments (IPEs) that are used to teach computer science.  Specifically, the PI will investigate the science and necessary tool construction to support speech-enabled adaptation of IPEs such as Scratch, Lego LabVIEW for Mindstorms, and Alice, which were originally designed for manual input with a keyboard and mouse, in order to allow children with limited use of their limbs to interact via alternative interfaces. The aforementioned IPEs traditionally rely on user interfaces involving windows, icons, and other graphical widgets, and require a mode of program input that can pose a barrier to those with upper limb motor impairments, who may lack the dexterity and mobility needed to control a mouse or keyboard with their hands.  The PI's approach will be to imitate the common mouse and keyboard interactions with a voice-driven interface that is customized for each IPE.  To these ends, the PI will develop a speech-aware application that runs in parallel to the IPE, listens to the voice commands from the user, interprets the commands according to a grammar influenced by the IPE concepts, and imitates appropriate actions similar to mouse and keyboard operation within the IPE.  Core research questions will include how such assistive customizations can be added with automation using reverse engineering and model-driven engineering.&lt;br/&gt;&lt;br/&gt;During the first year of the project, the PI will extended a previously developed proof-of-concept to cover the entire Scratch interface.  The result will be a robust tool that enables Programming by Voice in Scratch, and which will serve as the evaluation instrument for a target group of children with disabilities.  The lessons learned from the first phase of the project will drive a generalization of the steps needed to customize a speech interface for an existing application.  Techniques involving screen scraping and reverse engineering, as well as model-driven engineering, will be investigated to automate the process of adapting IPEs to Programming by Voice.  The resulting tools will be applied to a new IPE, the Lego LabVIEW for Mindstorms, to allow children with disabilities to program robots.  The design and evaluation of the project will be performed in collaboration with United Cerebral Palsy of Birmingham, who will recruit participants into the project and provide resources for training, evaluation, and feedback on the project design.&lt;br/&gt;&lt;br/&gt;Broader Impacts:  This project unites ideas of human-computer interaction with computer science education to provide customized assistive environments for teaching computational thinking to children with disabilities (targeting grades 6-12).  The work will advance our ability to automate the generation of software development environments that support Programming by Voice, resulting in advanced capabilities to enable children with upper limb motor impairments to participate fully in computer science education opportunities.  The PI will involve graduate students in the research; he will supervision undergraduate Honors projects, and he will also mentor high school students from underrepresented groups with science fair projects related to this work.  The results of the research will be disseminated through a project web page that will include open source software, teaching materials, video demonstrations and publications.</AbstractNarration>
<MinAmdLetterDate>09/09/2011</MinAmdLetterDate>
<MaxAmdLetterDate>05/07/2013</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1117940</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Gray</LastName>
<PI_MID_INIT>G</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey G Gray</PI_FULL_NAME>
<EmailAddress>gray@cs.ua.edu</EmailAddress>
<PI_PHON>2053482847</PI_PHON>
<NSF_ID>000247328</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Sandra</FirstName>
<LastName>Nichols</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Sandra C Nichols</PI_FULL_NAME>
<EmailAddress>scnichols@bamaed.ua.edu</EmailAddress>
<PI_PHON>2053485152</PI_PHON>
<NSF_ID>000584521</NSF_ID>
<StartDate>09/09/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Alabama Tuscaloosa</Name>
<CityName>Tuscaloosa</CityName>
<ZipCode>354870001</ZipCode>
<PhoneNumber>2053485152</PhoneNumber>
<StreetAddress>801 University Blvd.</StreetAddress>
<StreetAddress2><![CDATA[152 Rose Admin. / Box 870104]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<StateCode>AL</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>AL07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>045632635</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF ALABAMA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>808245794</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Alabama Tuscaloosa]]></Name>
<CityName>Tuscaloosa</CityName>
<StateCode>AL</StateCode>
<ZipCode>354870001</ZipCode>
<StreetAddress><![CDATA[801 University Blvd.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Alabama</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>AL07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7367</Code>
<Text>HCC-Human-Centered Computing</Text>
</ProgramElement>
<ProgramReference>
<Code>7367</Code>
<Text>Cyber-Human Systems</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>9150</Code>
<Text>EXP PROG TO STIM COMP RES</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~315524</FUND_OBLG>
<FUND_OBLG>2013~16000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p>Computer Science (CS) educators frequently develop new methodologies, languages, and programming environments to teach novice programmers the fundamental concepts of CS. A recent trend has focused on new environments that reduce the initial challenges associated with the heavy syntax focus of traditional textual programming languages. There are numerous Initial Programming Environments (IPEs) available that have been created for student use that in some cases have fostered self-discovery and inquiry-based exploration (e.g., Scratch (2015), Lego Mindstorms LabVIEW, and Blockly. The creative and graphical nature of IPEs have been shown to increase student and teacher interest in CS through adoption in classrooms worldwide at the K-12 levels, as well as interest in introductory university courses. Moreover, the removal of syntax issues in block-based languages that are often encountered in textual languages lowers the floor thereby making block-based languages easier to start using.</p> <p>Although the block-based nature of IPEs can be helpful for learning concepts in CS, a small group of students is being left out from learning experiences and engagement in CS (approximately 5% of students) due to block-based environments&rsquo; dependence on the Windows Icon Mouse Pointer (WIMP) metaphor. Block-based environments often require the use of both a mouse and keyboard, which motorically challenged users often are unable to operate. Based on research performed in this project, a Vocal User Interface (VUI) is a viable solution that offers a &ldquo;Programming by Voice&rdquo; (PBV) capability (i.e., a capability to describe a program without the use of a keyboard or mouse). The PBV approach leads to a time-consuming process in terms of adapting legacy applications, particularly, if multiple applications (such as the three IPEs previously mentioned) require specialized VUIs. Each environment has its own visual layout and its own commands; therefore, each application requires a different VUI. In order to create a more generic solution, a Domain-Specific Language (DSL) has been created in this project to create a semi-automated process allowing a level of abstraction that captures the specific needs of each IPE. From the specification of each IPE, a customized VUI can be generated that integrates with the legacy application in a non-invasive manner.</p> <p>The intellectual merit of the projects contributions is focused on techniques and supporting tools that assist in mapping a GUI to a VUI, including the ability to record the locations of existing GUI widgets and an underlying PBV engine that integrates a speech recognizer with programmatic control of the mouse and keyboard. The broad impacts of the project are concerned with the opportunities that are provided to children and others with mobility challenges who want to learn computer programming. The following are summary highlights from the project:</p> <ul> <li>A project web page is available at <a href="http://myna.cs.ua.edu/">http://myna.cs.ua.edu/</a> </li> <li>The main contribution of the project is the Myna framework, which provides a means to link a speech recognition system to an underlying engine that provides programmatic control of the mouse and keyboard. The final version of the project permits the full use of drag and drop features within several different programming environments. </li> <li>Myna has been applied to Scratch (both desktop and web), Snap! (BYOB and web), Lego Mindstorms NXT, Code.org Hour of Code puzzles, and two Blockly languages (Pixly and Spherly).</li> <li>The project also investigated the creation of a semi-automated tool that could be used to collect specific information (e.g., xy-coordinate, name, type, behavior, etc.) about various components within a GUI. This tool, Myna Scraper, allows the user to click through the process of data collection, which is then stored in proper...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[    Computer Science (CS) educators frequently develop new methodologies, languages, and programming environments to teach novice programmers the fundamental concepts of CS. A recent trend has focused on new environments that reduce the initial challenges associated with the heavy syntax focus of traditional textual programming languages. There are numerous Initial Programming Environments (IPEs) available that have been created for student use that in some cases have fostered self-discovery and inquiry-based exploration (e.g., Scratch (2015), Lego Mindstorms LabVIEW, and Blockly. The creative and graphical nature of IPEs have been shown to increase student and teacher interest in CS through adoption in classrooms worldwide at the K-12 levels, as well as interest in introductory university courses. Moreover, the removal of syntax issues in block-based languages that are often encountered in textual languages lowers the floor thereby making block-based languages easier to start using.  Although the block-based nature of IPEs can be helpful for learning concepts in CS, a small group of students is being left out from learning experiences and engagement in CS (approximately 5% of students) due to block-based environmentsÃ† dependence on the Windows Icon Mouse Pointer (WIMP) metaphor. Block-based environments often require the use of both a mouse and keyboard, which motorically challenged users often are unable to operate. Based on research performed in this project, a Vocal User Interface (VUI) is a viable solution that offers a "Programming by Voice" (PBV) capability (i.e., a capability to describe a program without the use of a keyboard or mouse). The PBV approach leads to a time-consuming process in terms of adapting legacy applications, particularly, if multiple applications (such as the three IPEs previously mentioned) require specialized VUIs. Each environment has its own visual layout and its own commands; therefore, each application requires a different VUI. In order to create a more generic solution, a Domain-Specific Language (DSL) has been created in this project to create a semi-automated process allowing a level of abstraction that captures the specific needs of each IPE. From the specification of each IPE, a customized VUI can be generated that integrates with the legacy application in a non-invasive manner.  The intellectual merit of the projects contributions is focused on techniques and supporting tools that assist in mapping a GUI to a VUI, including the ability to record the locations of existing GUI widgets and an underlying PBV engine that integrates a speech recognizer with programmatic control of the mouse and keyboard. The broad impacts of the project are concerned with the opportunities that are provided to children and others with mobility challenges who want to learn computer programming. The following are summary highlights from the project:  A project web page is available at http://myna.cs.ua.edu/  The main contribution of the project is the Myna framework, which provides a means to link a speech recognition system to an underlying engine that provides programmatic control of the mouse and keyboard. The final version of the project permits the full use of drag and drop features within several different programming environments.  Myna has been applied to Scratch (both desktop and web), Snap! (BYOB and web), Lego Mindstorms NXT, Code.org Hour of Code puzzles, and two Blockly languages (Pixly and Spherly). The project also investigated the creation of a semi-automated tool that could be used to collect specific information (e.g., xy-coordinate, name, type, behavior, etc.) about various components within a GUI. This tool, Myna Scraper, allows the user to click through the process of data collection, which is then stored in property files, rather than manually collect the information and create each individual file. The number of property files created depends on the number of components within the applica...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
