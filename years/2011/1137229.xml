<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>EFRI-M3C: Partnered Rehabilitative Movement: Cooperative Human-robot Interactions for Motor Assistance, Learning, and Communication</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2011</AwardEffectiveDate>
<AwardExpirationDate>08/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>2000000.00</AwardTotalIntnAmount>
<AwardAmount>2000000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07040000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>EFMA</Abbreviation>
<LongName>Emerging Frontiers &amp; Multidisciplinary Activities</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Radhakisan Baheti</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>Our vision is to develop caregiver robots that interact fluidly and flexibly with humans during functional&lt;br/&gt;motor activities, while providing motor assistance, enhancement, and communication to facilitate motor&lt;br/&gt;learning. However, we currently lack theories to understand how rehabilitation and movement therapists&lt;br/&gt;provide timely and appropriate physical feedback and assistance to improve mobility in individuals with&lt;br/&gt;motor impairments. To develop devices that could accompany an individual as both assistant and&lt;br/&gt;movement therapist, our goal is to study human motor coordination during cooperative physical&lt;br/&gt;interactions with a humanoid assistive robot. We will use rehabilitative partner dance as a paradigm to&lt;br/&gt;examine a sensory-motor theory of cooperative physical interactions relevant to walking and other&lt;br/&gt;functional motor activities. We will use a "partnered box step", a constrained and defined pattern of&lt;br/&gt;weight shifts and directional changes, as a paradigm for a cooperative physical interaction with welldefined&lt;br/&gt;motor goals.&lt;br/&gt;Objectives: To 1) experimentally verify a hierarchical theory of human sensory-motor control and&lt;br/&gt;learning and 2) develop predictive models of whole-body human movement for cooperative physical&lt;br/&gt;interactions with machines. Over four years, we will test our models by demonstrating the successful&lt;br/&gt;participation of the robot in a box step as leader or follower and adapt its movements to the motor skill&lt;br/&gt;level of a human partner.&lt;br/&gt;Intellectual Merit: Our work will provide transformative experimental, theoretical, and practical&lt;br/&gt;interdisciplinary frameworks that will forge new paths toward autonomous cooperative robots with&lt;br/&gt;physical intelligence to enhance, assist, and improve motor skills in humans with varying motor&lt;br/&gt;capabilities. These advances will aid prosthetic and robotic design and may advance our understanding of&lt;br/&gt;the brain.&lt;br/&gt;Broader Impacts: The expected project outcomes would have long-term impact on the quality of life of&lt;br/&gt;millions of Americans by improving fitness, motor skills, and social engagement. Applications include&lt;br/&gt;healthcare devices or sports robots that entertain and improve fitness. We will provide seminars on&lt;br/&gt;mobility-related issues and rehabilitative dance instruction to older adult living communities and&lt;br/&gt;populations with motor impairments. The broad appeal and social nature of this work will likely garner&lt;br/&gt;media publicity that will increase public interest in science and technology.</AbstractNarration>
<MinAmdLetterDate>09/01/2011</MinAmdLetterDate>
<MaxAmdLetterDate>09/01/2011</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1137229</AwardID>
<Investigator>
<FirstName>Lena</FirstName>
<LastName>Ting</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Lena H Ting</PI_FULL_NAME>
<EmailAddress>lting@emory.edu</EmailAddress>
<PI_PHON>4047272744</PI_PHON>
<NSF_ID>000095985</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>C. Karen</FirstName>
<LastName>Liu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>C. Karen Liu</PI_FULL_NAME>
<EmailAddress>karenliu@cs.stanford.edu</EmailAddress>
<PI_PHON>4048252745</PI_PHON>
<NSF_ID>000430108</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Charles</FirstName>
<LastName>Kemp</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Charles C Kemp</PI_FULL_NAME>
<EmailAddress>charlie.kemp@bme.gatech.edu</EmailAddress>
<PI_PHON>4047252488</PI_PHON>
<NSF_ID>000367160</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Madeleine</FirstName>
<LastName>Hackney</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Madeleine Hackney</PI_FULL_NAME>
<EmailAddress>mehackn@emory.edu</EmailAddress>
<PI_PHON>4047272503</PI_PHON>
<NSF_ID>000579506</NSF_ID>
<StartDate>09/01/2011</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Emory University</Name>
<CityName>Atlanta</CityName>
<ZipCode>303224250</ZipCode>
<PhoneNumber>4047272503</PhoneNumber>
<StreetAddress>1599 Clifton Rd NE, 4th Floor</StreetAddress>
<StreetAddress2><![CDATA[1599-001-1BA]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<StateCode>GA</StateCode>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>GA05</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>066469933</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EMORY UNIVERSITY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>066469933</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Georgia Institute of Technology]]></Name>
<CityName>Atlanta</CityName>
<StateCode>GA</StateCode>
<ZipCode>303180535</ZipCode>
<StreetAddress><![CDATA[313 Ferst Dr NW]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Georgia</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>05</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>GA05</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7633</Code>
<Text>EFRI Research Projects</Text>
</ProgramElement>
<ProgramReference>
<Code>7633</Code>
<Text>EFRI RESEARCH PROJECTS</Text>
</ProgramReference>
<Appropriation>
<Code>0111</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2011~2000000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Our highly collaborative interdisciplinary team advanced both fundamental science and engineering toward the yet-unrealized vision of seamless physical interactions between humans and machines to improve our work, health, and recreation. &nbsp;The research team had expertise spanning mobile robotics, human-robot interaction, human biomechanics and neuroscience, physical rehabilitation, and computer animation. Our focus was on autonomous (non-wearable) robots that can touch and interact physically with humans.</p> <p>Our work provides fundamental new knowledge about how humans can interact physically at the hands to coordinate their walking with a robot or another person to improve their gait. Specifically, we showed that a humanoid robot move in coordination with a person simply by the forces at the hand. We found that relatively small forces at the hands could communicate rich information about intended movement direction and speed, skill level, and movement errors between two people in the absence of visual or auditory cues. Further, we showed that older adults are accepting of robot technologies for exercise-based robotic rehabilitation.</p> <p>We advanced technologies and algorithms to improve the ability of robots to physically interact with humans and explore unknown objects in the environment. We developed sensors and algorithms to allow a robot to explore a cluttered environment by pushing on objects, inferring their properties, and altering their movement to adapt to soft and hard objects. Further, a novel robotic sensing device allowed humans and objects to be distinguished through touch using force and thermal sensing.</p> <p>To improve the ability to design more intuitive physical interactions between humans and robots, we developed computer simulation algorithm of physical interactions between humans and both stationary and moving objects.</p> <p>Finally, as a simple example of an intuitive assistive physical interaction between a human and machine during walking, we developed a prototype of energy-recycling assistive stairs that store and return energy to a human user. The stairs store energy during when walking down the steps, cushioning the descent, and release energy when walking up the steps, assisting the ascent. &nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/22/2017<br>      Modified by: Lena&nbsp;H&nbsp;Ting</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386681840_DSC_0202--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386681840_DSC_0202--rgov-800width.jpg" title="Energy-Recycling Assistive Stairs"><img src="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386681840_DSC_0202--rgov-66x44.jpg" alt="Energy-Recycling Assistive Stairs"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Energy-recycling assistive stairs reduce joint torque in both ascending and descending stairs.From: Song, Y.S., Ha, S., Shu, H., Ting, L.H., Liu, C.K. (2017) Stair Negotiation Made Easier Using Novel Interactive Energy-Recycling Assistive Stairs, PLoS ONE, Jul 12;12(7):e0179637</div> <div class="imageCredit">Y.S. Song</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lena&nbsp;H&nbsp;Ting</div> <div class="imageTitle">Energy-Recycling Assistive Stairs</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386342484_human-humanstepping--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386342484_human-humanstepping--rgov-800width.jpg" title="Human-Human Partnered Stepping"><img src="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386342484_human-humanstepping--rgov-66x44.jpg" alt="Human-Human Partnered Stepping"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Small force interactions at the hand communicate rich information to coordinate stepping between two humans. From Sawers, A., Bhattacharjee, T., McKay, J.L., Hackney, M.E., Kemp C.C., Ting, L.H. (2017) Small Forces Can Communicate Movement Goals and Distinguish Expert and Novice Human-Human Phy</div> <div class="imageCredit">A. Sawers</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lena&nbsp;H&nbsp;Ting</div> <div class="imageTitle">Human-Human Partnered Stepping</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386551592_OAAcceptance--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386551592_OAAcceptance--rgov-800width.jpg" title="Older Adult Human-Robot Partnered Stepping"><img src="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386551592_OAAcceptance--rgov-66x44.jpg" alt="Older Adult Human-Robot Partnered Stepping"></a> <div class="imageCaptionContainer"> <div class="imageCaption">Older adults are accepting of a  robot for exercised-based therapies to improve mobility. From: Chen, T.L., Beer, J.M., Bhattacharjee, T., Ting, L.H., Hackney, Rogers, W.A, Kemp, C.C. (2017) Older Adults? Acceptance of a Robot for Partner Dance-Based Exer1.Exercise. PLoS ONE, Â 12 (10), e0182736</div> <div class="imageCredit">T. Chen</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lena&nbsp;H&nbsp;Ting</div> <div class="imageTitle">Older Adult Human-Robot Partnered Stepping</div> </div> </li> <li> <a href="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386105798_Figure1_exp_setup--rgov-214x142.jpg" original="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386105798_Figure1_exp_setup--rgov-800width.jpg" title="Human-Robot Partnered Stepping"><img src="/por/images/Reports/POR/2017/1137229/1137229_10130186_1511386105798_Figure1_exp_setup--rgov-66x44.jpg" alt="Human-Robot Partnered Stepping"></a> <div class="imageCaptionContainer"> <div class="imageCaption">A robot followers moves with a human leader based solely on force interactions at the hands.From Chen, T.L., Bhattacharjee, T., McKay, J.L., Hackney, M.E., Borinski, Ting, L.H., Kemp, C.C., (2015) Dance with me: Human-robot partnered stepping based on haptic communication. PLoS ONE, 10(5):e01251</div> <div class="imageCredit">T. Chen</div> <div class="imagePermisssions">Creative Commons</div> <div class="imageSubmitted">Lena&nbsp;H&nbsp;Ting</div> <div class="imageTitle">Human-Robot Partnered Stepping</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Our highly collaborative interdisciplinary team advanced both fundamental science and engineering toward the yet-unrealized vision of seamless physical interactions between humans and machines to improve our work, health, and recreation.  The research team had expertise spanning mobile robotics, human-robot interaction, human biomechanics and neuroscience, physical rehabilitation, and computer animation. Our focus was on autonomous (non-wearable) robots that can touch and interact physically with humans.  Our work provides fundamental new knowledge about how humans can interact physically at the hands to coordinate their walking with a robot or another person to improve their gait. Specifically, we showed that a humanoid robot move in coordination with a person simply by the forces at the hand. We found that relatively small forces at the hands could communicate rich information about intended movement direction and speed, skill level, and movement errors between two people in the absence of visual or auditory cues. Further, we showed that older adults are accepting of robot technologies for exercise-based robotic rehabilitation.  We advanced technologies and algorithms to improve the ability of robots to physically interact with humans and explore unknown objects in the environment. We developed sensors and algorithms to allow a robot to explore a cluttered environment by pushing on objects, inferring their properties, and altering their movement to adapt to soft and hard objects. Further, a novel robotic sensing device allowed humans and objects to be distinguished through touch using force and thermal sensing.  To improve the ability to design more intuitive physical interactions between humans and robots, we developed computer simulation algorithm of physical interactions between humans and both stationary and moving objects.  Finally, as a simple example of an intuitive assistive physical interaction between a human and machine during walking, we developed a prototype of energy-recycling assistive stairs that store and return energy to a human user. The stairs store energy during when walking down the steps, cushioning the descent, and release energy when walking up the steps, assisting the ascent.            Last Modified: 11/22/2017       Submitted by: Lena H Ting]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
