<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CIF:Medium:Space-from-Time Imaging: Fundamental Limits, Algorithms, and Preliminary Demonstrations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>04/01/2012</AwardEffectiveDate>
<AwardExpirationDate>03/31/2015</AwardExpirationDate>
<AwardTotalIntnAmount>750000.00</AwardTotalIntnAmount>
<AwardAmount>750000</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>CCF</Abbreviation>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>John Cozzens</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>For centuries, the primary technical meaning of image has been a visual representation or counterpart, formed through the interaction of light with mirrors and lenses, and recorded through a photochemical process.  In digital photography, the photochemical process has been replaced by a sensor array, but the use of optical elements is unchanged.  Thus, the spatial resolution in this traditional imaging is limited by the quality of the optics and the number of sensors in the array.  This project develops the foundations of achieving spatial resolution, in 2D and 3D, by measuring temporal variations of light intensity in response to temporally- or spatiotemporally-varying illumination.  This is a radical departure from traditional imaging, in which time is associated only with fixing the period over which light must be collected to achieve the desired contrast.  Reducing requirements for lenses, mirrors, and numbers of sensors can both lower costs and enable entirely new imaging configurations.  Of particular interest is to enable 3D capture in mobile devices such as cell phones.&lt;br/&gt;&lt;br/&gt;Space-from-time imaging (SFTI) is based on the recognition that information of interest in a scene, such as bidirectional reflectance distribution functions at various wavelengths and distances from the imaging device, are embedded in the transfer function from a light source to a light sensor.  Furthermore, light transfer is linear.  Thus, SFTI introduces new inverse problems with at least portions of the forward models being linear.  The investigators will apply and extend analysis techniques developed for other imaging methods, such as computed tomography and synthetic aperture radar, to develop theoretical foundations for SFTI.  Analysis will inspire and be informed by proof-of-concept experiments.</AbstractNarration>
<MinAmdLetterDate>03/02/2012</MinAmdLetterDate>
<MaxAmdLetterDate>03/05/2014</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1161413</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Shapiro</LastName>
<PI_MID_INIT>H</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey H Shapiro</PI_FULL_NAME>
<EmailAddress>jhs@mit.edu</EmailAddress>
<PI_PHON>6172534179</PI_PHON>
<NSF_ID>000188661</NSF_ID>
<StartDate>03/02/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Ngai</FirstName>
<LastName>Wong</LastName>
<PI_MID_INIT>C</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Ngai C Wong</PI_FULL_NAME>
<EmailAddress>ncw@mit.edu</EmailAddress>
<PI_PHON>6172538131</PI_PHON>
<NSF_ID>000157086</NSF_ID>
<StartDate>03/02/2012</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Vivek</FirstName>
<LastName>Goyal</LastName>
<PI_MID_INIT>K</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Vivek K Goyal</PI_FULL_NAME>
<EmailAddress>vgoyal@mit.edu</EmailAddress>
<PI_PHON>6173240367</PI_PHON>
<NSF_ID>000392466</NSF_ID>
<StartDate>03/02/2012</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Massachusetts Institute of Technology</Name>
<CityName>Cambridge</CityName>
<ZipCode>021394301</ZipCode>
<PhoneNumber>6172531000</PhoneNumber>
<StreetAddress>77 MASSACHUSETTS AVE</StreetAddress>
<StreetAddress2><![CDATA[NE18-901]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<StateCode>MA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001425594</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>MASSACHUSETTS INSTITUTE OF TECHNOLOGY</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001425594</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Massachusetts Institute of Technology]]></Name>
<CityName/>
<StateCode>MA</StateCode>
<ZipCode>021394301</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Massachusetts</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7797</Code>
<Text>Comm &amp; Information Foundations</Text>
</ProgramElement>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7936</Code>
<Text>SIGNAL PROCESSING</Text>
</ProgramReference>
<Appropriation>
<Code>0112</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0114</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2012~491476</FUND_OBLG>
<FUND_OBLG>2014~258524</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>A traditional camera uses lenses to form an optical image of the scene and thus obtain spatial correspondences between the scene and film or a digital sensor array.&nbsp; The film or sensor array has an integration time but only one value per spatial location &ndash; no time resolution. &nbsp;Thus, even though the traditional camera may use short exposures or flash illuminations to effectively &ldquo;stop time,&rdquo; it is not exploiting time to change how the scene is sensed.&nbsp; Lidar systems and time-of-flight cameras use the finite speed of light to measure distances through delays or phase shifts, but they do not exploit time for intensity estimation.</p> <p>&nbsp;This project introduced several signal processing methods that create new imaging capabilities from existing components, with a common theme of making new uses of temporal variations in measured light.&nbsp; The implications range from specialized long-range imaging systems to the potential for low-cost mass-market electronics, which inspired a successful start-up company.&nbsp; The project also introduced new methods and analyses to improve some more conventional optical imaging systems and to mimic a quantum imaging phenomenon with a more-efficient classical system.</p> <p>&nbsp;</p> <p><strong>First-photon imaging and other imaging with few photons.</strong>&nbsp; A lidar system illuminates a scene pixel-by-pixel with a stream of laser pulses and measures the back-reflected light.&nbsp; The most photon-efficient systems detect individual photons and form histograms of the time delays between transmitted and detected pulses.&nbsp; Range information is found from the histogram peak and relative reflectivity from the number of detections.&nbsp; While tens of photon detections per pixel could suffice for range imaging when background light is inconsequential, at least hundreds are needed for accurate reflectivity imaging, even in the absence of background light.&nbsp; This project introduced <strong>first-photon imaging</strong> (FPI) as a way to form range and reflectivity images simultaneously from only <strong>one detected photon per pixel</strong>, despite half the detections being due to background light and dark counts.&nbsp; Each scene pixel is illuminated repeatedly until a photon is detected, with the intensity of illumination and ambient light such that, on average over the whole scene, about 1% of pulse repetition periods have a detection; this detection rate is in the typical range for lidar.&nbsp; FPI introduces the time until the first detection as an indication of inverse reflectivity.&nbsp; It also uses effective regularization and approximate separation of signal- and background-based detections.&nbsp; The result is a dramatic improvement in imaging from very few photons (see image 1).</p> <p>The project also introduced other computational methods for imaging with few photons.&nbsp; Similar modeling and algorithms could achieve comparable image quality at around 1 detected photon per pixel, with the advantage of a fixed, deterministic acquisition time at each pixel.</p> <p>&nbsp;</p> <p><strong>Classical imaging with undetected photons.&nbsp; </strong>Light is intrinsically quantum mechanical, and photodetection is a quantum measurement. &nbsp;Consequently, <em>all</em> imaging is really quantum mechanical. &nbsp;It has long been known, however, that the semiclassical theory of photodetection &ndash; in which light is a classical field and the discreteness of the electron charge results in photodetection shot noise &ndash; predicts measurement statistics identical to those obtained from quantum theory when the illumination is in a classical state.&nbsp; Thus, there is value in reserving the term &ldquo;quantum imaging&rdquo; for imagers whose understanding requires quantum theory.&nbsp; Furthermore, classical imaging can use high-brightness pseudothermal sources, making it much mor...]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ A traditional camera uses lenses to form an optical image of the scene and thus obtain spatial correspondences between the scene and film or a digital sensor array.  The film or sensor array has an integration time but only one value per spatial location &ndash; no time resolution.  Thus, even though the traditional camera may use short exposures or flash illuminations to effectively "stop time," it is not exploiting time to change how the scene is sensed.  Lidar systems and time-of-flight cameras use the finite speed of light to measure distances through delays or phase shifts, but they do not exploit time for intensity estimation.   This project introduced several signal processing methods that create new imaging capabilities from existing components, with a common theme of making new uses of temporal variations in measured light.  The implications range from specialized long-range imaging systems to the potential for low-cost mass-market electronics, which inspired a successful start-up company.  The project also introduced new methods and analyses to improve some more conventional optical imaging systems and to mimic a quantum imaging phenomenon with a more-efficient classical system.     First-photon imaging and other imaging with few photons.  A lidar system illuminates a scene pixel-by-pixel with a stream of laser pulses and measures the back-reflected light.  The most photon-efficient systems detect individual photons and form histograms of the time delays between transmitted and detected pulses.  Range information is found from the histogram peak and relative reflectivity from the number of detections.  While tens of photon detections per pixel could suffice for range imaging when background light is inconsequential, at least hundreds are needed for accurate reflectivity imaging, even in the absence of background light.  This project introduced first-photon imaging (FPI) as a way to form range and reflectivity images simultaneously from only one detected photon per pixel, despite half the detections being due to background light and dark counts.  Each scene pixel is illuminated repeatedly until a photon is detected, with the intensity of illumination and ambient light such that, on average over the whole scene, about 1% of pulse repetition periods have a detection; this detection rate is in the typical range for lidar.  FPI introduces the time until the first detection as an indication of inverse reflectivity.  It also uses effective regularization and approximate separation of signal- and background-based detections.  The result is a dramatic improvement in imaging from very few photons (see image 1).  The project also introduced other computational methods for imaging with few photons.  Similar modeling and algorithms could achieve comparable image quality at around 1 detected photon per pixel, with the advantage of a fixed, deterministic acquisition time at each pixel.     Classical imaging with undetected photons.  Light is intrinsically quantum mechanical, and photodetection is a quantum measurement.  Consequently, all imaging is really quantum mechanical.  It has long been known, however, that the semiclassical theory of photodetection &ndash; in which light is a classical field and the discreteness of the electron charge results in photodetection shot noise &ndash; predicts measurement statistics identical to those obtained from quantum theory when the illumination is in a classical state.  Thus, there is value in reserving the term "quantum imaging" for imagers whose understanding requires quantum theory.  Furthermore, classical imaging can use high-brightness pseudothermal sources, making it much more practical, including reducing image acquisition time.  For example, first-photon imaging is not quantum imaging under this definition.  This project created a classical analogue to Barreto Lemos et al.Ã†s quantum imaging with undetected photons.  It is a striking accomplishment because the photons that interact ...]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
