<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Perceiving high-level relations</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/15/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>527767.00</AwardTotalIntnAmount>
<AwardAmount>527767</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Hout</SignBlockName>
<PO_EMAI>mhout@nsf.gov</PO_EMAI>
<PO_PHON>7032922163</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The world contains not only objects and features, but also relations holding between them. When a piece of fruit is in a bowl, and the bowl is on a table, we appreciate not only the visual appearance of the individual objects (e.g., a red apple, a wooden bowl), but also the relations "containment" (in) and "support" (on). A surprising number of everyday tasks depend on these and other relational representations, such as assembling furniture, packing a suitcase, reading a medical chart, or navigating a scene. How does the mind represent visual relations themselves, beyond the objects participating in them? The work proposed here explores an exciting hypothesis about how the mind extracts relations from images —— namely, that relations are properly *perceived*, in a fast and automatic manner akin to the perception of more familiar visual properties such as size, color, or shape. Across multiple case studies —— including the perception of fit, balance, containment, support, adhesion, enclosure, and more —— this work will take a psychophysical approach to the perception of relations, asking whether relational perception proceeds rapidly, automatically, reflexively, and in ways that interact with other perceptual processes.&lt;br/&gt;&lt;br/&gt;The work proposed here has three primary aims: (1) To characterize the kinds of relations we perceive; (2) To understand how such relations are extracted by the mind; and (3) To elucidate their function in the mind at large. First, what kinds of relations can we see? Previous work has focused mostly on basic geometric and spatial relations (such as being beside, above, or behind); but objects in the world are related to each other in far richer ways. Here, the investigator will catalog the kinds of relations that appear in perception, with a special focus on “force-dynamic” relations, including combining, containing, supporting, balancing, covering, tying, connecting, hanging, and other relations in which objects exert physical forces on one another. Second, do we only consider, judge, or infer the relations between objects? Or can we also see them directly? The next aim of this proposal is to investigate the nature of relational perception, by asking whether the mind extracts visual relations in ways that show signatures of genuinely perceptual processing, such as speed, automaticity, reflexiveness (or cognitive impenetrability), and interaction with other perceptual processes. Third, why do we perceive relations at all? Are they just curious quirks of the mind, or do they support other kinds of knowledge? For example, once we see that two objects are connected, do our minds automatically predict that tugging on one will bring the other along? Once we see that two object-parts can combine into a whole, is it easier to remember or count such objects? The final aim of this project is to explore how relational perception supports other sophisticated kinds of mental processing, including automatic prediction of physical contingencies between objects.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/24/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2021053</AwardID>
<Investigator>
<FirstName>Chaz</FirstName>
<LastName>Firestone</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Chaz Firestone</PI_FULL_NAME>
<EmailAddress>chaz@jhu.edu</EmailAddress>
<PI_PHON>4105160371</PI_PHON>
<NSF_ID>000784505</NSF_ID>
<StartDate>07/24/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Johns Hopkins University</Name>
<CityName>Baltimore</CityName>
<ZipCode>212182686</ZipCode>
<PhoneNumber>4439971898</PhoneNumber>
<StreetAddress>1101 E 33rd St</StreetAddress>
<StreetAddress2><![CDATA[Suite B001]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<StateCode>MD</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>MD07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>001910777</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>JOHNS HOPKINS UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>001910777</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[JOHNS HOPKINS UNIVERSITY]]></Name>
<CityName>Baltimore</CityName>
<StateCode>MD</StateCode>
<ZipCode>212182685</ZipCode>
<StreetAddress><![CDATA[3400 North Charles Street]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Maryland</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>MD07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~527767</FUND_OBLG>
</Award>
</rootTag>
