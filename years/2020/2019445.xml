<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Attentional Guidance in Real-World Scenes: The Role of Meaning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/15/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2023</AwardExpirationDate>
<AwardTotalIntnAmount>562823.00</AwardTotalIntnAmount>
<AwardAmount>562823</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Michael Hout</SignBlockName>
<PO_EMAI>mhout@nsf.gov</PO_EMAI>
<PO_PHON>7032922163</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Real-world scenes comprise a blooming, buzzing confusion of information. Yet at any given moment, we can only perceive and understand a small portion of that information. What we see and understand is quite literally determined by where we look. But what determines where we look? This project seeks to answer this important question. The project investigates the idea that the meaning of a scene plays the key role in guiding our eyes. If our hypothesis is correct, then we should find that meaning predicts where people look. Such a result will advance scientific knowledge of how our brains and minds work. The results will also be useful to applied areas of computer science and engineering, contributing to increased US economic competitiveness. High-tech applications include virtual and augmented reality, artificial vision and gaze-based input systems, baggage screening, medical image assessment, satellite image analysis, and other computer-based vision systems. The project will also contribute to training of culture- and gender-diverse students and researchers in these high-tech and scientific fields, advancing the development of a diverse, globally competitive workforce. &lt;br/&gt;&lt;br/&gt;Attentional guidance during scene viewing draws on both the visual properties of the scene and its semantic content. Although the role of visual properties (e.g., physical salience) on attentional guidance has been extensively studied, far less is known about how the semantic content of a scene guides attention. Recent work in my lab indicates that the influence of physical saliency is reduced or even eliminated when semantic content is available to guide attention. These findings have opened up important new questions about how and when meaning guides attention in scenes. The central idea behind the current work is to address these questions with a set of targeted experiments. The proposed experiments will integrate new methods for representing the meaning of local objects and scene regions with high-resolution eyetracking to measure the influence of knowledge on attentional guidance in real-world scenes. The research includes experiments motivated by four goals: (1) To compare the roles of contextualized versus context-free local scene meaning in attentional guidance. (2) To evaluate the causal relationship between local scene meaning and attentional guidance. (3) To test the generality of guidance by scene meaning across tasks and viewing time. (4) To compare the roles of physical versus semantic features in scenes using search target templates. The research will establish the foundation for a new theoretical approach to the representation of scene meaning and attentional guidance in scenes. The ultimate objective is to develop a theory of attentional guidance in scenes based on guidance by meaning.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/24/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/24/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2019445</AwardID>
<Investigator>
<FirstName>John</FirstName>
<LastName>Henderson</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>John M Henderson</PI_FULL_NAME>
<EmailAddress>johnhenderson@ucdavis.edu</EmailAddress>
<PI_PHON>8037774137</PI_PHON>
<NSF_ID>000104587</NSF_ID>
<StartDate>07/24/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of California-Davis</Name>
<CityName>Davis</CityName>
<ZipCode>956186134</ZipCode>
<PhoneNumber>5307547700</PhoneNumber>
<StreetAddress>OR/Sponsored Programs</StreetAddress>
<StreetAddress2><![CDATA[1850 Research Park Dr., Ste 300]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>047120084</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF CALIFORNIA, DAVIS</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>071549000</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of California-Davis]]></Name>
<CityName>Davis</CityName>
<StateCode>CA</StateCode>
<ZipCode>956185412</ZipCode>
<StreetAddress><![CDATA[267 Cousteau Pl]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~562823</FUND_OBLG>
</Award>
</rootTag>
