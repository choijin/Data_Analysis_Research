<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Visuospatial modulation of bimanual touch perception in real and virtual environments</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>08/01/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2023</AwardExpirationDate>
<AwardTotalIntnAmount>379229.00</AwardTotalIntnAmount>
<AwardAmount>379229</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Betty Tuller</SignBlockName>
<PO_EMAI>btuller@nsf.gov</PO_EMAI>
<PO_PHON>7032927238</PO_PHON>
</ProgramOfficer>
<AbstractNarration>In order to use our hands to manipulate objects, the nervous system must be able to coordinate and interpret a large amount of disparate sensory information about the position of our hands, the arrangement and positioning of the fingers, and what each is contacting. The computations are complicated by the fact that the information from the two hands must be interpreted relative to their spatial position. For example, the left and right index fingers occupy adjacent spaces when the hands are held together but the fingers can occupy vastly different spaces when the arms are spread apart. In these different situations, we can conceivably rely on our sense of vision to help interpret our sense of touch. Vision can provide information about where our hands are in space as well as what objects we touch and manipulate. Despite the obvious importance of vision for interpreting touch information from the hands, the specific ways in which the visual influence occurs remains poorly understood. This research is aimed at investigating the effects of simple and complex visual cues on touch information from the two hands in bimanual tasks. Understanding bimanual touch processing will facilitate effective nonverbal physical communication between people and intelligent machines and it will support the development of highly dexterous robotic systems, advanced neuroprosthetics, and sensorimotor rehabilitation strategies. Establishing how vision influences our perception of our body in real and virtual environments has clear implications for societyâ€™s use of immersive and interactive technologies. The project will also promote the participation and education of young women in neuroscience and related STEM fields.  &lt;br/&gt; &lt;br/&gt;This research program uses rigorous psychophysics, virtual reality technologies, and computational modeling to elucidate the effects of simple and complex visuospatial cues on the perception of bimanual touch. Behavioral experiments will be performed to quantify the effects of simple light flashes on tactile perception and learning. Neural network models will be fitted to the behavioral data to make inferences about how the nervous system mediates visuotactile and spatial interactions and how neural circuits may be modified through learning. The effects of complex visual cues on proprioception and bimanual touch will also be examined by performing psychophysics experiments in virtual reality. Specifically, visual proprioception cues and virtual object information will be altered in a virtual reality design, allowing the investigator to test hypotheses regarding multisensory integration for proprioception and causal inference processing. Collectively, the research program will yield novel insight into how visual information modulates the perception of tactile cues experienced over the two hands.&lt;br/&gt;&lt;br/&gt;Co-funded by the M3X (Engineering) and PAC (Social, Behavioral, and Economic Sciences) Programs&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/29/2020</MinAmdLetterDate>
<MaxAmdLetterDate>07/29/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2019959</AwardID>
<Investigator>
<FirstName>Jeffrey</FirstName>
<LastName>Yau</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jeffrey Yau</PI_FULL_NAME>
<EmailAddress>jeffrey.yau@bcm.edu</EmailAddress>
<PI_PHON>7137985150</PI_PHON>
<NSF_ID>000700328</NSF_ID>
<StartDate>07/29/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Baylor College of Medicine</Name>
<CityName>HOUSTON</CityName>
<ZipCode>770303411</ZipCode>
<PhoneNumber>7137981297</PhoneNumber>
<StreetAddress>ONE BAYLOR PLAZA</StreetAddress>
<StreetAddress2><![CDATA[MS-310]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<StateCode>TX</StateCode>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>TX09</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>051113330</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>BAYLOR COLLEGE OF MEDICINE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>051113330</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Baylor College of Medicine]]></Name>
<CityName>Houston</CityName>
<StateCode>TX</StateCode>
<ZipCode>770303411</ZipCode>
<StreetAddress><![CDATA[One Baylor Plaza]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Texas</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>09</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>TX09</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>058y</Code>
<Text/>
</ProgramElement>
<ProgramElement>
<Code>7252</Code>
<Text>Perception, Action &amp; Cognition</Text>
</ProgramElement>
<ProgramReference>
<Code>7252</Code>
<Text>Perception, Action and Cognition</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>REU SUPP-Res Exp for Ugrd Supp</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~379229</FUND_OBLG>
</Award>
</rootTag>
