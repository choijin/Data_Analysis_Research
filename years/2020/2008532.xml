<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Small: A Study of New Aggregate Losses for Machine Learning</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>10/01/2020</AwardEffectiveDate>
<AwardExpirationDate>12/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>449985.00</AwardTotalIntnAmount>
<AwardAmount>449985</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Rebecca Hwa</SignBlockName>
<PO_EMAI>rhwa@nsf.gov</PO_EMAI>
<PO_PHON>7032927148</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Machine learning is instrumental for the recent advances in AI and big data analysis. They have been used in almost every area of computer science and many fields of natural sciences, engineering, and social sciences. The central task of machine learning is to “train” a model, which entails seeking models that minimize certain performance metrics over a set of training examples. Such performance metrics are termed as the aggregate losses, which are to be distinguished from the individual losses that measures the quality of the model on a single training example. As the link between the training data and the model to be learned, the aggregate loss is a fundamental component in machine learning algorithms, and its theoretical and practical significance warrants a comprehensive and systematic study. The proposed work will focus on several fundamental research questions concerning the aggregate loss: are there any other types of aggregate loss beyond the average individual losses?; if so, what will be a general abstract formulation of these new aggregate loss?; how can the new aggregate losses be adapted to different machine learning problems?; and what are the statistical and computational behaviors of machine learning algorithms using the general aggregate losses?. &lt;br/&gt;&lt;br/&gt;The technical aims of the project are divided into four interrelated thrusts. The first thrust explores new types of rank-based aggregate losses for binary classification and study efficient algorithms optimizing learning objectives formed based upon them. The new aggregate losses will be applied to problems such as object detection, where rank-based evaluation metric is used dominantly. The second thrust aims to deepen our understanding of the binary classification algorithms developed using the rank-based aggregate losses and will be focused on a study of their statistical theories such as generalization and consistency. The third thrust will extend the study of new types of aggregate losses to other supervised problems (multi-class and multi-label learning and supervised metric learning) and unsupervised learning. The fourth thrust dedicates to the theoretical aspects of aggregate losses, in which an aggregate loss will be abstracted as a set function that maps the ensemble of individual losses to a number. This abstraction will be exploited to study the properties of new aggregate losses that make them superior than the average loss and propose new aggregate losses beyond rank-based ones.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>09/01/2020</MinAmdLetterDate>
<MaxAmdLetterDate>10/19/2020</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2008532</AwardID>
<Investigator>
<FirstName>Siwei</FirstName>
<LastName>Lyu</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Siwei Lyu</PI_FULL_NAME>
<EmailAddress>siweilyu@buffalo.edu</EmailAddress>
<PI_PHON>7166451587</PI_PHON>
<NSF_ID>000536963</NSF_ID>
<StartDate>09/01/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Yiming</FirstName>
<LastName>Ying</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Yiming Ying</PI_FULL_NAME>
<EmailAddress>yying@albany.edu</EmailAddress>
<PI_PHON>5184424613</PI_PHON>
<NSF_ID>000693504</NSF_ID>
<StartDate>09/01/2020</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>SUNY at Albany</Name>
<CityName>Albany</CityName>
<ZipCode>122220100</ZipCode>
<PhoneNumber>5184374974</PhoneNumber>
<StreetAddress>1400 WASHINGTON AVE MSC 100A</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NY20</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>152652822</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>RESEARCH FOUNDATION FOR THE STATE UNIVERSITY OF NEW YORK, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>020657151</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[SUNY at Albany]]></Name>
<CityName/>
<StateCode>NY</StateCode>
<ZipCode>122220001</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>20</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NY20</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~0</FUND_OBLG>
</Award>
</rootTag>
