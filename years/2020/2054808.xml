<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Deep Learning and Random Forests for High-Dimensional Regression</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2020</AwardEffectiveDate>
<AwardExpirationDate>07/31/2022</AwardExpirationDate>
<AwardTotalIntnAmount>157960.00</AwardTotalIntnAmount>
<AwardAmount>157960</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Pena Edsel</SignBlockName>
<PO_EMAI>epena@nsf.gov</PO_EMAI>
<PO_PHON>7032928080</PO_PHON>
</ProgramOfficer>
<AbstractNarration>This project aims to investigate two of the most widely used and state-of-the-art methods for high-dimensional regression: deep neural networks and random forests. Despite their widespread implementation, pinning down their theoretical properties has eluded researchers until recently. The proposed research aims to add to the growing body of literature on their analysis, by both developing tools of theoretical value and providing guarantees and guidance for practitioners and applied scientists who use these popular methods frequently in their work.&lt;br/&gt;&lt;br/&gt;The success of multi-layer networks has largely been buoyed by their ability to generalize well despite being able to fit most datasets, given enough parameters. This phenomenon is particularly striking when the input dimension is far greater than the available sample size, as is the case with many modern applications in molecular biology, medical imaging, and astrophysics, to name a few. A major component of the proposed work will be to obtain complexity bounds for classes of deep neural networks with controls on the size of their weights, which can then be used to bound generalization error and statistical risk. These complexity bounds reveal the role of complexity penalization, which is based on certain norms of the weights of the network. Motivated by these observations, another stream of the proposed research seeks to provide statistical guarantees of certain complexity penalized estimators and their adaptive properties. Current theoretical results for random forests are either for stylized versions of those that are used in practice or are asymptotic in nature and it is therefore difficult to determine the quality of convergence as a function of the parameters of the random forest. Furthermore, the setting for the analysis of more practical implementations of random forests is limited to structured, fixed-dimensional regression function classes. Given these restrictions, the first component of the proposal aims to investigate how random forests behave in the high-dimensional regime when the number of predictors grows with the sample size. Another research objective is to isolate and study families of flexible high-dimensional regression functions for which finite sample convergence rates can be established. The final endeavor of this project is to connect popular measures of variable importance  to the bias of random forests. Since variable importance measures are used for assessing the role each predictor variable plays in influencing the output, this connection will partially explain why random forests are adaptive to sparsity. The relationship will also help to theoretically motivate variable importance measures as useful tools for model interpretability.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>10/19/2020</MinAmdLetterDate>
<MaxAmdLetterDate>08/03/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2054808</AwardID>
<Investigator>
<FirstName>Jason</FirstName>
<LastName>Klusowski</LastName>
<PI_MID_INIT>M</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Jason M Klusowski</PI_FULL_NAME>
<EmailAddress>jason.klusowski@princeton.edu</EmailAddress>
<PI_PHON>6092583000</PI_PHON>
<NSF_ID>000791723</NSF_ID>
<StartDate>10/19/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[Off. of Research & Proj. Admin.]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1269</Code>
<Text>STATISTICS</Text>
</ProgramElement>
<ProgramReference>
<Code>075Z</Code>
<Text>Artificial Intelligence (AI)</Text>
</ProgramReference>
<ProgramReference>
<Code>079Z</Code>
<Text>Machine Learning Theory</Text>
</ProgramReference>
<Appropriation>
<Code>0119</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2019~10132</FUND_OBLG>
<FUND_OBLG>2020~73213</FUND_OBLG>
<FUND_OBLG>2021~74615</FUND_OBLG>
</Award>
</rootTag>
