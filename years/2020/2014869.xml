<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SBIR Phase I:  3D Markerless Motion Capture Technology For Gait Analysis During Rehabilitation</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2020</AwardEffectiveDate>
<AwardExpirationDate>06/30/2021</AwardExpirationDate>
<AwardTotalIntnAmount>224990.00</AwardTotalIntnAmount>
<AwardAmount>244990</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>07070000</Code>
<Directorate>
<Abbreviation>ENG</Abbreviation>
<LongName>Directorate For Engineering</LongName>
</Directorate>
<Division>
<Abbreviation>IIP</Abbreviation>
<LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Alastair Monk</SignBlockName>
<PO_EMAI>amonk@nsf.gov</PO_EMAI>
<PO_PHON>7032924392</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The broader impact/commercial potential of this Small Business Innovation Research (SBIR) Phase I project is to advance diagnosis and treatment of walking disorders and associated rehabilitation. An estimated 50 million Americans experience gait (walking) impairment due to injury, disease, and age, and more than 38,800 physical therapy clinics treat these patients. This project will develop an artificial intelligence system to extract gait metrics from video data from cameras surrounding a small area. A new diagnostic tool will track nuanced gait metrics throughout rehabilitation treatment. This technology will enable new and faster ways for physical therapists to precisely diagnose gait abnormalities and track treatment.&lt;br/&gt;&lt;br/&gt;This Small Business Innovation Research (SBIR) Phase I project could result in a system to diagnose gait deficiencies for people who suffer from neurological impairments. The proposed project will develop a markerless three-dimensional motion capture system to accurately diagnose gait pathologies in a time- and cost-efficient manner for clinicians. The project will: validate of the markerless motion capture system to ensure accurate measurement of raw kinematic metrics within 10% error of standard methods and potentially expand the system metrics; and conduct verification and validation processes.&lt;br/&gt;&lt;br/&gt;This award reflects NSF's statutory mission and has been deemed worthy of support through evaluation using the Foundation's intellectual merit and broader impacts review criteria.</AbstractNarration>
<MinAmdLetterDate>07/07/2020</MinAmdLetterDate>
<MaxAmdLetterDate>04/30/2021</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.041</CFDA_NUM>
<NSF_PAR_USE_FLAG>1</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>2014869</AwardID>
<Investigator>
<FirstName>Andrew</FirstName>
<LastName>Ekelem</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Andrew Ekelem</PI_FULL_NAME>
<EmailAddress>ae@evolutiondevices.com</EmailAddress>
<PI_PHON>9515334390</PI_PHON>
<NSF_ID>000782647</NSF_ID>
<StartDate>07/07/2020</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>EVOLUTION DEVICES, INC.</Name>
<CityName>BERKELEY</CityName>
<ZipCode>947041370</ZipCode>
<PhoneNumber>6619930685</PhoneNumber>
<StreetAddress>2150 SHATTUCK AVE FL PH</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA13</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>116985408</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>EVOLUTION DEVICES, INC.</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM/>
</Institution>
<Performance_Institution>
<Name><![CDATA[EVOLUTION DEVICES, INC.]]></Name>
<CityName/>
<StateCode>CA</StateCode>
<ZipCode>947041345</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>13</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA13</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>5371</Code>
<Text>SBIR Phase I</Text>
</ProgramElement>
<ProgramElement>
<Code>8091</Code>
<Text>SBIR Outreach &amp; Tech. Assist</Text>
</ProgramElement>
<ProgramReference>
<Code>8032</Code>
<Text>Software Services and Applications</Text>
</ProgramReference>
<Appropriation>
<Code>0120</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0121</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2020~224990</FUND_OBLG>
<FUND_OBLG>2021~20000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>&nbsp;</p> <p dir="ltr">Evolution Devices is developing a new system for 3D pose estimation using multiple economic web-cameras and computer vision. The system was built and compared to a gait laboratory system. The six cameras where installed on tripods and compared to a state of the art Vicon motion camera system. The Vicon system uses numerous reflective markers that need to be placed on each subject by an expert biomechanist. In contrast, the novel system is markerless and uses computer vision to detect joints from photos of people.&nbsp;</p> <p dir="ltr">As a result of the Phase I research and development the markerless prototype was capable of measuring joint positions within 35 mm of the Vicon system. This project evaluated the novel techniques on five healthy adults and one child subjects to demonstrate feasibility. The resulting joint angles were estimated within 5 degrees. This project demonstrates that simple optical cameras can be used to estimate human pose reasonably accurately.&nbsp;</p> <p dir="ltr">Furthermore, a graphical user interface (GUI) was developed for physical therapists during the Phase I NSF award. This GUI grants clinicians a simple, fast, and accurate measure of motion. The software post-processing capabilities facilitate clinical interpretation for the medical recording of motion assessments. Design considerations were also taken to secure personal health information while using cloud computing to keep the system costs low. Children's hospitals are the immediate use case for this technology because children are intolerable to the marker placement process. Furthermore, decreasing the costs of computational motion analysis will enable more patients to benefit from the insight it can provide clinical teams.</p> <p dir="ltr">Additionally, the system was evaluated through comparison to a wearable, sensor-based, motion tracking system. The error was larger between the wearable system and the novel camera system, an average of 21% for mean spatiotemporal metrics such as steplength, swing/stance ratio, and gait speed.&nbsp; The protocol also demonstrated that wearable error varies depending on walking pattern, meaning the wearable accuracy can vary across subjects and walking patterns, this deviation was not observed when comparing the two optical motion capture technologies. In conclusion, the novel optical motion capture system is thought to be more accurate that wearable sensor-based motion capture technology.</p> <p dir="ltr">Motion data is used by numerous industries, such as: physical rehabilitation, sports training, factory safety, and entertainment.&nbsp; Previous technologies are prohibitive in many settings due to upfront costs, long setup time, or steep learning curves. In this grant, Evolution Devices developed and validated a prototype system with minimal setup time per subject. Evolution Devices seeks to develop this system further to provide clinicians and physical therapists a qulaity tool for assessing patients.&nbsp;</p> <p dir="ltr">By taking advantage of motion capture, healthcare workers will improve diagnostics and monitoring of therapeutics by better understanding how patients move. An economical solution to motion capture will also enable significantly more researchers and clinicians to evaluate therapies and improve the healthcare outcomes.&nbsp; Additional industries such as ergonomics or entertainment, may also benefit from the capability to quickly record and measure objects, motion and scenes in three dimensions with high resolution.</p> <p dir="ltr">Although markerless motion tracking has been developed extensively in computer vision over the past few years, it remains to be widely applied for quantifying improvement in physical therapy settings. Evolution Devices identified and made progress on two key challenges that have received comparatively little attention. First, it showed that 3D markerless motion tracking with multiple optical cameras can produce accurate tracking, comparable to the gold standard marker-based Vicon system. Second, it prototyped an interface for clinicians to easily capture videos and inspect the metrics obtained from the motion tracking.&nbsp;</p> <div>&nbsp;</div> <p>&nbsp;</p> <p>&nbsp;</p><br> <p>            Last Modified: 08/11/2021<br>      Modified by: Andrew&nbsp;Ekelem</p> </div> <div class="porSideCol"> <div class="each-gallery"> <div class="galContent" id="gallery0"> <div class="photoCount" id="photoCount0">          Images (<span id="selectedPhoto0">1</span> of <span class="totalNumber"></span>)           </div> <div class="galControls" id="controls0"></div> <div class="galSlideshow" id="slideshow0"></div> <div class="galEmbox" id="embox"> <div class="image-title"></div> </div> </div> <div class="galNavigation" id="navigation0"> <ul class="thumbs" id="thumbs0"> <li> <a href="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628733042118_ChildFeasiblity--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628733042118_ChildFeasiblity--rgov-800width.jpg" title="EvoVision markerless tracking on child"><img src="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628733042118_ChildFeasiblity--rgov-66x44.jpg" alt="EvoVision markerless tracking on child"></a> <div class="imageCaptionContainer"> <div class="imageCaption">EvoVision was compared to a Vicon system to evaluate tracking accuracy. Feasibility was demonstrated on five adults and one child.</div> <div class="imageCredit">Evolution Devices</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Andrew&nbsp;Ekelem</div> <div class="imageTitle">EvoVision markerless tracking on child</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628732246513_ImagefromiOS(1)--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628732246513_ImagefromiOS(1)--rgov-800width.jpg" title="Conventional Marker Setup"><img src="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628732246513_ImagefromiOS(1)--rgov-66x44.jpg" alt="Conventional Marker Setup"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The lengthy set-up for conventional marker set ups are tedious procedures for expert biomechanists who place markers on anatomical landmarks. Children are particularly intolerable of the lengthy process that requires attention.</div> <div class="imageCredit">Evolution Devices</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Andrew&nbsp;Ekelem</div> <div class="imageTitle">Conventional Marker Setup</div> </div> </li> <li> <a href="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628733613323_GUI_sample--rgov-214x142.jpg" original="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628733613323_GUI_sample--rgov-800width.jpg" title="Graphical User Interface"><img src="/por/images/Reports/POR/2021/2014869/2014869_10683103_1628733613323_GUI_sample--rgov-66x44.jpg" alt="Graphical User Interface"></a> <div class="imageCaptionContainer"> <div class="imageCaption">The graphical user interface will enable clinicians to quickly collect, review, and publish computational gait analysis data.</div> <div class="imageCredit">Evolution Devices</div> <div class="imagePermisssions">Copyrighted</div> <div class="imageSubmitted">Andrew&nbsp;Ekelem</div> <div class="imageTitle">Graphical User Interface</div> </div> </li> </ul> </div> </div> </div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[   Evolution Devices is developing a new system for 3D pose estimation using multiple economic web-cameras and computer vision. The system was built and compared to a gait laboratory system. The six cameras where installed on tripods and compared to a state of the art Vicon motion camera system. The Vicon system uses numerous reflective markers that need to be placed on each subject by an expert biomechanist. In contrast, the novel system is markerless and uses computer vision to detect joints from photos of people.  As a result of the Phase I research and development the markerless prototype was capable of measuring joint positions within 35 mm of the Vicon system. This project evaluated the novel techniques on five healthy adults and one child subjects to demonstrate feasibility. The resulting joint angles were estimated within 5 degrees. This project demonstrates that simple optical cameras can be used to estimate human pose reasonably accurately.  Furthermore, a graphical user interface (GUI) was developed for physical therapists during the Phase I NSF award. This GUI grants clinicians a simple, fast, and accurate measure of motion. The software post-processing capabilities facilitate clinical interpretation for the medical recording of motion assessments. Design considerations were also taken to secure personal health information while using cloud computing to keep the system costs low. Children's hospitals are the immediate use case for this technology because children are intolerable to the marker placement process. Furthermore, decreasing the costs of computational motion analysis will enable more patients to benefit from the insight it can provide clinical teams. Additionally, the system was evaluated through comparison to a wearable, sensor-based, motion tracking system. The error was larger between the wearable system and the novel camera system, an average of 21% for mean spatiotemporal metrics such as steplength, swing/stance ratio, and gait speed.  The protocol also demonstrated that wearable error varies depending on walking pattern, meaning the wearable accuracy can vary across subjects and walking patterns, this deviation was not observed when comparing the two optical motion capture technologies. In conclusion, the novel optical motion capture system is thought to be more accurate that wearable sensor-based motion capture technology. Motion data is used by numerous industries, such as: physical rehabilitation, sports training, factory safety, and entertainment.  Previous technologies are prohibitive in many settings due to upfront costs, long setup time, or steep learning curves. In this grant, Evolution Devices developed and validated a prototype system with minimal setup time per subject. Evolution Devices seeks to develop this system further to provide clinicians and physical therapists a qulaity tool for assessing patients.  By taking advantage of motion capture, healthcare workers will improve diagnostics and monitoring of therapeutics by better understanding how patients move. An economical solution to motion capture will also enable significantly more researchers and clinicians to evaluate therapies and improve the healthcare outcomes.  Additional industries such as ergonomics or entertainment, may also benefit from the capability to quickly record and measure objects, motion and scenes in three dimensions with high resolution. Although markerless motion tracking has been developed extensively in computer vision over the past few years, it remains to be widely applied for quantifying improvement in physical therapy settings. Evolution Devices identified and made progress on two key challenges that have received comparatively little attention. First, it showed that 3D markerless motion tracking with multiple optical cameras can produce accurate tracking, comparable to the gold standard marker-based Vicon system. Second, it prototyped an interface for clinicians to easily capture videos and inspect the metrics obtained from the motion tracking.                Last Modified: 08/11/2021       Submitted by: Andrew Ekelem]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
