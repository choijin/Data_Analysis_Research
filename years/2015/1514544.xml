<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>RI: Medium: Collaborative Research: Understanding Individual-Level Speech Variability: From Novel Articulatory Data to Robust Speaker Recognition</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2020</AwardExpirationDate>
<AwardTotalIntnAmount>1199532.00</AwardTotalIntnAmount>
<AwardAmount>1199532</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tatiana Korelsky</SignBlockName>
<PO_EMAI>tkorelsk@nsf.gov</PO_EMAI>
<PO_PHON>7032928930</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Speech is a unique human capability. The vocal tract is the universal human instrument played with great dexterity and skill in the production of speech to convey rich linguistic and paralinguistic information.  The project will enable fundamental understanding of how individuals differ in their speech articulation due to differences in shape and size of their physical vocal instrument. Knowledge of how people differ in their speech production can help create improved automatic speaker recognition, technologies important for national security.  The project can inform design of technologies for robust speech-based access for all members of the population, including children, the elderly, and non-native speakers of a language. Results from the project can also assist in better understanding and treating disorders (e.g., cleft lip/palate), illness (e.g., head and neck cancer, apnea) or injury where human speech articulation is affected. The novel imaging data from 200 individuals, and associated  tools, annotations and interpretations created by the interdisciplinary team will be shared broadly with the scientific community. The project will provide a unique research training opportunity for students in integrated speech science and technology. &lt;br/&gt;&lt;br/&gt;The overarching goal of this project is to advance scientific understanding of how vocal tract morphology and speech articulation interact and explain the variant and invariant aspects of speech signal properties across talkers. Of particular scientific interest is the nature of articulatory strategies adopted by individuals in the presence of structural differences across them to achieve phonetic equivalence. Equally of interest are what aspects of, and how, vocal tract morphological differences are reflected in the acoustic speech signal, and if those differences can be estimated from speech acoustics. A crucial part of this goal is to create forward and inverse computational models that relate vocal tract details to speech acoustics toward shedding light on individual speaker differences and informing design of robust speaker recognition technologies. This project goes beyond state-of-the-art methods by focusing on direct investigation of the dynamic human vocal tract using novel imaging techniques and computational modeling to illuminate inter-speaker variability in vocal tract structure, as well as the strategies by which linguistic articulation is implemented. Using novel Magnetic Resonance Imaging with superior spatial resolution of the entire moving vocal tract that we helped develop (dynamic realtime 2D with excellent temporal resolution and accelerated volumetric 3D), the project will gather and quantify spatio-temporal details of speech production from 160 native American English covering the major dialectal regions of North America and 40 non-native speakers. The experimental, theoretical, and methodological approaches investigating the interplay between structure (shape and size) and function (dynamics of vocal-tract shaping and its acoustic consequences) can lead to new theoretical advances with improved phonetic characterizations of linguistic units that are general across speakers. It also offers the ability to explain individual specific speech patterns that can improve both understanding the scientific underpinning and creating robust automatic speaker recognition technology, enabling to determine not only that two talkers are different by the adoption of novel speaker dependent features, but also how and why they differ, by analyzing biologically-inspired details of structure and articulation.</AbstractNarration>
<MinAmdLetterDate>08/31/2015</MinAmdLetterDate>
<MaxAmdLetterDate>09/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1514544</AwardID>
<Investigator>
<FirstName>Shrikanth</FirstName>
<LastName>Narayanan</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Shrikanth S Narayanan</PI_FULL_NAME>
<EmailAddress>shri@sipi.usc.edu</EmailAddress>
<PI_PHON>2137406432</PI_PHON>
<NSF_ID>000377152</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Krishna</FirstName>
<LastName>Nayak</LastName>
<PI_MID_INIT>S</PI_MID_INIT>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Krishna S Nayak</PI_FULL_NAME>
<EmailAddress>knayak@usc.edu</EmailAddress>
<PI_PHON>2137403494</PI_PHON>
<NSF_ID>000266001</NSF_ID>
<StartDate>08/31/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Southern California</Name>
<CityName>Los Angeles</CityName>
<ZipCode>900890001</ZipCode>
<PhoneNumber>2137407762</PhoneNumber>
<StreetAddress>University Park</StreetAddress>
<StreetAddress2><![CDATA[3720 S. Flower St.]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<StateCode>CA</StateCode>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>CA37</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>072933393</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF SOUTHERN CALIFORNIA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>072933393</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Southern California]]></Name>
<CityName>Los Angeles</CityName>
<StateCode>CA</StateCode>
<ZipCode>900890001</ZipCode>
<StreetAddress><![CDATA[3740 McClintock Ave, EEB 430]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>California</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>37</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>CA37</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>7495</Code>
<Text>Robust Intelligence</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>7924</Code>
<Text>MEDIUM PROJECT</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0116</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0117</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~290875</FUND_OBLG>
<FUND_OBLG>2016~294432</FUND_OBLG>
<FUND_OBLG>2017~614225</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The overarching goal of this project is to advance scientific understanding of how vocal tract morphology and speech articulation interact and explain the variant and invariant aspects of speech signal properties across talkers. Of particular scientific interest is the nature of articulatory strategies adopted by individuals in the presence of structural differences across them to achieve phonetic equivalence. Equally of interest are what aspects of, and how, vocal tract morphological differences are reflected in the acoustic speech signal, and if those differences can be estimated from speech acoustics.</p> <p>&nbsp;</p> <p>Our activities to this end had two main components: the first one was the collection of speech production data from an unprecedented number of speakers, using state-of-the-art MRI techniques for imaging the vocal tract, including real-time MRI that our team has pioneered. In particular, we developed a protocol where, in a single 2-hour session, three types of MRI data are collected from a speaker: (i) dynamic, real-time MRI of the tract&rsquo;s mid-sagittal slice at 83 frames per second during production of a comprehensive set of scripted and spontaneous speech material, averaging 17 minutes per subject; (ii) &ldquo;accelerated volumetric&rdquo; images of the vocal tract in 3D, captured while the subjects sustain for 7 seconds any sound from the full set of American English vowels and continuant consonants; (iii) a T2-weighted volumetric image at rest position, capturing with fine detail anatomical characteristics of the vocal tract. We collected such data from 72 speakers, which are to be freely disseminated to the research community.</p> <p>&nbsp;</p> <p>The second component of our activities was the development of models, directly informed by data, that capture aspects of individual variability in speech production behavior, constrained by individual vocal-tract morphology. Central to our efforts was the development of a computational framework that calculates the so-called &ldquo;forward map&rdquo; of any given speaker, which determines the speaker-specific ways by which a speech motor control program (that can be thought also as a linguistic/phonological specification) generates vocal-tract shaping dynamics (that can be observed with real-time MRI). We have shown that knowledge of the forward map can help predict more fine-grained speech production behaviors, for example how much a speaker will move her jaw relative to her tongue when she produces an /t/ or a /k/, and made strides toward personalized articulatory speech synthesis, that is, generation of speech by simulating the cognitive and physical processes involved in human speech production. These modeling efforts were complemented by highlighting numerous biomarkers characterizing individual vocal tract anatomy that can be effectively measured on the T2-weighted images, enabling the investigation of links between articulation and anatomy.</p> <p>&nbsp;</p> <p>Speech production is a critical, uniquely human behavior. Its understanding has been classically limited by the sparsity of appropriate data on articulation. It is our hope that this project will help change this picture, with the release of unprecedented data, as well as the introduction of novel analytical tools.</p> <p>&nbsp;</p><br> <p>            Last Modified: 11/10/2020<br>      Modified by: Shrikanth&nbsp;S&nbsp;Narayanan</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The overarching goal of this project is to advance scientific understanding of how vocal tract morphology and speech articulation interact and explain the variant and invariant aspects of speech signal properties across talkers. Of particular scientific interest is the nature of articulatory strategies adopted by individuals in the presence of structural differences across them to achieve phonetic equivalence. Equally of interest are what aspects of, and how, vocal tract morphological differences are reflected in the acoustic speech signal, and if those differences can be estimated from speech acoustics.     Our activities to this end had two main components: the first one was the collection of speech production data from an unprecedented number of speakers, using state-of-the-art MRI techniques for imaging the vocal tract, including real-time MRI that our team has pioneered. In particular, we developed a protocol where, in a single 2-hour session, three types of MRI data are collected from a speaker: (i) dynamic, real-time MRI of the tractâ€™s mid-sagittal slice at 83 frames per second during production of a comprehensive set of scripted and spontaneous speech material, averaging 17 minutes per subject; (ii) "accelerated volumetric" images of the vocal tract in 3D, captured while the subjects sustain for 7 seconds any sound from the full set of American English vowels and continuant consonants; (iii) a T2-weighted volumetric image at rest position, capturing with fine detail anatomical characteristics of the vocal tract. We collected such data from 72 speakers, which are to be freely disseminated to the research community.     The second component of our activities was the development of models, directly informed by data, that capture aspects of individual variability in speech production behavior, constrained by individual vocal-tract morphology. Central to our efforts was the development of a computational framework that calculates the so-called "forward map" of any given speaker, which determines the speaker-specific ways by which a speech motor control program (that can be thought also as a linguistic/phonological specification) generates vocal-tract shaping dynamics (that can be observed with real-time MRI). We have shown that knowledge of the forward map can help predict more fine-grained speech production behaviors, for example how much a speaker will move her jaw relative to her tongue when she produces an /t/ or a /k/, and made strides toward personalized articulatory speech synthesis, that is, generation of speech by simulating the cognitive and physical processes involved in human speech production. These modeling efforts were complemented by highlighting numerous biomarkers characterizing individual vocal tract anatomy that can be effectively measured on the T2-weighted images, enabling the investigation of links between articulation and anatomy.     Speech production is a critical, uniquely human behavior. Its understanding has been classically limited by the sparsity of appropriate data on articulation. It is our hope that this project will help change this picture, with the release of unprecedented data, as well as the introduction of novel analytical tools.          Last Modified: 11/10/2020       Submitted by: Shrikanth S Narayanan]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
