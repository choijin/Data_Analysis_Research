<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>Collaborative Research:  Efficient Parallel Iterative Monte Carlo Methods for Statistical Analysis of Big Data</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>03/02/2015</AwardEffectiveDate>
<AwardExpirationDate>07/31/2017</AwardExpirationDate>
<AwardTotalIntnAmount>200499.00</AwardTotalIntnAmount>
<AwardAmount>200499</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>03040000</Code>
<Directorate>
<Abbreviation>MPS</Abbreviation>
<LongName>Direct For Mathematical &amp; Physical Scien</LongName>
</Directorate>
<Division>
<Abbreviation>DMS</Abbreviation>
<LongName>Division Of Mathematical Sciences</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Yong Zeng</SignBlockName>
<PO_EMAI>yzeng@nsf.gov</PO_EMAI>
<PO_PHON>7032927902</PO_PHON>
</ProgramOfficer>
<AbstractNarration>The integration of computer technology into science and daily life has enabled the collection of massive volumes of data. To analyze these data, one may have to resort to parallel and distributed architectures. While the parallel and distributed architectures present new capabilities for storage and manipulation of big data, it is unclear, from the inferential point of view, how the current statistical methodology can be transported to the paradigm of big data. Also, growing data size typically comes together with a growing complexity of data structures and of the models needed to account for the structures. Although iterative Monte Carlo algorithms, such as the Markov chain Monte Carlo (MCMC), stochastic approximation, and expectation-maximization (EM) algorithms, have proven to be very powerful and typically unique computational tools for analyzing data of complex structures, they are infeasible for big data as for which a large number of iterations and a complete scan of the full dataset for each iteration are typically required. Big data have put a great challenge on the current statistical methodology. The investigators propose a general principle for developing Monte Carlo algorithms that are feasible for big data and workable on parallel and distributed architectures; that is, using Monte Carlo averages calculated in parallel from subsamples to approximate the quantities that originally need to calculate from the full dataset. This principle avoids the requirement for repeated scans of full data in algorithm iterations, while enabling the algorithm to produce statistically sensible solutions to the problem under consideration. Under this principle, a general algorithm, the so-called subsampling approximation-based parallel stochastic approximation algorithm, is proposed for parameter estimation for big data problems. Unlike the existing algorithms, such as the bag of little bootstraps, aggregated estimation equation, and split-and-conquer algorithms, the proposed algorithm works for the problems for which the observations are generally dependent. Under the same principle, a subsampling approximation-based parallel Metropolis-Hastings algorithm is proposed for Bayesian analysis of big data, and a subsampling approximation-based parallel Monte Carlo EM algorithm is proposed for parameter estimation for the big data problems with missing observations. In addition to the subsampling approximation-based parallel iterative Monte Carlo algorithms, an embarrassingly parallel MCMC algorithm is proposed for Bayesian analysis of big data based on the popular idea of divide-and-conquer. Various schemes of dataset partition and results aggregation are proposed. The validity of the proposed parallel iterative Monte Carlo algorithms, including both the subsampling approximation-based and embarrassingly parallel ones, will be rigorously studied. The proposed algorithms will be applied to spatio-temporal modeling of satellite climate data, genome-wide association study, and stream data analysis.&lt;br/&gt;&lt;br/&gt;The intellectual merit of this project is to propose a general principle for statistical analysis of big data: Using Monte Carlo averages of subsamples to approximate the quantities that originally need to calculate from the full dataset. This principle provides a general strategy for transporting the current statistical methodology to the paradigm of big data. Under this principle, a few subsampling approximation-based parallel iterative Monte Carlo algorithms are proposed. The proposed algorithms address the core problem of big data analysis:how to make a statistically sensible analysis for big data while avoiding repeated scans of the full dataset? This project will have broader impacts because big data are ubiquitous throughout almost all fields of science and technology. A successful research program in theory and methods of parallel iterative Monte Carlo computations can have immense benefit widely throughout science and technology.  The research results will be disseminated to the communities of interest, such as atmospheric science, biomedical science, engineering, and social science, via direct collaboration with researchers in these disciplines, conference presentations, books, and papers to be published in academic journals. The project will have also significant impacts on education through direct involvement of graduate students in the project and incorporation of results into undergraduate and graduate courses. In addition, the package Distributed Iterative Statistical Computing (DISC) that will be developed under this project is designed to provide a platform for Ph.D. students and researchers like the investigators with network-connected computers to experiment new ideas of developing efficient iterative Monte Carlo algorithms in parallel or, more exactly, grid computing environments.</AbstractNarration>
<MinAmdLetterDate>06/02/2015</MinAmdLetterDate>
<MaxAmdLetterDate>06/02/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.049</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1545202</AwardID>
<Investigator>
<FirstName>Faming</FirstName>
<LastName>Liang</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Faming Liang</PI_FULL_NAME>
<EmailAddress>fmliang@purdue.edu</EmailAddress>
<PI_PHON>7654944452</PI_PHON>
<NSF_ID>000490214</NSF_ID>
<StartDate>06/02/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Florida</Name>
<CityName>GAINESVILLE</CityName>
<ZipCode>326112002</ZipCode>
<PhoneNumber>3523923516</PhoneNumber>
<StreetAddress>1 UNIVERSITY OF FLORIDA</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<StateCode>FL</StateCode>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>FL03</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>969663814</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF FLORIDA</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>159621697</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Florida]]></Name>
<CityName>GAINESVILLE</CityName>
<StateCode>FL</StateCode>
<ZipCode>326112002</ZipCode>
<StreetAddress><![CDATA[1 UNIVERSITY OF FLORIDA]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Florida</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>03</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>FL03</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8069</Code>
<Text>CDS&amp;E-MSS</Text>
</ProgramElement>
<ProgramElement>
<Code>8084</Code>
<Text>CDS&amp;E</Text>
</ProgramElement>
<ProgramReference>
<Code>9263</Code>
<Text>COMPUTATIONAL SCIENCE &amp; ENGING</Text>
</ProgramReference>
<Appropriation>
<Code>0113</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2013~200499</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>The major goal of this project is to develop parallel Monte Carlo algorithms for statistical inference of big data. During the project period, the PI, working with the co-PIs, students and colleagues, has developed quite a few such algorithms, including a Bootstrap Metropolis-Hastings algorithm for Bayesian analysis of big data (Liang, Kim and Song, 2006),&nbsp; a split-and-merge algorithm for singular value decomposition&nbsp; of large-scale matrix (Liang, Shi and Mo, 2016), a split-and-merge Bayesian variable selection algorithm for ultra-high dimensional regression (Song and Liang, 2015), a population stochastic approximation Monte Carlo algorithm for&nbsp; parallel Monte Carlo simulations (Song, Wu and Liang, 2014), a parallel and interactive stochastic approximation Monte Carlo algorithm for global optimization (Karagiannis et al., 2017), a double parallel Markov chain Monte Carlo algorithm for general big dataKaragiannis analysis (Xue and Liang, 2017), and some parallel computing algorithms for estimation of large-scale Gaussian graphical models.</p> <p>Meanwhile, the PI and collaborators have proposed some different principles for developing these algorithms. For example, the bootstrap Metropolis-Hastings algorithm is developed under the principle that <em>using Monte Carlo averages calculated in parallel from subsamples to approximate the quantity that originally needs to calculate from the full dataset</em>, which provides a general strategy of transporting the current statistical&nbsp; methodology to the paradigm of big data. The algorithms developed under this principle is particularly suitable for running on parallel and distributed architectures. The other algorithms, such as the split-and-merge singular value decomposition algorithm, the split-and-merge Bayesian variable selection&nbsp; algorithm, and the parallel computing algorithms for estimation of large-scale Gaussian graphical models,&nbsp; are developed under the principle of <em>split-and-merge</em>. Therefore, these algorithms have the embarrassingly parallel structure and can be simply implemented on a parallel machine, which avoid intensive communications between different nodes of the parallel machine. Both the principles, i.e., Monte Carlo average replacement and split-and-merge address the core problem of big data analysis---how to make a statistically sensible analysis for big data while avoiding repeated scans of the full dataset.</p> <p>The population stochastic approximation Monte Carlo algorithm provides a general way for parallel Monte Carlo simulations. It is shown that the proposed algorithm accelerates the convergences of the simulation through interactions between different Markov chains, and it can be more efficient than the single Markov chain algorithm. An explicit ratio of the convergence rates of the two algorithms has been derived.</p> <p>The double parallel Markov chain Monte Carlo algorithm consists of two levels of parallel,&nbsp; data parallel and simulation parallel. The former is developed under the principle of split-and-merge, where a simple and efficient method is proposed for combining the samples from the subset data posterior distributions to approximate those from the full data posterior distribution. The latter is based on the population stochastic approximation Monte Carlo algorithm.&nbsp; Such a combination makes the algorithm extremely efficient for Bayesian analysis of&nbsp; big data and it is expected to have many applications in big data study.</p> <p>The algorithms developed under the project are very general and can be applied to many other disciplines, such as social science, atmospheric science, and biomedical science, which are challenged by big data. Given the ubiquitous nature of big data, the potential benefits of these algorithms can be immense widely throughout science and technology. These algorithms have been disseminated to the coummunities of interest via journal publications and conference and departmental seminar presentations. The project has also significant impacts on education through direct involvement of graduate students in the project and incorporation of reseatch results into undergraduate and graduate courses.&nbsp;</p> <p>&nbsp;</p> <p><strong>References (* </strong>indicates students or postdocs)<strong>:</strong></p> <p>Karagiannis*, G., Konomi, B.A., Lin, G., and Liang, F.&nbsp; (2017). Parallel and Interactive Stochastic Approximation Annealing Algorithms for Global Optimization. <em>Statistics and Computing</em>, in press.</p> <p>Liang, F., Kim*, J. and Song*, Q. (2016). A Bootstrap Metropolis-Hastings Algorithm for Bayesian Analysis of Big Data. <em>Technometrics</em>, <strong>58</strong>(3), 304-318. (this paper won the Youden Prize 2017)</p> <p>Liang, F., Shi*, R. and Mo, Q. (2016). A Split-and-Merge Approach for Singular Value Decomposition of Large-Scale Matrices. <em>Statistics and Its Interface</em>, <strong>9</strong>(4), 453-459.</p> <p>Song*, Q. and Liang, F. (2015). A Split-and-Merge Bayesian Variable Selection Approach for Ultra-high dimensional Regression. <em>Journal of the Royal Statistical Society, Series B</em>,<strong> 77</strong>(5), 947-972.</p> <p>Song*, Q., Wu*, M. and Liang, F. (2014). Weak Convergence Rates of Population versus Single-Chain Stochastic Approximation MCMC Algorithms. <em>Advances in Applied Probability,</em> <strong>46,</strong> 1059-1083.</p> <p>Xue*, J. and Liang, F. (2017). A double-parallel MCMC algorithm for big data analysis. <em>Statistics and Computing</em>, revised.</p><br> <p>            Last Modified: 09/28/2017<br>      Modified by: Faming&nbsp;Liang</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ The major goal of this project is to develop parallel Monte Carlo algorithms for statistical inference of big data. During the project period, the PI, working with the co-PIs, students and colleagues, has developed quite a few such algorithms, including a Bootstrap Metropolis-Hastings algorithm for Bayesian analysis of big data (Liang, Kim and Song, 2006),  a split-and-merge algorithm for singular value decomposition  of large-scale matrix (Liang, Shi and Mo, 2016), a split-and-merge Bayesian variable selection algorithm for ultra-high dimensional regression (Song and Liang, 2015), a population stochastic approximation Monte Carlo algorithm for  parallel Monte Carlo simulations (Song, Wu and Liang, 2014), a parallel and interactive stochastic approximation Monte Carlo algorithm for global optimization (Karagiannis et al., 2017), a double parallel Markov chain Monte Carlo algorithm for general big dataKaragiannis analysis (Xue and Liang, 2017), and some parallel computing algorithms for estimation of large-scale Gaussian graphical models.  Meanwhile, the PI and collaborators have proposed some different principles for developing these algorithms. For example, the bootstrap Metropolis-Hastings algorithm is developed under the principle that using Monte Carlo averages calculated in parallel from subsamples to approximate the quantity that originally needs to calculate from the full dataset, which provides a general strategy of transporting the current statistical  methodology to the paradigm of big data. The algorithms developed under this principle is particularly suitable for running on parallel and distributed architectures. The other algorithms, such as the split-and-merge singular value decomposition algorithm, the split-and-merge Bayesian variable selection  algorithm, and the parallel computing algorithms for estimation of large-scale Gaussian graphical models,  are developed under the principle of split-and-merge. Therefore, these algorithms have the embarrassingly parallel structure and can be simply implemented on a parallel machine, which avoid intensive communications between different nodes of the parallel machine. Both the principles, i.e., Monte Carlo average replacement and split-and-merge address the core problem of big data analysis---how to make a statistically sensible analysis for big data while avoiding repeated scans of the full dataset.  The population stochastic approximation Monte Carlo algorithm provides a general way for parallel Monte Carlo simulations. It is shown that the proposed algorithm accelerates the convergences of the simulation through interactions between different Markov chains, and it can be more efficient than the single Markov chain algorithm. An explicit ratio of the convergence rates of the two algorithms has been derived.  The double parallel Markov chain Monte Carlo algorithm consists of two levels of parallel,  data parallel and simulation parallel. The former is developed under the principle of split-and-merge, where a simple and efficient method is proposed for combining the samples from the subset data posterior distributions to approximate those from the full data posterior distribution. The latter is based on the population stochastic approximation Monte Carlo algorithm.  Such a combination makes the algorithm extremely efficient for Bayesian analysis of  big data and it is expected to have many applications in big data study.  The algorithms developed under the project are very general and can be applied to many other disciplines, such as social science, atmospheric science, and biomedical science, which are challenged by big data. Given the ubiquitous nature of big data, the potential benefits of these algorithms can be immense widely throughout science and technology. These algorithms have been disseminated to the coummunities of interest via journal publications and conference and departmental seminar presentations. The project has also significant impacts on education through direct involvement of graduate students in the project and incorporation of reseatch results into undergraduate and graduate courses.      References (* indicates students or postdocs):  Karagiannis*, G., Konomi, B.A., Lin, G., and Liang, F.  (2017). Parallel and Interactive Stochastic Approximation Annealing Algorithms for Global Optimization. Statistics and Computing, in press.  Liang, F., Kim*, J. and Song*, Q. (2016). A Bootstrap Metropolis-Hastings Algorithm for Bayesian Analysis of Big Data. Technometrics, 58(3), 304-318. (this paper won the Youden Prize 2017)  Liang, F., Shi*, R. and Mo, Q. (2016). A Split-and-Merge Approach for Singular Value Decomposition of Large-Scale Matrices. Statistics and Its Interface, 9(4), 453-459.  Song*, Q. and Liang, F. (2015). A Split-and-Merge Bayesian Variable Selection Approach for Ultra-high dimensional Regression. Journal of the Royal Statistical Society, Series B, 77(5), 947-972.  Song*, Q., Wu*, M. and Liang, F. (2014). Weak Convergence Rates of Population versus Single-Chain Stochastic Approximation MCMC Algorithms. Advances in Applied Probability, 46, 1059-1083.  Xue*, J. and Liang, F. (2017). A double-parallel MCMC algorithm for big data analysis. Statistics and Computing, revised.       Last Modified: 09/28/2017       Submitted by: Faming Liang]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
