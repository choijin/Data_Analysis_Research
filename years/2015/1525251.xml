<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>NRI: Rich Task Perception for Programming by Demonstration</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
<AwardExpirationDate>08/31/2019</AwardExpirationDate>
<AwardTotalIntnAmount>1200000.00</AwardTotalIntnAmount>
<AwardAmount>1200000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<Abbreviation>CSE</Abbreviation>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<Abbreviation>IIS</Abbreviation>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Jie Yang</SignBlockName>
<PO_EMAI>jyang@nsf.gov</PO_EMAI>
<PO_PHON>7032924768</PO_PHON>
</ProgramOfficer>
<AbstractNarration>Robots that can work alongside humans and take on repetitive, time-consuming tasks could greatly improve productivity and reliability in task-oriented environments such as laboratories, manufacturing facilities, or commercial kitchens. One of the key challenges in realizing this vision is that every combination of environment, user, and task presents unique requirements for the robot's behavior and it is impractical to employ traditional approaches for programming these robots. Instead, the PIs envision robots that are programmable by their end-users in their particular operation environment and for the particular tasks they are needed for. To overcome limitations of existing approaches, the PIs propose to develop a framework for rich task perception, which is able to extract detailed task descriptions from intuitive human demonstrations. Building on recent advances in depth camera sensing, GPU-optimized visual processing, and language understanding, the proposed framework will track all objects and people in a scene, recognize their goals and task context, and parse speech to extract higher-level task structure from a demonstration. The PIs will also introduce new programming by demonstration techniques that take full advantage of such rich task information and enable users to program robots by demonstrating their desired behavior. The proposed research has the potential to advance national health, prosperity and welfare by developing research and commercial robotic systems for use in factories, laboratories, and households. It will be an enabling technology for a new generation of highly flexible robots that can be programmed on-the-job to increase the productivity of task environments, such as laboratories or manufacturing facilities. The proposed work will also promote the progress of science by enabling reliable documentation and replication of experiments performed in scientific  research wet-labs. Through a new undergraduate capstone course, this project will educate students to develop and program this next generation of robots. To motivate participation in STEM careers, the PIs will demonstrate their work at yearly public outreach events at the University of Washington, and will organize a summer camp for K-16 students through the UW DawgBytes program.&lt;br/&gt;&lt;br/&gt;Co-robots that can take on repetitive, time-consuming tasks could greatly improve productivity and reliability in task-oriented environments currently occupied by human workers; such as laboratories, manufacturing facilities, or commercial kitchens. One of the key challenges in realizing this vision is that every combination of environment, user, and task presents unique requirements for the co-robot's behavior and it is impractical to employ traditional approaches for programming these robots. Instead, the PIs envision co-robots that are programmable by their end-users in their particular operation environment and for the particular tasks they are needed for. A popular end-user programming approach in robotics is Programming by Demonstration (PbD), which enables users to program robots by demonstrating their desired behavior. While state-of-the-art PbD techniques have generated impressive robotic behaviors, current approaches have limitations that prevent them from becoming practical and widely adopted. Many of these limitations are specifically related to perception, preventing robots from understanding the detailed context of human demonstrations. To overcome these limitations, The PIs propose to develop a framework for rich task perception, which is able to extract detailed task descriptions from intuitive human demonstrations. Building on recent advances in RGB-D camera sensing, GPU-optimized visual processing, and language grounding, the proposed framework will track all objects and people in a scene at a very fi ne granularity, and parse speech to extract higher-level task structure from a demonstration. The PIs will also introduce new PbD techniques that better take advantage of such rich task information both in the programming and execution of tasks.</AbstractNarration>
<MinAmdLetterDate>08/17/2015</MinAmdLetterDate>
<MaxAmdLetterDate>08/17/2015</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.070</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>1525251</AwardID>
<Investigator>
<FirstName>Dieter</FirstName>
<LastName>Fox</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Dieter Fox</PI_FULL_NAME>
<EmailAddress>fox@cs.washington.edu</EmailAddress>
<PI_PHON>2066852517</PI_PHON>
<NSF_ID>000210667</NSF_ID>
<StartDate>08/17/2015</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Luke</FirstName>
<LastName>Zettlemoyer</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME>Dr.</PI_SUFX_NAME>
<PI_FULL_NAME>Luke Zettlemoyer</PI_FULL_NAME>
<EmailAddress>lsz@cs.washington.edu</EmailAddress>
<PI_PHON>2066851227</PI_PHON>
<NSF_ID>000581613</NSF_ID>
<StartDate>08/17/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Investigator>
<FirstName>Maya</FirstName>
<LastName>Cakmak</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Maya Cakmak</PI_FULL_NAME>
<EmailAddress>mcakmak@cs.washington.edu</EmailAddress>
<PI_PHON>2065434043</PI_PHON>
<NSF_ID>000661460</NSF_ID>
<StartDate>08/17/2015</StartDate>
<EndDate/>
<RoleCode>Co-Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>University of Washington</Name>
<CityName>Seattle</CityName>
<ZipCode>981950001</ZipCode>
<PhoneNumber>2065434043</PhoneNumber>
<StreetAddress>4333 Brooklyn Ave NE</StreetAddress>
<StreetAddress2/>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<StateCode>WA</StateCode>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>WA07</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>605799469</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>UNIVERSITY OF WASHINGTON</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>042803536</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[University of Washington]]></Name>
<CityName>Seattle</CityName>
<StateCode>WA</StateCode>
<ZipCode>981952350</ZipCode>
<StreetAddress/>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>Washington</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>07</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>WA07</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>8013</Code>
<Text>NRI-National Robotics Initiati</Text>
</ProgramElement>
<ProgramReference>
<Code>8086</Code>
<Text>Natl Robotics Initiative (NRI)</Text>
</ProgramReference>
<Appropriation>
<Code>0115</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2015~1200000</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p><span id="docs-internal-guid-e96fde3e-7fff-8975-64fc-5d22e26fabba"> <p dir="ltr"><span>Co-robots that can take on repetitive, time-consuming tasks could greatly improve productivity and reliability in task-oriented environments such as laboratories, manufacturing facilities, or commercial kitchens. One of the key challenges in realizing this vision is that every combination of environment, user and task presents unique requirements for the co-robot&rsquo;s behavior and it is impractical to employ traditional approaches for programming these robots. The goal of this project was the development of perceptual capabilities that enable co-robot programming by demonstration (PbD) in such task-oriented environments.&nbsp;</span></p> <p dir="ltr"><span>One line of work focused on detection and tracking of objects, since the ability to reason about the objects in a dynamic scene is crucial for task learning and execution. The outcome of this work includes Re3, a deep network that can track arbitrary objects in a video sequence at very high speed, and a framework for detection and tracking human bodies including the deformations of the body and cloth surfaces, which is important for a robot operating in close proximity of a human.&nbsp; Most recently, we also investigated how a robot can detect drawers and cabinet doors along with their handles so that it can reason about how to physically interact with articulated objects frequently found in task environments.</span></p> <p dir="ltr"><span>To integrate new perceptual capabilities with programming by demonstration, we developed a new interface that enables users to define perceptual representations of objects or parts of objects that are relevant for manipulation actions, along with an efficient search algorithm that allows locating the object or part in a new scene. As a capstone demonstration, we integrated robust hand, skeleton, and object tracking algorithms to support programming of tasks by simply doing them. This required new algorithms to transfer tracked human trajectories to the robot.</span></p> <p dir="ltr"><span>Motivated by the wetlab application domain of this project, we also investigated how a robot can learn to perceive and reason about liquids as it moves them from one container to another. This work was the first to connect liquid-based simulation with real-world visual information to track liquids in real time, thereby enabling robots to robustly operate with liquid containers.&nbsp;</span></p> <p dir="ltr"><span>The techniques developed in this project were motivated by the programming by demonstration goals, where they directly led to improvements. However, they also have applications in a wide range of robotics tasks beyond the PbD setting investigated in this project. All datasets generated during this project were made publicly available to the research community.</span></p> <p dir="ltr"><span>The students funded on this project were trained to conduct research on cutting edge topics that span deep learning, object detection and tracking, and human-robot interaction. They also gained important experience working in a team and advising junior undergraduate researchers.</span></p> <div><span><br /></span></div> </span></p> <p>&nbsp;</p><br> <p>            Last Modified: 12/15/2019<br>      Modified by: Dieter&nbsp;Fox</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[  Co-robots that can take on repetitive, time-consuming tasks could greatly improve productivity and reliability in task-oriented environments such as laboratories, manufacturing facilities, or commercial kitchens. One of the key challenges in realizing this vision is that every combination of environment, user and task presents unique requirements for the co-robotâ€™s behavior and it is impractical to employ traditional approaches for programming these robots. The goal of this project was the development of perceptual capabilities that enable co-robot programming by demonstration (PbD) in such task-oriented environments.  One line of work focused on detection and tracking of objects, since the ability to reason about the objects in a dynamic scene is crucial for task learning and execution. The outcome of this work includes Re3, a deep network that can track arbitrary objects in a video sequence at very high speed, and a framework for detection and tracking human bodies including the deformations of the body and cloth surfaces, which is important for a robot operating in close proximity of a human.  Most recently, we also investigated how a robot can detect drawers and cabinet doors along with their handles so that it can reason about how to physically interact with articulated objects frequently found in task environments. To integrate new perceptual capabilities with programming by demonstration, we developed a new interface that enables users to define perceptual representations of objects or parts of objects that are relevant for manipulation actions, along with an efficient search algorithm that allows locating the object or part in a new scene. As a capstone demonstration, we integrated robust hand, skeleton, and object tracking algorithms to support programming of tasks by simply doing them. This required new algorithms to transfer tracked human trajectories to the robot. Motivated by the wetlab application domain of this project, we also investigated how a robot can learn to perceive and reason about liquids as it moves them from one container to another. This work was the first to connect liquid-based simulation with real-world visual information to track liquids in real time, thereby enabling robots to robustly operate with liquid containers.  The techniques developed in this project were motivated by the programming by demonstration goals, where they directly led to improvements. However, they also have applications in a wide range of robotics tasks beyond the PbD setting investigated in this project. All datasets generated during this project were made publicly available to the research community. The students funded on this project were trained to conduct research on cutting edge topics that span deep learning, object detection and tracking, and human-robot interaction. They also gained important experience working in a team and advising junior undergraduate researchers.             Last Modified: 12/15/2019       Submitted by: Dieter Fox]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
