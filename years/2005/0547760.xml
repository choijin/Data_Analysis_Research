<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CAREER: The Neuro-cognitive Evolution of Speech-reading</AwardTitle>
<AGENCY>NSF</AGENCY>
<AwardEffectiveDate>07/01/2006</AwardEffectiveDate>
<AwardExpirationDate>06/30/2012</AwardExpirationDate>
<AwardTotalIntnAmount>600001.00</AwardTotalIntnAmount>
<AwardAmount>600001</AwardAmount>
<AwardInstrument>
<Value>Continuing Grant</Value>
</AwardInstrument>
<Organization>
<Code>04040000</Code>
<Directorate>
<Abbreviation>SBE</Abbreviation>
<LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
</Directorate>
<Division>
<Abbreviation>BCS</Abbreviation>
<LongName>Division Of Behavioral and Cognitive Sci</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Akaysha Tang</SignBlockName>
<PO_EMAI/>
<PO_PHON/>
</ProgramOfficer>
<AbstractNarration>With a CAREER award from the National Science Foundation, Dr. Asif Ghazanfar at Princeton University will further develop a primate model system to investigate the neural bases for integrating communication signals across sensory modalities. Previous work from his group and others suggest that many perceptual processes related to social communication by monkeys are similar to the processes exhibited by human infants and adults. Like humans, macaque monkeys produce unique facial expressions when producing different vocal signals and they can also perceptually match the appropriate facial expression with a vocalization. The eye movement patterns that monkeys use to process these "multisensory" social inputs are also similar to those used by human adults and children when they view human faces producing speech. &lt;br/&gt;&lt;br/&gt;Building upon these findings, the major aim of this project will be to understand the role that brain areas in the macaque temporal lobe play in integrating faces and voices. Specifically, Dr. Ghazanfar's team will investigate how dynamic facial expressions are integrated with vocal expressions in the auditory cortex and high-level visual cortex. By examining the roles of facial postures and dynamics, eye movements and social experience, they hope to uncover principles of visual-auditory neuronal interactions related to social cognition.&lt;br/&gt;&lt;br/&gt;In addition to providing new insights into normal communication processes, this research could also help us to better understand disabling abnormalities in the development of social skills. That is, despite the fact that dysfunctions of the temporal lobe in humans contribute to a variety of debilitating communication disorders, the underlying neural mechanisms remain relatively unexplored by neurobiologists. For example, autistic children fail to develop skills related to social signal processing. The hallmark of autism is an inability to behave in a socially-appropriate manner; people with autism do not process the relevant sensory cues necessary for normal social interactions with other individuals. In both the auditory and visual domains, autistic children have great difficulties interpreting facial and vocal signals and fail to properly integrate the two modalities. This deficit is a specific impairment in face and voice processing, and does not extend to other types of visual or auditory signals. The goals of this research thus have direct relevance to understanding the neurobiology of communication disorders in general and autism in particular. &lt;br/&gt;</AbstractNarration>
<MinAmdLetterDate>06/29/2006</MinAmdLetterDate>
<MaxAmdLetterDate>05/27/2010</MaxAmdLetterDate>
<ARRAAmount/>
<TRAN_TYPE>Grant</TRAN_TYPE>
<CFDA_NUM>47.075</CFDA_NUM>
<NSF_PAR_USE_FLAG>0</NSF_PAR_USE_FLAG>
<FUND_AGCY_CODE>4900</FUND_AGCY_CODE>
<AWDG_AGCY_CODE>4900</AWDG_AGCY_CODE>
<AwardID>0547760</AwardID>
<Investigator>
<FirstName>Asif</FirstName>
<LastName>Ghazanfar</LastName>
<PI_MID_INIT/>
<PI_SUFX_NAME/>
<PI_FULL_NAME>Asif Ghazanfar</PI_FULL_NAME>
<EmailAddress>asifg@princeton.edu</EmailAddress>
<PI_PHON>6092589314</PI_PHON>
<NSF_ID>000413803</NSF_ID>
<StartDate>06/29/2006</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Princeton University</Name>
<CityName>Princeton</CityName>
<ZipCode>085442020</ZipCode>
<PhoneNumber>6092583090</PhoneNumber>
<StreetAddress>Off. of Research &amp; Proj. Admin.</StreetAddress>
<StreetAddress2><![CDATA[P.O. Box 36]]></StreetAddress2>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<StateCode>NJ</StateCode>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_ORG>NJ12</CONGRESS_DISTRICT_ORG>
<ORG_DUNS_NUM>002484665</ORG_DUNS_NUM>
<ORG_LGL_BUS_NAME>TRUSTEES OF PRINCETON UNIVERSITY, THE</ORG_LGL_BUS_NAME>
<ORG_PRNT_DUNS_NUM>002484665</ORG_PRNT_DUNS_NUM>
</Institution>
<Performance_Institution>
<Name><![CDATA[Princeton University]]></Name>
<CityName>Princeton</CityName>
<StateCode>NJ</StateCode>
<ZipCode>085442020</ZipCode>
<StreetAddress><![CDATA[Off. of Research &amp; Proj. Adm]]></StreetAddress>
<CountryCode>US</CountryCode>
<CountryName>United States</CountryName>
<StateName>New Jersey</StateName>
<CountryFlag>1</CountryFlag>
<CONGRESSDISTRICT>12</CONGRESSDISTRICT>
<CONGRESS_DISTRICT_PERF>NJ12</CONGRESS_DISTRICT_PERF>
</Performance_Institution>
<ProgramElement>
<Code>1699</Code>
<Text>Cognitive Neuroscience</Text>
</ProgramElement>
<ProgramReference>
<Code>0000</Code>
<Text>UNASSIGNED</Text>
</ProgramReference>
<ProgramReference>
<Code>1045</Code>
<Text>CAREER-Faculty Erly Career Dev</Text>
</ProgramReference>
<ProgramReference>
<Code>1187</Code>
<Text>PECASE- eligible</Text>
</ProgramReference>
<ProgramReference>
<Code>OTHR</Code>
<Text>OTHER RESEARCH OR EDUCATION</Text>
</ProgramReference>
<Appropriation>
<Code>0106</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0107</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>490100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0108</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0109</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<Appropriation>
<Code>0110</Code>
<Name>NSF RESEARCH &amp; RELATED ACTIVIT</Name>
<APP_SYMB_ID>040100</APP_SYMB_ID>
</Appropriation>
<FUND_OBLG>2006~202124</FUND_OBLG>
<FUND_OBLG>2007~37874</FUND_OBLG>
<FUND_OBLG>2008~120001</FUND_OBLG>
<FUND_OBLG>2009~120001</FUND_OBLG>
<FUND_OBLG>2010~120001</FUND_OBLG>
<POR>
<DRECONTENT><![CDATA[<div class="porColContainerWBG"> <div class="porContentCol"><p>Speech may well be what makes us human. As far back as 400 BC, Isocrates suggested that we avoid &ldquo;living like animals&rdquo; through our ability to communicate to each other in a uniquely sophisticated manner, which gives us the capacity to build cities, make laws, invent art and so on. The uniqueness of speech to humans is indisputable, but how did it happen? Did speech evolve gradually via communication precursors in the primate lineage or did it arise spontaneously through a fortuitous confluence of neuroanatomical changes that are found only in humans? Even Thomas Huxley, Darwin&rsquo;s irascible promoter of the theory of evolution by natural selection, found the idea that speech could evolve gradually through animal precursors to be too difficult to swallow. Huxley wrote, &ldquo;Believing, as I do&hellip;, that the possession of articulate speech is the grand distinctive character of man&hellip;, I find it very easy to comprehend that some&hellip;inconspicuous structural differences may have been the primary cause of the immeasurable and practically infinite divergence of the Human form from the simian strips.&rdquo;</p> <p>The overarching goal of this proposal was to investigate the evolutionary origins of speech by comparing human behavior with that of the macaque monkey. Any similarities that we find between the two species would suggest that their last common ancester also shared the same feature. This would provide as with an "origin" for different aspects of speech behavior. We tested a variety of features and mechanisms related to speech. These included the 1) rhythmic origins of speech (speech has a universal rhythm of 3-8Hz; that is, your mouth moves between 3 and 8 times per second no matter what language you are speaking), 2) what this rhythm is good for, 3) how facial motion and vocal sounds are linked together during development, and 4) what linking faces to voices are good for in terms of enhancing speech perception.</p> <p>We found that the origin of the speech rhythm is likely through the rhythmic facial expressions of an ancestral primates. Macaques produce a lipsmacking gesture that has the same rhythm has speech and that develops with the same trajectory as speech. What does this rhythm do? It seems that in both species it is used to "chunk" complicated acoustic signals into smaller bits that the brain can process more easily. From these chunks, the brain can extract information about vowel sounds and the identity of the individual speaking. We also learned that the ability to learn how different facial motions are linked to vocal sounds undergoes a period of "narrowing" in humans. We are really good at matching faces and voices no matter what the species (e.g., monkeys and humans) when we are 4 months old, but our ability then becomes limited to only linking human faces and voices by 6 months of age. Monkeys do not undergo this same narrowing process, suggesting that they are less "plastic" during early life. Finally, we learned that one huge advantage for both species is that the ability to link facial motion and sounds leads to faster behavior: both species can detect salient communication events in noisy environments much more quickly if we can use both visual and auditory information together. This points to an evolutionary advantage (thus, a selection pressure) for both species and points to the reason why monkeys and human share some communication abilities.</p><br> <p>            Last Modified: 05/14/2013<br>      Modified by: Asif&nbsp;Ghazanfar</p> </div> <div class="porSideCol"></div> </div>]]></DRECONTENT>
<POR_COPY_TXT><![CDATA[ Speech may well be what makes us human. As far back as 400 BC, Isocrates suggested that we avoid "living like animals" through our ability to communicate to each other in a uniquely sophisticated manner, which gives us the capacity to build cities, make laws, invent art and so on. The uniqueness of speech to humans is indisputable, but how did it happen? Did speech evolve gradually via communication precursors in the primate lineage or did it arise spontaneously through a fortuitous confluence of neuroanatomical changes that are found only in humans? Even Thomas Huxley, DarwinÃ†s irascible promoter of the theory of evolution by natural selection, found the idea that speech could evolve gradually through animal precursors to be too difficult to swallow. Huxley wrote, "Believing, as I do&hellip;, that the possession of articulate speech is the grand distinctive character of man&hellip;, I find it very easy to comprehend that some&hellip;inconspicuous structural differences may have been the primary cause of the immeasurable and practically infinite divergence of the Human form from the simian strips."  The overarching goal of this proposal was to investigate the evolutionary origins of speech by comparing human behavior with that of the macaque monkey. Any similarities that we find between the two species would suggest that their last common ancester also shared the same feature. This would provide as with an "origin" for different aspects of speech behavior. We tested a variety of features and mechanisms related to speech. These included the 1) rhythmic origins of speech (speech has a universal rhythm of 3-8Hz; that is, your mouth moves between 3 and 8 times per second no matter what language you are speaking), 2) what this rhythm is good for, 3) how facial motion and vocal sounds are linked together during development, and 4) what linking faces to voices are good for in terms of enhancing speech perception.  We found that the origin of the speech rhythm is likely through the rhythmic facial expressions of an ancestral primates. Macaques produce a lipsmacking gesture that has the same rhythm has speech and that develops with the same trajectory as speech. What does this rhythm do? It seems that in both species it is used to "chunk" complicated acoustic signals into smaller bits that the brain can process more easily. From these chunks, the brain can extract information about vowel sounds and the identity of the individual speaking. We also learned that the ability to learn how different facial motions are linked to vocal sounds undergoes a period of "narrowing" in humans. We are really good at matching faces and voices no matter what the species (e.g., monkeys and humans) when we are 4 months old, but our ability then becomes limited to only linking human faces and voices by 6 months of age. Monkeys do not undergo this same narrowing process, suggesting that they are less "plastic" during early life. Finally, we learned that one huge advantage for both species is that the ability to link facial motion and sounds leads to faster behavior: both species can detect salient communication events in noisy environments much more quickly if we can use both visual and auditory information together. This points to an evolutionary advantage (thus, a selection pressure) for both species and points to the reason why monkeys and human share some communication abilities.       Last Modified: 05/14/2013       Submitted by: Asif Ghazanfar]]></POR_COPY_TXT>
</POR>
</Award>
</rootTag>
